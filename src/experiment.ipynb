{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator import GeneratorFactory\n",
    "from typing import List\n",
    "\n",
    "def generate(model_name: str, temperature: float, messages: List) -> str:\n",
    "    generator = GeneratorFactory().get_generator(\n",
    "        model_name=model_name, \n",
    "        temperature=temperature\n",
    "    )\n",
    "    response = generator.generate(messages=messages)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinecone Key:  4bc3fa0d-a789-4187-aa8f-d6b17d0ea6a3\n",
      "GitHub Key:  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\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07/11/24 15:03:15] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Initialize CfgNet prompt settings.                              <a href=\"file:///home/ssimon/projects/cval/src/prompt_settings.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">prompt_settings.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ssimon/projects/cval/src/prompt_settings.py#192\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">192</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[07/11/24 15:03:15]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Initialize CfgNet prompt settings.                              \u001b]8;id=661756;file:///home/ssimon/projects/cval/src/prompt_settings.py\u001b\\\u001b[2mprompt_settings.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=460175;file:///home/ssimon/projects/cval/src/prompt_settings.py#192\u001b\\\u001b[2m192\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> GPT <span style=\"font-weight: bold\">(</span>gpt-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.5</span>-turbo-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0125</span><span style=\"font-weight: bold\">)</span> generator initialized.                        <a href=\"file:///home/ssimon/projects/cval/src/generator.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">generator.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ssimon/projects/cval/src/generator.py#47\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">47</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m GPT \u001b[1m(\u001b[0mgpt-\u001b[1;36m3.5\u001b[0m-turbo-\u001b[1;36m0125\u001b[0m\u001b[1m)\u001b[0m generator initialized.                        \u001b]8;id=567960;file:///home/ssimon/projects/cval/src/generator.py\u001b\\\u001b[2mgenerator.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=326587;file:///home/ssimon/projects/cval/src/generator.py#47\u001b\\\u001b[2m47\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07/11/24 15:03:19] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.openai.com/v1/chat/completions</span>          <a href=\"file:///home/ssimon/projects/cval/cval_env/lib/python3.10/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ssimon/projects/cval/cval_env/lib/python3.10/site-packages/httpx/_client.py#1026\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1026</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[07/11/24 15:03:19]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttps://api.openai.com/v1/chat/completions\u001b[0m          \u001b]8;id=53418;file:///home/ssimon/projects/cval/cval_env/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=213736;file:///home/ssimon/projects/cval/cval_env/lib/python3.10/site-packages/httpx/_client.py#1026\u001b\\\u001b[2m1026\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m                                                      \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> GPT <span style=\"font-weight: bold\">(</span>gpt-4o-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">05</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span><span style=\"font-weight: bold\">)</span> generator initialized.                         <a href=\"file:///home/ssimon/projects/cval/src/generator.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">generator.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ssimon/projects/cval/src/generator.py#47\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">47</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m GPT \u001b[1m(\u001b[0mgpt-4o-\u001b[1;36m2024\u001b[0m-\u001b[1;36m05\u001b[0m-\u001b[1;36m13\u001b[0m\u001b[1m)\u001b[0m generator initialized.                         \u001b]8;id=761829;file:///home/ssimon/projects/cval/src/generator.py\u001b\\\u001b[2mgenerator.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=558100;file:///home/ssimon/projects/cval/src/generator.py#47\u001b\\\u001b[2m47\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07/11/24 15:03:24] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.openai.com/v1/chat/completions</span>          <a href=\"file:///home/ssimon/projects/cval/cval_env/lib/python3.10/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ssimon/projects/cval/cval_env/lib/python3.10/site-packages/httpx/_client.py#1026\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1026</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[07/11/24 15:03:24]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttps://api.openai.com/v1/chat/completions\u001b[0m          \u001b]8;id=680103;file:///home/ssimon/projects/cval/cval_env/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=997526;file:///home/ssimon/projects/cval/cval_env/lib/python3.10/site-packages/httpx/_client.py#1026\u001b\\\u001b[2m1026\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m                                                      \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from prompt_settings import PrompSettingsFactory\n",
    "from util import load_config\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import mlflow\n",
    "import json\n",
    "\n",
    "config_file = \"../config.toml\"\n",
    "env_file = \"../.env\"\n",
    "config = load_config(config_file=config_file)\n",
    "load_dotenv(dotenv_path=env_file)\n",
    "\n",
    "print(\"Pinecone Key: \", os.getenv(\"PINECONE_API_KEY\"))\n",
    "print(\"GitHub Key: \", os.getenv(\"GITHUB_TOKEN\"))\n",
    "\n",
    "index_name = \"tech-docs\"\n",
    "data_file = f\"../data/evaluation/all_dependencies_{index_name}.json\"\n",
    "output_file = f\"../data/evaluation/all_dependencies_{index_name}_results.json\"\n",
    "model_names = [\"gpt-3.5-turbo-0125\", \"gpt-4o-2024-05-13\"] # \"llama3:70b\", \"llama3:8b\"\n",
    "\n",
    "mlflow.set_experiment(experiment_name=f\"rag_experiment\")\n",
    "\n",
    "with mlflow.start_run(run_name=f\"{index_name}_{config['rerank']}_{config['embed_model']}-embedding\"): \n",
    "\n",
    "    mlflow.log_params(config)\n",
    "    mlflow.log_artifact(local_path=env_file)\n",
    "    mlflow.log_artifact(local_path=data_file)\n",
    "\n",
    "    with open(data_file, \"r\", encoding=\"utf-8\") as src:\n",
    "        data = json.load(src)\n",
    "\n",
    "    prompt_settings = PrompSettingsFactory.get_prompt_settings(tool_name=config[\"tool_name\"])\n",
    "\n",
    "    results = []\n",
    "    for entry in data:\n",
    "        query_str = prompt_settings.query_prompt.format(\n",
    "            context_str=entry[\"context_str\"], \n",
    "            task_str=entry[\"task_str\"],\n",
    "            format_str=prompt_settings.get_format_prompt()\n",
    "        ) \n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": entry[\"system_str\"]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query_str\n",
    "            }\n",
    "            ]\n",
    "\n",
    "        responses = []\n",
    "        for model_name in model_names:\n",
    "            response = generate(\n",
    "                model_name=model_name, \n",
    "                temperature=config[\"temperature\"],\n",
    "                messages=messages\n",
    "            )\n",
    "            responses.append({model_name: response})\n",
    "\n",
    "        entry[\"responses\"] = responses\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as dest:\n",
    "        json.dump(data, dest, indent=2)\n",
    "\n",
    "    mlflow.log_artifact(local_path=output_file)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cval_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
