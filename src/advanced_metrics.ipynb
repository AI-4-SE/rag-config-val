{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def compute_metrics(df: pd.DataFrame, model_name: str) -> None:\n",
    "    \n",
    "    true_positives = []\n",
    "    true_negatives = []\n",
    "    false_positives = []\n",
    "    false_negatives = []\n",
    "    categories = []\n",
    "    format_failure_count = 0\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "\n",
    "        rating = row[\"rating\"]\n",
    "        response = row[\"responses\"]\n",
    "\n",
    "        try:\n",
    "            response_dict = json.loads(response, strict=False)\n",
    "            isDependency = response_dict[\"isDependency\"]\n",
    "        except:\n",
    "            format_failure_count += 1\n",
    "            continue\n",
    "\n",
    "\n",
    "        # TP: The LLM validates a dependency as correct and the dependency is correct\n",
    "        if isDependency and str(rating).lower() == \"true\":\n",
    "            true_positives.append(1)\n",
    "                \n",
    "        # FP: The LLM validates a dependency as correct, but the dependency is actually incorrect\n",
    "        if isDependency and str(rating).lower() == \"false\":\n",
    "            false_positives.append(1)\n",
    "            categories.append(row[\"final_failure_category\"])\n",
    "\n",
    "        # TN: The LLM validates a dependency as incorrect and the dependency is incorrect\n",
    "        if not isDependency and str(rating).lower() == \"false\":\n",
    "            true_negatives.append(1)\n",
    "            \n",
    "\n",
    "        # FN: The LLM validates a dependency as incorrect, but the dependency is actually correct\n",
    "        if not isDependency and  str(rating).lower() == \"true\":\n",
    "            false_negatives.append(1)\n",
    "            categories.append(row[\"final_failure_category\"])\n",
    "    \n",
    "\n",
    "    tp = sum(true_positives)\n",
    "    fp = sum(false_positives)\n",
    "    fn = sum(false_negatives)\n",
    "    tn = sum(true_negatives)\n",
    "\n",
    "    assert tp + tn + fp + fn + format_failure_count == len(df)\n",
    "\n",
    "    print(f\"Num format failures: {format_failure_count}\")\n",
    "    print(f\"Num Failures for RAG config2 {model_name}: {len(df)}\")\n",
    "    print(f\"Num Failures for RAG config2 with specialized prompt and few-shot: {model_name}: {fn +fp}\")\n",
    "    print(Counter(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num format failures: 0\n",
      "Num Failures for RAG config2 llama3:70b: 152\n",
      "Num Failures for RAG config2 with specialized prompt and few-shot: llama3:70b: 58\n",
      "Counter({'Inheritance and Overrides': 29, 'Configuration Consistency': 24, 'Resource Sharing': 2, 'Inferring Dependencies': 2, 'Port Mapping': 1})\n"
     ]
    }
   ],
   "source": [
    "model_name = \"llama3:70b\" #\"gpt-3.5-turbo-0125\"\n",
    "df = pd.read_csv(f\"../data/analysis/failures_{model_name}.csv\")\n",
    "compute_metrics(df=df, model_name=model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cval_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
