{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simisimon/GitHub/cval/env/lib/python3.9/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "document_types = {\n",
    "    \"DOCUMENTATIONS\": {\n",
    "        \"index_name\": \"technology-docs\",\n",
    "        \"data_dir\": \"../../data/tech_docs\"\n",
    "    },\n",
    "    \"BLOG_POSTS\": {\n",
    "        \"index_name\": \"blog-posts\",\n",
    "        \"data_dir\": \"../../data/blog_posts\"\n",
    "    },\n",
    "    \"STACK_OVERFLOW_POSTS\": {\n",
    "        \"index_name\": \"so-posts\",\n",
    "        \"data_dir\": \"../../data/so_posts\"\n",
    "    },\n",
    "    \"WEB_SEARCH\": {\n",
    "        \"index_name\": \"web-search\",\n",
    "        \"data_dir\": \"../../data/web-search\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blog-posts', 'technology-docs', 'so-posts']\n"
     ]
    }
   ],
   "source": [
    "config = dotenv_values(\"../../.env\")\n",
    "pc = Pinecone(api_key=config[\"PINECONE_API_KEY\"])\n",
    "indexes = pc.list_indexes().names()\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create indexes\n",
    "for document_type, values in document_types.items():\n",
    "    if values[\"index_name\"] not in indexes: \n",
    "        print(f\"Create index for {document_type}\")\n",
    "        pc.create_index(\n",
    "            name=values[\"index_name\"],\n",
    "            dimension=1536,\n",
    "            metric=\"cosine\",\n",
    "            spec=ServerlessSpec(\n",
    "                cloud=\"aws\",\n",
    "                region=\"us-east-1\"\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_up_text(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove unwanted characters and patterns in text input.\n",
    "\n",
    "    :param content: Text input.\n",
    "    \n",
    "    :return: Cleaned version of original text input.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fix hyphenated words broken by newline\n",
    "    content = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', content)\n",
    "\n",
    "    # Remove specific unwanted patterns and characters\n",
    "    unwanted_patterns = [\n",
    "        \"\\\\n\", \"  —\", \"——————————\", \"—————————\", \"—————\",\n",
    "        r'\\\\u[\\dA-Fa-f]{4}', r'\\uf075', r'\\uf0b7'\n",
    "    ]\n",
    "    for pattern in unwanted_patterns:\n",
    "        content = re.sub(pattern, \"\", content)\n",
    "\n",
    "    # Fix improperly spaced hyphenated words and normalize whitespace\n",
    "    content = re.sub(r'(\\w)\\s*-\\s*(\\w)', r'\\1-\\2', content)\n",
    "    content = re.sub(r'\\s+', ' ', content)\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing STACK_OVERFLOW_POSTS\n",
      "docker-compose_maven\n",
      "docker-compose_spring-boot\n",
      "docker_docker-compose\n",
      "docker_maven\n",
      "docker_spring-boot\n",
      "spring-boot_maven\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upserted vectors: 100%|██████████| 1130/1130 [00:25<00:00, 44.00it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import uuid\n",
    "import os\n",
    "\n",
    "\n",
    "embed_model = OpenAIEmbedding(api_key=config[\"OPENAI_KEY\"])\n",
    "\n",
    "# add data\n",
    "for document_type, values in document_types.items():\n",
    "    if not document_type == \"STACK_OVERFLOW_POSTS\":\n",
    "        continue\n",
    "    \n",
    "    print(\"Indexing\", document_type)\n",
    "    pinecone_index = pc.Index(values[\"index_name\"])\n",
    "    #pinecone_index.delete(deleteAll=\"true\")\n",
    "    \n",
    "   \n",
    "    # Initialize VectorStore\n",
    "    vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
    "\n",
    "    cleaned_documents = []\n",
    "\n",
    "    if not os.path.exists(values[\"data_dir\"]):\n",
    "        continue\n",
    "\n",
    "    for dir in glob.glob(values[\"data_dir\"] + \"/**\"):\n",
    "        technology_name = dir.split(\"/\")[-1]\n",
    "        print(technology_name)\n",
    "\n",
    "\n",
    "        documents = SimpleDirectoryReader(dir).load_data()\n",
    "\n",
    "        # clean up documents and add technology name to metadata\n",
    "        for d in documents: \n",
    "            cleaned_text = clean_up_text(d.text)\n",
    "            d.text = cleaned_text\n",
    "            d.metadata[\"technology\"] = technology_name\n",
    "            cleaned_documents.append(d)\n",
    "\n",
    "    # define the ingestion pipeline\n",
    "    pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SemanticSplitterNodeParser(\n",
    "            buffer_size=1,\n",
    "            breakpoint_percentile_threshold=95, \n",
    "            embed_model=embed_model,\n",
    "            ),\n",
    "        embed_model,\n",
    "        ],\n",
    "        vector_store=vector_store\n",
    "    )\n",
    "\n",
    "    # run the ingestion pipeline\n",
    "    pipeline.run(documents=cleaned_documents)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
