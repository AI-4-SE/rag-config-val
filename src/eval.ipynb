{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cval import CVal\n",
    "\n",
    "config_file = \"../config.toml\"\n",
    "env_file = \"../.env\"\n",
    "\n",
    "cval = CVal.init(\n",
    "    config_file=config_file,\n",
    "    env_file=env_file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import Dependency\n",
    "\n",
    "dep = Dependency(\n",
    "    project=\"piggymetrics\",\n",
    "    dependency_category=\"value-equality\",\n",
    "    option_name=\"EXPOSE\",\n",
    "    option_value=\"8080\",\n",
    "    option_type=\"PORT\",\n",
    "    option_file=\"Dockerfile\",\n",
    "    option_technology=\"Docker\",\n",
    "    dependent_option_name=\"server.port\",\n",
    "    dependent_option_value=\"8080\",\n",
    "    dependent_option_file=\"application.yml\",\n",
    "    dependent_option_type=\"PORT\",\n",
    "    dependent_option_technology=\"Spring-Boot\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_data_file = \"../data/test_data/dependencies_without_rules.csv\"\n",
    "df = pd.read_csv(eval_data_file, sep=\";\")\n",
    "df_sample = df[:10]\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric, ContextualRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from dotenv import dotenv_values\n",
    "import os\n",
    "\n",
    "config = dotenv_values(dotenv_path=\"../.env\")\n",
    "os.environ['OPENAI_API_KEY'] = config[\"OPENAI_KEY\"]\n",
    "\n",
    "answer_relevancy_metric = AnswerRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "faithfulness_metric = FaithfulnessMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "context_relevancy_metric = ContextualRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def measure_validation_metrics(outputs: List, df_baseline: pd.DataFrame):\n",
    "\n",
    "    true_positives = []\n",
    "    true_negatives = []\n",
    "    false_positives = []\n",
    "    false_negatives = []\n",
    "    accuracy = []\n",
    "\n",
    "    for response, baseline in zip(outputs, df_baseline.to_dict(\"records\")):\n",
    "\n",
    "        response_dict = json.loads(response.response)\n",
    "\n",
    "        if baseline[\"rating\"] in (\"True\") and response_dict[\"isDependency\"]:\n",
    "            accuracy.append(1)\n",
    "            true_positives.append(1)\n",
    "        if baseline[\"rating\"] in (\"False\") and not response_dict[\"isDependency\"]:\n",
    "            accuracy.append(1)\n",
    "            true_negatives.append(1)\n",
    "        if baseline[\"rating\"] in (\"True\") and not response_dict[\"isDependency\"]:\n",
    "            accuracy.append(0)\n",
    "            false_negatives.append(1)\n",
    "        if baseline[\"rating\"] in (\"False\") and response_dict[\"isDependency\"]:\n",
    "            accuracy.append(0)\n",
    "            false_positives.append(1)\n",
    "\n",
    "\n",
    "    #precision = sum(true_positives)/(sum(true_positives)+sum(false_positives))\n",
    "    #recall = sum(true_positives)/(sum(true_positives)+sum(false_negatives))\n",
    "    #f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    print(\"Accuracy\", sum(accuracy)/len(accuracy))\n",
    "    print(\"TP\", sum(true_positives))\n",
    "    print(\"FP\", sum(false_positives))\n",
    "    print(\"TN\", sum(true_negatives))\n",
    "    print(\"FN\", sum(false_negatives))\n",
    "    #print(\"Precision\", precision)\n",
    "    #print(\"Recall\", recall)\n",
    "    #print(\"F1 Score: \", f1_score)\n",
    "\n",
    "    return {\n",
    "        \"true_positives\": sum(true_positives),\n",
    "        \"false_positives\": sum(false_positives),\n",
    "        \"true_negatives\": sum(true_negatives),\n",
    "        \"false_negatives\": sum(false_negatives),\n",
    "        \"accuracy\": sum(accuracy)/len(accuracy)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def measure_llm_netrics(outputs: List):\n",
    "    faithfulness_scores = []\n",
    "    answer_relevancy_scores = []\n",
    "    context_relevancy_scores = []\n",
    "\n",
    "    for response in outputs:\n",
    "        \n",
    "        test_case = LLMTestCase(\n",
    "            input=response.input,\n",
    "            actual_output=response.response,\n",
    "            retrieval_context=[source_node.node.get_content() for source_node in response.source_nodes]\n",
    "        )\n",
    "\n",
    "        context_relevancy_metric.measure(test_case)\n",
    "        faithfulness_metric.measure(test_case)\n",
    "        answer_relevancy_metric.measure(test_case)\n",
    "\n",
    "        context_relevancy_scores.append(context_relevancy_metric.score)\n",
    "        faithfulness_scores.append(faithfulness_metric.score)\n",
    "        answer_relevancy_scores.append(answer_relevancy_metric.score)       \n",
    "\n",
    "    context_relevance = sum(context_relevancy_scores)/len(context_relevancy_scores)\n",
    "    answer_relevance = sum(answer_relevancy_scores)/len(answer_relevancy_scores)\n",
    "    faithfulness = sum(faithfulness_scores)/len(faithfulness_scores)\n",
    "            \n",
    "    print(\"Context Relevancy: \", context_relevancy_scores, context_relevance)\n",
    "    print(\"Answer Relevancy: \", answer_relevancy_scores, answer_relevance)\n",
    "    print(\"Faithfulness: \", faithfulness_scores, faithfulness)\n",
    "\n",
    "    return {\n",
    "        \"answer_relevance\": answer_relevance,\n",
    "        \"faithfulness\": faithfulness,\n",
    "        \"context_relevance\": context_relevance\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import Dependency\n",
    "import mlflow\n",
    "import datetime\n",
    "\n",
    "index_name = \"all\"\n",
    "exp_name = \"preliminary-experiments\"\n",
    "date_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "\n",
    "mlflow.set_experiment(experiment_name=exp_name)\n",
    "\n",
    "with mlflow.start_run(run_name=f\"{exp_name}_{date_time}\"):\n",
    "\n",
    "    mlflow.log_params(cval.config)\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    for x in df_sample.to_dict(\"records\"):\n",
    "        dependency = Dependency(\n",
    "            project=x[\"project\"],\n",
    "            option_name=x[\"option_name\"],\n",
    "            option_value=x[\"option_value\"],\n",
    "            option_type=x[\"option_type\"].split(\".\")[-1],\n",
    "            option_file=x[\"option_file\"],\n",
    "            option_technology=x[\"option_technology\"],\n",
    "            dependent_option_name=x[\"dependent_option_name\"],\n",
    "            dependent_option_value=x[\"dependent_option_value\"],\n",
    "            dependent_option_type=x[\"dependent_option_type\"].split(\".\")[-1],\n",
    "            dependent_option_file=x[\"dependent_option_file\"],\n",
    "            dependent_option_technology=x[\"dependent_option_technology\"]\n",
    "        )\n",
    "\n",
    "        response = cval.query(\n",
    "            dependency=dependency,\n",
    "            index_name=index_name\n",
    "        )\n",
    "\n",
    "        outputs.append(response)\n",
    "\n",
    "    llm_metrics = measure_llm_netrics(outputs=outputs)\n",
    "    validation_metrics =measure_validation_metrics(outputs=outputs, df_baseline=df_sample)\n",
    "\n",
    "    results = [x.response for x in outputs]\n",
    "    inputs = [x.input for x in outputs]\n",
    "    rating = df_sample[\"rating\"]\n",
    "    df_results = pd.DataFrame().from_dict(data={\"inputs\": inputs, \"outputs\": results, \"isDependency\": rating})\n",
    "    df_results.to_csv(f\"../data/results/test_{index_name}.csv\", index=False)\n",
    "    mlflow.log_table(data={\"inputs\": inputs, \"outputs\": results}, artifact_file=\"results.json\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
