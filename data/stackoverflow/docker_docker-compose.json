[
  {
    "Id": "44785585",
    "PostTypeId": "1",
    "AcceptedAnswerId": "44785784",
    "CreationDate": "2017-06-27T16:36:53.480",
    "Score": "992",
    "ViewCount": "1023912",
    "Body": "<p>I recently started using Docker and never realized that I should use <code>docker-compose down</code> instead of <code>ctrl-c</code> or <code>docker-compose stop</code> to get rid of my experiments. I now have a large number of unneeded docker images locally. </p>  <p>Is there a flag I can run to delete all the local docker images &amp; containers?</p>  <p>Something like <code>docker rmi --all --force</code> --all flag does not exist but I am looking for something with similar idea. </p> ",
    "OwnerUserId": "2335820",
    "LastEditorUserId": "12744",
    "LastEditDate": "2023-03-08T10:14:15.883",
    "LastActivityDate": "2023-10-13T08:03:37.670",
    "Title": "How can I delete all local Docker images?",
    "Tags": "<docker><docker-compose>",
    "AnswerCount": "25",
    "CommentCount": "6",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 4.0",
    "accepted_answer": "<p><strong>Unix</strong></p> <p>To delete all containers including its volumes use,</p> <pre><code>docker rm -vf $(docker ps -aq) </code></pre> <p>To delete all the images,</p> <pre><code>docker rmi -f $(docker images -aq) </code></pre> <p>Remember, you should remove all the containers before removing all the images from which those containers were created.</p> <p><strong>Windows - Powershell</strong></p> <pre><code>docker images -a -q | % { docker image rm $_ -f } </code></pre> <p><strong>Windows - cmd.exe</strong></p> <pre><code>for /F %i in ('docker images -a -q') do docker rmi -f %i </code></pre> ",
    "highest_rated_answer": "<p>Use this to <strong>delete everything</strong>:</p>  <pre><code>docker system prune -a --volumes </code></pre>  <blockquote>   <p>Remove all unused containers, volumes, networks and images</p> </blockquote>  <pre><code>WARNING! This will remove:     - all stopped containers     - all networks not used by at least one container     - all volumes not used by at least one container     - all images without at least one container associated to them     - all build cache </code></pre>  <p><a href='https://docs.docker.com/engine/reference/commandline/system_prune/#extended-description' rel='noreferrer'>https://docs.docker.com/engine/reference/commandline/system_prune/#extended-description</a></p> "
  },
  {
    "Id": "34324277",
    "PostTypeId": "1",
    "AcceptedAnswerId": "47643816",
    "CreationDate": "2015-12-16T23:43:51.877",
    "Score": "98",
    "ViewCount": "60220",
    "Body": "<p>Docker 1.9 allows to pass arguments to a dockerfile. See link: <a href='https://docs.docker.com/engine/reference/builder/#arg' rel='noreferrer'>https://docs.docker.com/engine/reference/builder/#arg</a></p>  <p>How can i pass the same arugments within ENTRYPOINT Instruction??  </p>  <p>My dockerfile has  </p>  <blockquote>   <p>ARG $Version=3.1<br>   ENTRYPOINT /tmp/folder-$Version/sample.sh start  </p> </blockquote>  <p>I am getting an error while creating container with above dockerfile. Please suggest what is the correct way to specify the argument within ENTRYPOINT instruction??</p> ",
    "OwnerUserId": "5632400",
    "LastActivityDate": "2022-02-25T14:27:13.980",
    "Title": "How to pass ARG value to ENTRYPOINT?",
    "Tags": "<docker><dockerfile><docker-compose><docker-registry>",
    "AnswerCount": "5",
    "CommentCount": "2",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>Like <em>Blake Mitchell</em> sais, you cannot use <code>ARG</code> in <code>ENTRYPOINT</code>. However you can use your <code>ARG</code> as a value for <code>ENV</code>, that way you can use it with <code>ENTRYPOINT</code>:</p>  <p><strong>Dockerfile</strong></p>  <pre><code>ARG my_arg ENV my_env_var=$my_arg  ENTRYPOINT echo $my_env_var </code></pre>  <p>and  run:</p>  <pre><code>docker build --build-arg 'my_arg=foo' ... </code></pre> ",
    "highest_rated_answer": "<p>Short answer: you need to use <code>ENV</code></p>  <p>Both <code>ARG</code> and <code>ENV</code> are not expanded in <code>ENTRYPOINT</code> or <code>CMD</code>. (<a href='https://docs.docker.com/engine/reference/builder/#environment-replacement' rel='noreferrer'>https://docs.docker.com/engine/reference/builder/#environment-replacement</a>) However, because <code>ENV</code>s are passed in as part of the environment, they are available at run time, so the shell can expand them. (This means you can't use the array form of <code>ENTRYPOINT</code> or <code>CMD</code>.)</p>  <p>Here is an example:</p>  <pre><code>$ cat arg/Dockerfile FROM debian:jessie ARG FOO=bar ENTRYPOINT echo ${FOO:-foo} $ sudo docker build arg Sending build context to Docker daemon 2.048 kB Step 1 : FROM debian:jessie  ---&gt; f50f9524513f Step 2 : ARG FOO=bar  ---&gt; Using cache  ---&gt; 2cfdcb514b62 Step 3 : ENTRYPOINT echo ${FOO:-foo}  ---&gt; Running in 21fb9b42c10d  ---&gt; 75e5018bad83 Removing intermediate container 21fb9b42c10d Successfully built 75e5018bad83 $ sudo docker run 75e5018bad83 foo $ sudo docker run -e FOO=bas 75e5018bad83 bas $ sudo docker build --build-arg FOO=bas arg Sending build context to Docker daemon 2.048 kB Step 1 : FROM debian:jessie  ---&gt; f50f9524513f Step 2 : ARG FOO=bar  ---&gt; Using cache  ---&gt; 2cfdcb514b62 Step 3 : ENTRYPOINT echo ${FOO:-foo}  ---&gt; Using cache  ---&gt; 75e5018bad83 Successfully built 75e5018bad83 $ sudo docker run 75e5018bad83 foo $ cat env/Dockerfile FROM debian:jessie ARG FOO=bar ENV FOO=${FOO} ENTRYPOINT echo ${FOO:-foo} $ sudo docker build env Sending build context to Docker daemon 2.048 kB Step 1 : FROM debian:jessie  ---&gt; f50f9524513f Step 2 : ARG FOO=bar  ---&gt; Using cache  ---&gt; 2cfdcb514b62 Step 3 : ENV FOO ${FOO}  ---&gt; Running in f157a07c1b3e  ---&gt; a5e8c5b65a17 Removing intermediate container f157a07c1b3e Step 4 : ENTRYPOINT echo ${FOO:-foo}  ---&gt; Running in 66e9800ef403  ---&gt; 249fe326e9ce Removing intermediate container 66e9800ef403 Successfully built 249fe326e9ce $ sudo docker run 249fe326e9ce bar $ sudo docker run -e FOO=bas 249fe326e9ce bas $ sudo docker build --build-arg FOO=bas env Sending build context to Docker daemon 2.048 kB Step 1 : FROM debian:jessie  ---&gt; f50f9524513f Step 2 : ARG FOO=bar  ---&gt; Using cache  ---&gt; 2cfdcb514b62 Step 3 : ENV FOO ${FOO}  ---&gt; Running in 6baf31684b9f  ---&gt; 8f77ad154798 Removing intermediate container 6baf31684b9f Step 4 : ENTRYPOINT echo ${FOO:-foo}  ---&gt; Running in 892ac47cabed  ---&gt; fa97da85bf8a Removing intermediate container 892ac47cabed Successfully built fa97da85bf8a $ sudo docker run fa97da85bf8a bas $ </code></pre> "
  },
  {
    "Id": "43408493",
    "PostTypeId": "1",
    "CreationDate": "2017-04-14T09:05:36.800",
    "Score": "98",
    "ViewCount": "61234",
    "Body": "<p>When do we use a <code>docker service create</code> command and when do we use a <code>docker run</code> command?</p> ",
    "OwnerUserId": "7863706",
    "LastEditorUserId": "4922375",
    "LastEditDate": "2018-12-22T23:36:09.900",
    "LastActivityDate": "2023-07-19T16:39:51.067",
    "Title": "What is the difference between Docker Service and Docker Container?",
    "Tags": "<docker><docker-compose><dockerfile>",
    "AnswerCount": "6",
    "CommentCount": "0",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 4.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p><strong>In short:</strong> Docker service is used mostly when you configured the master node with Docker swarm so that docker containers <strong>will run in a distributed environment</strong> and it can be easily managed.</p>  <p><strong>Docker run:</strong> The docker run command first creates a writeable container layer over the specified image, and then starts it using the specified command. </p>  <p>That is, docker run is equivalent to the API /containers/create then /containers/(id)/start</p>  <p>source: <a href='https://docs.docker.com/engine/reference/commandline/run/#parent-command' rel='noreferrer'>https://docs.docker.com/engine/reference/commandline/run/#parent-command</a></p>  <p><strong>Docker service:</strong> Docker service will be the image for a microservice within the context of some larger application. Examples of services might include an HTTP server, a database, or any other type of executable program that you wish to run in a distributed environment.</p>  <p>When you create a service, you specify which container image to use and which commands to execute inside running containers. You also define options for the service including:</p>  <ul> <li>the port where the swarm will make the service available outside the swarm</li> <li>an overlay network for the service to connect to other services in the swarm</li> <li>CPU and memory limits and reservations</li> <li>a rolling update policy</li> <li>the number of replicas of the image to run in the swarm</li> </ul>  <p>source: <a href='https://docs.docker.com/engine/swarm/how-swarm-mode-works/services/#services-tasks-and-containers' rel='noreferrer'>https://docs.docker.com/engine/swarm/how-swarm-mode-works/services/#services-tasks-and-containers</a></p> "
  },
  {
    "Id": "31149501",
    "PostTypeId": "1",
    "AcceptedAnswerId": "35691865",
    "CreationDate": "2015-06-30T22:16:13.497",
    "Score": "97",
    "ViewCount": "139661",
    "Body": "<p>Is there a way I can reach my docker containers using names instead of ip addresses?</p>  <p>I've heard of pipework and I've seen some dns and hostname type options for docker, but I still am unable to piece everything together.</p>  <p>Thank you for your time.</p>  <p>I'm not sure if this is helpful, but this is what I've done so far:</p>  <ul> <li>installed docker container host using docker-machine and the vmwarevsphere driver</li> <li>started up all the services with docker-compose</li> <li>I can reach all of the services from any other machine on the network using IP and port</li> </ul>  <p>I've added a DNS alias entry to my private network DNS server and it matches the machine name that's used by docker-machine.  But the machine always picks up a different IP address when it boots and connects to the network.</p>  <p>I'm just lost as to where to tackle this:</p>  <ul> <li>network DNS server</li> <li>docker-machine hostname</li> <li>docker container hostname</li> <li>probably some combination of all of them</li> </ul>  <p>I'm probably looking for something similar to this question:</p>  <p><a href='https://stackoverflow.com/questions/27342382/how-can-let-docker-use-my-network-router-to-assign-dhcp-ip-to-containers-easily'>How can let docker use my network router to assign dhcp ip to containers easily instead of pipework?</a></p>  <p>Any general direction will be awesome...thanks again!</p> ",
    "OwnerUserId": "1207596",
    "LastEditorUserId": "-1",
    "LastEditDate": "2017-05-23T12:02:00.800",
    "LastActivityDate": "2022-10-23T10:41:42.040",
    "Title": "How to reach docker containers by name instead of IP address?",
    "Tags": "<dns><docker><hostname><docker-compose><docker-machine>",
    "AnswerCount": "7",
    "CommentCount": "2",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>Docker <code>1.10</code> has a built in DNS. If your containers are connected to the same <em>user defined</em> network (create a network <code>docker network create my-network</code> and run your container with <code>--net my-network</code>) they can reference each other using the container name. (<a href='https://docs.docker.com/v17.09/engine/userguide/networking/configure-dns/' rel='noreferrer'>Docs</a>).</p>  <p>Cool!</p>  <p>One caveat if you are using Docker compose you know that it adds a prefix to your container names, i.e. <code>&lt;project name&gt;_&lt;service name&gt;-#</code>. This makes your container names somewhat more difficult to control, but it might be ok for your use case. You can <a href='https://docs.docker.com/compose/compose-file/#container-name' rel='noreferrer'>override the docker compose naming functionality by manually setting the container name in your compose template, but then you wont be able to scale with compose</a>. </p> ",
    "highest_rated_answer": "<p>Create a new bridge network other than docker0, run your containers inside it and you can reference the containers inside that network by their names. </p>  <blockquote>   <p>Docker daemon runs an embedded DNS server to provide automatic service   discovery for containers connected to user-defined networks. Name   resolution requests from the containers are handled first by the   embedded DNS server.</p> </blockquote>  <p>Try this:</p>  <pre><code>docker network create &lt;network name&gt; docker run --net &lt;network name&gt; --name test busybox nc -l 0.0.0.0:7000 docker run --net &lt;network name&gt; busybox ping test </code></pre>  <p>First, we create a new network. Then, we run a busybox container named <strong>test</strong> listening on port 7000 (just to keep it running). Finally, we ping the test container by its name and it should work. </p> "
  },
  {
    "Id": "47615495",
    "PostTypeId": "1",
    "AcceptedAnswerId": "47622441",
    "CreationDate": "2017-12-03T05:15:01.137",
    "Score": "97",
    "ViewCount": "120508",
    "Body": "<p>I am currently trying out this tutorial for <code>node express</code> with <code>mongodb</code> <a href='https://medium.com/@sunnykay/docker-development-workflow-node-express-mongo-4bb3b1f7eb1e' rel='noreferrer'>https://medium.com/@sunnykay/docker-development-workflow-node-express-mongo-4bb3b1f7eb1e</a></p> <p>the first part works fine where to build the <code>docker-compose.yml</code> it works totally fine building it locally so I tried to tag it and push into my <code>dockerhub</code> to learn and try more.</p> <p>this is originally what's in the <code>yml</code> file followed by the tutorial</p> <pre><code>version: &quot;2&quot; services:   web:     build: .     volumes:       - ./:/app     ports:       - &quot;3000:3000&quot; </code></pre> <p>this works like a charm when I use <code>docker-compose build</code> and <code>docker-compose up</code></p> <p>so I tried to push it to my dockerhub and I also tag it as <code>node-test</code></p> <p>I then changed the <code>yml</code> file into</p> <pre><code>version: &quot;2&quot; services:   web:     image: &quot;et4891/node-test&quot;     volumes:       - ./:/app     ports:       - &quot;3000:3000&quot; </code></pre> <p>then I removed all images I have previously to make sure this also works...but when I run <code>docker-compose build</code>  I see this message <code>error: web uses an image, skipping</code> and nothing happens.</p> <p>I tried googling the error but nothing much I can find.</p> <p>Can someone please give me a hand?</p> ",
    "OwnerUserId": "1850712",
    "LastEditorUserId": "11107541",
    "LastEditDate": "2022-12-18T23:17:00.127",
    "LastActivityDate": "2023-04-21T13:15:26.410",
    "Title": "Docker: Uses an image, skipping (docker-compose)",
    "Tags": "<node.js><mongodb><docker><docker-compose>",
    "AnswerCount": "5",
    "CommentCount": "4",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 4.0",
    "accepted_answer": "<p>I found out, I was being stupid.</p>  <p>I didn't need to run <code>docker-compose build</code> I can just directly run <code>docker-compose up</code> since then it'll pull the images down, the <code>build</code> is just to build locally</p> ",
    "highest_rated_answer": "<p>in my case below command worked:</p>  <pre><code>docker-compose up --force-recreate </code></pre>  <p>I hope this helps!</p> "
  },
  {
    "Id": "30063907",
    "PostTypeId": "1",
    "AcceptedAnswerId": "30064175",
    "CreationDate": "2015-05-05T21:53:37.160",
    "Score": "969",
    "ViewCount": "1093335",
    "Body": "<p>I want to do something like this where I can run multiple commands in the following code:</p> <pre class='lang-yaml prettyprint-override'><code>db:   image: postgres web:   build: .   command: python manage.py migrate   command: python manage.py runserver 0.0.0.0:8000   volumes:     - .:/code   ports:     - &quot;8000:8000&quot;   links:     - db </code></pre> <p>How could I execute multiple commands?</p> ",
    "OwnerUserId": "2587797",
    "LastEditorUserId": "11107541",
    "LastEditDate": "2022-12-27T00:59:12.397",
    "LastActivityDate": "2023-02-26T11:27:07.617",
    "Title": "Docker Compose - How to execute multiple commands?",
    "Tags": "<docker><docker-compose>",
    "AnswerCount": "21",
    "CommentCount": "0",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 4.0",
    "accepted_answer": "<p>Figured it out, use <strong><code>bash -c</code></strong>.</p>  <p>Example:</p>  <pre><code>command: bash -c 'python manage.py migrate &amp;&amp; python manage.py runserver 0.0.0.0:8000' </code></pre>  <p>Same example in multilines:</p>  <pre><code>command: &gt;     bash -c 'python manage.py migrate     &amp;&amp; python manage.py runserver 0.0.0.0:8000' </code></pre>  <p>Or:</p>  <pre><code>command: bash -c '     python manage.py migrate     &amp;&amp; python manage.py runserver 0.0.0.0:8000   ' </code></pre> ",
    "highest_rated_answer": "<p>I recommend using <code>sh</code> as opposed to <code>bash</code> because it is more readily available on most Unix based images (alpine, etc).</p> <p>Here is an example <code>docker-compose.yml</code>:</p> <pre class='lang-yaml prettyprint-override'><code>version: '3'  services:   app:     build:       context: .     command: &gt;       sh -c &quot;python manage.py wait_for_db &amp;&amp;              python manage.py migrate &amp;&amp;              python manage.py runserver 0.0.0.0:8000&quot; </code></pre> <p>This will call the following commands in order:</p> <ul> <li><code>python manage.py wait_for_db</code> - wait for the DB to be ready</li> <li><code>python manage.py migrate</code> - run any migrations</li> <li><code>python manage.py runserver 0.0.0.0:8000</code> - start my development server</li> </ul> "
  },
  {
    "Id": "35560894",
    "PostTypeId": "1",
    "AcceptedAnswerId": "35562189",
    "CreationDate": "2016-02-22T18:15:08.793",
    "Score": "96",
    "ViewCount": "39210",
    "Body": "<p>I have a Dockerfile where an <code>ARG</code> is used in the <code>CMD</code> instruction:</p>  <pre><code>ARG MASTER_NAME CMD spark-submit --deploy-mode client --master ${MASTER_URL} </code></pre>  <p>The arg is passed via docker-compose:</p>  <pre><code>  spark:     build:       context: spark       args:         - MASTER_URL=spark://master:7077 </code></pre>  <p>However, the <code>ARG</code> does not seem to get expanded for <code>CMD</code>. After I <code>docker-compose up</code>.</p>  <p>Here's what inspect shows:</p>  <pre><code>docker inspect  -f '{{.Name}} {{.Config.Cmd}}' $(docker ps -a -q) /spark {[/bin/sh -c spark-submit --deploy-mode client --master ${MASTER_URL}]} </code></pre> ",
    "OwnerUserId": "1086117",
    "LastEditorUserId": "745868",
    "LastEditDate": "2018-12-28T00:10:46.477",
    "LastActivityDate": "2018-12-28T00:10:46.477",
    "Title": "Is Docker ARG allowed within CMD instruction",
    "Tags": "<docker><docker-compose>",
    "AnswerCount": "1",
    "CommentCount": "0",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 4.0",
    "accepted_answer": "<p>The thing is that <code>args</code> only can be used at build time, and the <code>CMD</code> is executing at run time. I guess that the only approach now to achieve what you want is setting an environment variable in the Dockerfile with the <code>MASTER_NAME</code> value.</p>  <pre><code>ARG MASTER_NAME ENV MASTER_NAME ${MASTER_NAME} CMD spark-submit --deploy-mode client --master ${MASTER_NAME} </code></pre> ",
    "highest_rated_answer": null
  },
  {
    "Id": "52429984",
    "PostTypeId": "1",
    "AcceptedAnswerId": "52430444",
    "CreationDate": "2018-09-20T17:01:32.607",
    "Score": "96",
    "ViewCount": "83524",
    "Body": "<p><strong>TL;DR :</strong> How can I pass env variable when building the image with <code>docker-compose</code> and have <code>docker run image command</code> recognize them ?</p>  <p>I have this Dockerfile :</p>  <pre><code>FROM mhart/alpine-node:10 ADD . /app WORKDIR /app RUN apk --no-cache add --virtual builds-deps build-base python &amp;&amp;\\     yarn global add nodemon &amp;&amp;\\     yarn &amp;&amp;\\     apk del builds-deps build-base python </code></pre>  <p>and this docker-compose.yml :</p>  <pre><code>version: '3.3' services:    api:     build:       context: .       dockerfile: Dockerfile-preprod     image: registry.git.louis-girones.fr:4567/make-your-night/back:preprod     environment:       - NODE_ENV=development     env_file:       - .env     volumes:       - .:/app     ports:       - '${PORT_PREPROD}:${PORT_PREPROD}'     command: sh -c 'mkdir -p dist &amp;&amp; touch ./dist/app.js &amp;&amp; yarn run start'    mongo:     image: mongo:4.0     ports:       - '${MONGO_PREPROD}'     command: mongod     volumes:       - ./data:/data/db    elasticsearch:     image: docker.elastic.co/elasticsearch/elasticsearch:6.1.1     volumes:       - ./esdata:/usr/share/elasticsearch/data     environment:       - bootstrap.memory_lock=true       - 'ES_JAVA_OPTS=-Xms512m -Xmx512m'       - discovery.type=single-node     ports:       - '9300:9300'       - '9200:9200'  volumes:   esdata: </code></pre>  <p>With this .env file (which is in the root folder, like docker-compose.yml and Dockerfile) :</p>  <pre><code>#!/usr/bin/env bash  NODE_ENV=development PORT=9000 SECRET_SESSION=superSecr3t APP_NAME=Night Vision API_VERSION=/api/v0/ DEFAULT_TZ=Europe/Paris ASSETS_URI=http://localhost:9000/public/img/ BCRYPT_WORKFACTOR=1 ES_PORT=9200 ES_LOG_LEVEL=trace </code></pre>  <p>And this code in the node server startup :</p>  <pre><code>// Export the config object based on the NODE_ENV // ============================================== const config: IConfig = commonConfig  if (commonConfig.env === 'development') {     _.merge(config, developmentConfig) } else if (commonConfig.env === 'test') {     _.merge(config, testConfig) } else if (commonConfig.env === 'preproduction') {     _.merge(config, preproductionConfig) } else if (commonConfig.env === 'production') {     _.merge(config, productionConfig) } else {     throw new Error('Please set an environment') } </code></pre>  <p>When I run the <code>docker-compose build</code> command, everything is fine, but for instance If I try <code>docker run myimage yarn run test</code> the Error 'Please set an environment' is thrown. </p>  <p>I would expect that </p>  <pre><code>env_file:   - .env </code></pre>  <p>makes the env variables of this file accessible in my image but that is not the case, that's why I tried to add </p>  <pre><code> environment:   - NODE_ENV=development </code></pre>  <p>But still no success, I have also tried to pass my env variable as command line argument when I run the build :</p>  <p><code>docker-compose build --build-arg  NODE_ENV=development api</code></p>  <p>But it gives me this message :</p>  <pre><code>[Warning] One or more build-args [NODE_ENV] were not consumed Successfully built 9b14dd5abc3f </code></pre>  <p>And I would really prefer to use the first or second methods</p>  <p>docker version : 18.06.1-ce docker-compose version : 1.19.0</p> ",
    "OwnerUserId": "7337055",
    "LastEditorUserId": "7337055",
    "LastEditDate": "2018-09-20T17:07:13.347",
    "LastActivityDate": "2022-03-14T20:53:02.407",
    "Title": "docker-compose build environment variable",
    "Tags": "<docker><docker-compose>",
    "AnswerCount": "2",
    "CommentCount": "5",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 4.0",
    "accepted_answer": "<p>There's a note <a href='https://docs.docker.com/compose/compose-file/#environment' rel='noreferrer'>in the docs</a>:</p>  <blockquote>   <p><strong>Note:</strong> If your service specifies a build option, variables defined in environment are not automatically visible during the build. Use the args sub-option of build to define build-time environment variables.</p> </blockquote>  <p>It's also <a href='https://docs.docker.com/compose/compose-file/#args' rel='noreferrer'>described in here</a>. First (as mentioned above), you need to specify <code>ARG</code> in Dockerfile:</p>  <pre><code>FROM mhart/alpine-node:10 ARG NODE_ENV ENV NODE_ENV $NODE_ENV ADD . /app WORKDIR /app RUN apk --no-cache add --virtual builds-deps build-base python &amp;&amp;\\     yarn global add nodemon &amp;&amp;\\     yarn &amp;&amp;\\     apk del builds-deps build-base python </code></pre>  <p>And then edit your <code>docker-compose</code> file to include the argument during build, like so:</p>  <pre><code>build:   context: .   dockerfile: Dockerfile-preprod   args:     - NODE_ENV=development </code></pre>  <p>Or even</p>  <pre><code>build:   context: .   dockerfile: Dockerfile-preprod   args:     - NODE_ENV=${NODE_ENV} </code></pre> ",
    "highest_rated_answer": "<p>As per documentation under <a href='https://docker-docs.netlify.app/compose/compose-file/#args' rel='noreferrer'>build args</a>.</p> <blockquote> <p>You can omit the value when specifying a build argument, in which case its value at build time is the value in the environment where Compose is running.</p> </blockquote> <pre class='lang-yaml prettyprint-override'><code>args:   NODE_ENV: </code></pre> <p>Additionally, it will use the <code>.env</code> file as documented under <a href='https://docker-docs.netlify.app/compose/compose-file/#variable-substitution' rel='noreferrer'>variable substitution</a>.</p> <blockquote> <p>You can set default values for environment variables using a .env file, which Compose automatically looks for. Values set in the shell environment override those set in the .env file.</p> </blockquote> <p>So you can create a <code>.env</code> file like below.</p> <pre><code>NODE_ENV=production BASE_URL=/foo/bar/ </code></pre> <p>And then just list them in the compose file either under <code>build.args</code> to make them available on <em>build</em>, or under <code>environment</code> to make them available on <em>run</em>.</p> <pre class='lang-yaml prettyprint-override'><code>version: '3.9' services:   app:     build:       context: .       # these are available on build       args:         NODE_ENV:     # these are available on run     environment:       BASE_URL: </code></pre> <p>This is kind of useful. However, the problem that comes along with this approach is that if the variables are not set from shell or env file, the default values in the Dockerfile will be overwritten with an empty value.</p> <p>If one wants to keep some sort of default, the following can be used.</p> <pre class='lang-yaml prettyprint-override'><code>version: '3.9' services:   app:     build:       context: .       args:         NODE_ENV: ${NODE_ENV:-dev}     environment:       BASE_URL: ${BASE_URL:-http://localhost:8080} </code></pre> <p>This is making usage of <a href='https://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_06_02' rel='noreferrer'>posix parameter expansion</a>. If the variable is not set, it will use the value after <code>:-</code>. So in the example above, it would default to <code>NODE_ENV=dev</code> and <code>BASE_URL=http://localhost:8080</code>, if those are not set.<br /> Allowing to override them with a <code>.env</code> file or by setting a shell variable, i.e. <code>export NODE_ENV=prod</code>.</p> <p>If you want to change the env file it's using, you can do that with the --env-file flag.</p> <pre class='lang-sh prettyprint-override'><code>docker compose --env-file .my-env up </code></pre> "
  },
  {
    "Id": "66912085",
    "PostTypeId": "1",
    "AcceptedAnswerId": "66912110",
    "CreationDate": "2021-04-01T22:00:08.163",
    "Score": "96",
    "ViewCount": "251790",
    "Body": "<p>I've been running docker-compose build for days, many times per day, and haven't changed my DOCKERFILEs or docker-compose.yml. Suddenly an hour ago I started getting this:</p> <pre><code>Building frontdesk-api failed to get console mode for stdout: The handle is invalid. [+] Building 10.0s (3/4)  =&gt; [internal] load build definition from Dockerfile                       0.0s [+] Building 10.1s (4/4) FINISHED  =&gt; [internal] load build definition from Dockerfile                       0.0s  =&gt; =&gt; transferring dockerfile: 32B                                        0.0s  =&gt; [internal] load .dockerignore                                          0.0s  =&gt; =&gt; transferring context: 2B                                            0.0s  =&gt; ERROR [internal] load metadata for mcr.microsoft.com/dotnet/sdk:5.0.  10.0sailed to do request: Head https://mcr.microsoft.com/v2/dotnet/sdk/manifests/5.0.201-buster-slim: dial tcp: lookup mcr.microsoft.com on 192.168.65.5:53:  =&gt; [internal] load metadata for mcr.microsoft.com/dotnet/aspnet:5.0-bust  0.0s ------  &gt; [internal] load metadata for mcr.microsoft.com/dotnet/sdk:5.0.201-buster-slim: ------ ERROR: Service 'frontdesk-api' failed to build </code></pre> <p>Things I've tried:</p> <ul> <li>Running it again</li> <li><code> docker rm -f $(docker ps -a -q)</code></li> <li><code>docker login</code></li> <li>Different SDK image</li> </ul> <p>Here is the DOCKERFILE:</p> <pre><code>FROM mcr.microsoft.com/dotnet/aspnet:5.0-buster-slim AS base WORKDIR /app EXPOSE 80 EXPOSE 443  FROM mcr.microsoft.com/dotnet/sdk:5.0.201-buster-slim AS build WORKDIR /app # run this from repository root COPY ./ ./  #RUN ls -lha .  RUN echo 'Building FrontDesk container'  WORKDIR /app/FrontDesk/src/FrontDesk.Api #RUN ls -lha . RUN dotnet restore  RUN dotnet build &quot;FrontDesk.Api.csproj&quot; -c Release -o /app/build  FROM build AS publish RUN dotnet publish &quot;FrontDesk.Api.csproj&quot; -c Release -o /app/publish  FROM base AS final WORKDIR /app COPY --from=publish /app/publish . ENTRYPOINT [&quot;dotnet&quot;, &quot;FrontDesk.Api.dll&quot;] </code></pre> <p>How can I get my build working again?</p> ",
    "OwnerUserId": "13729",
    "LastActivityDate": "2023-12-21T15:27:54.907",
    "Title": "Why is docker-compose failing with ERROR internal load metadata suddenly?",
    "Tags": "<docker><docker-compose>",
    "AnswerCount": "32",
    "CommentCount": "0",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 4.0",
    "accepted_answer": "<p>mcr.microsoft.com is down at the moment</p> <p>I'm receiving several different errors when pulling:</p> <pre><code>% docker pull mcr.microsoft.com/dotnet/sdk:5.0 Error response from daemon: Get https://mcr.microsoft.com/v2/: Service Unavailable  % docker pull mcr.microsoft.com/dotnet/sdk:5.0 5.0: Pulling from dotnet/sdk received unexpected HTTP status: 500 Internal Server Error </code></pre> ",
    "highest_rated_answer": "<p>the solution that worked for me is to delete the .docker/config.json by running</p> <pre><code>rm  ~/.docker/config.json  </code></pre> <p>then <code>docker-compose up your-services</code> should work</p> "
  },
  {
    "Id": "67642620",
    "PostTypeId": "1",
    "AcceptedAnswerId": "67643907",
    "CreationDate": "2021-05-21T19:03:01.900",
    "Score": "96",
    "ViewCount": "87563",
    "Body": "<p>I might have a bit of a messed Docker installation on my Mac.. At first I installed Docker desktop but then running it I learned that as I'm on an older Mac I had to install VirtualBox so I did following these steps:</p> <ol> <li><p>enable writing on the <code>/usr/local/bin</code> folder for user</p> <p><code>sudo chown -R $(whoami) /usr/local/bin</code></p> </li> <li><p>install Docker-Machine</p> </li> </ol> <pre class='lang-sh prettyprint-override'><code>base=https://github.com/docker/machine/releases/download/v0.16.0 &amp;&amp;   curl -L $base/docker-machine-$(uname -s)-$(uname -m) &gt;/usr/local/bin/docker-machine &amp;&amp;   chmod +x /usr/local/bin/docker-machine </code></pre> <ol start='3'> <li><p>install Xcode CLI..manually from dev account</p> </li> <li><p>Install Home Brew</p> </li> </ol> <pre class='lang-sh prettyprint-override'><code>/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot; </code></pre> <ol start='5'> <li><p>Install  Docker  + wget ( Using Brew)</p> <p><code>brew install docker</code></p> <p><code>brew install wget</code></p> </li> <li><p>Install bash completion scripts</p> </li> </ol> <pre class='lang-sh prettyprint-override'><code>base=https://raw.githubusercontent.com/docker/machine/v0.16.0 for i in docker-machine-prompt.bash docker-machine-wrapper.bash docker-machine.bash do     sudo wget &quot;$base/contrib/completion/bash/${i}&quot; -P /etc/bash_completion.d done </code></pre> <ol start='7'> <li><p>enable the docker-machine shell prompt</p> <p><code>echo 'PS1='[\\u@\\h \\W$(__docker_machine_ps1)]\\$ '' &gt;&gt; ~/.bashrc</code></p> </li> <li><p>Install VirtualBox, ExtensionPack and SDK: <a href='https://www.virtualbox.org/wiki/Downloads' rel='noreferrer'>https://www.virtualbox.org/wiki/Downloads</a></p> </li> </ol> <p>I now installed docker-compose (docker-compose version 1.29.2, build unknown) with home-brew but when running <code>docker-compose up</code> I get the following error:</p> <blockquote> <p><code>docker.credentials.errors.InitializationError: docker-credential-desktop not installed or not available in PATH</code></p> </blockquote> <p><code>which docker</code> prints /usr/local/bin/docker.</p> <p>Brew installations are in <code>/usr/local/Cellar/docker/20.10.6</code> and <code>/usr/local/Cellar/docker-compose/1.29.2</code>. As I see there is also a home-brew for docker-machine should I install docker-machine via home-brew instead?</p> <p>What can I check to make sure that I use the docker installations from home-brew and wipe/correct the installations made from steps above?</p> ",
    "OwnerUserId": "9663497",
    "LastEditorUserId": "1708751",
    "LastEditDate": "2022-02-16T17:37:11.043",
    "LastActivityDate": "2024-01-16T23:07:02.527",
    "Title": "docker-credential-desktop not installed or not available in PATH",
    "Tags": "<docker><docker-compose><homebrew><docker-machine>",
    "AnswerCount": "7",
    "CommentCount": "5",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 4.0",
    "accepted_answer": "<p>After a long googling I found out that the problem is with the <code>config.json</code> file. The <code>&quot;credsStore&quot; : &quot;docker-credential-desktop&quot;</code> is wrong one in :</p> <pre><code>{   &quot;credsStore&quot; : &quot;docker-credential-desktop&quot;,   &quot;stackOrchestrator&quot; : &quot;swarm&quot;,   &quot;experimental&quot; : &quot;disabled&quot; }  </code></pre> <p>changed the <code>&quot;credsStore&quot;</code> key value to <code>&quot;desktop&quot;</code> and compose now works as expected. Some pointed out that <code>credsDstore</code> typo was the problem and fixed it with <code>credDstore</code>, but in my case the value was the problem, it works both with <code>&quot;credsStore&quot; : &quot;desktop&quot;</code> and <code>&quot;credStore&quot; : &quot;desktop&quot;</code>.</p> <p>Hope it'll help others starting out with Docker. Cheers.</p> ",
    "highest_rated_answer": "<p>Check your <code>~/.docker/config.json</code> and replace &quot;credsStore&quot; by &quot;credStore&quot;</p> <pre><code>{   &quot;stackOrchestrator&quot; : &quot;swarm&quot;,   &quot;experimental&quot; : &quot;disabled&quot;,   &quot;credStore&quot; : &quot;desktop&quot; }  </code></pre> "
  },
  {
    "Id": "40801772",
    "PostTypeId": "1",
    "CreationDate": "2016-11-25T09:39:10.243",
    "Score": "956",
    "ViewCount": "625609",
    "Body": "<p>What is the difference between <code>ports</code> and <code>expose</code> options in <code>docker-compose.yml</code>?</p> ",
    "OwnerUserId": "84143",
    "LastEditorUserId": "8747928",
    "LastEditDate": "2022-10-21T11:26:46.430",
    "LastActivityDate": "2024-02-14T05:15:00.360",
    "Title": "What is the difference between ports and expose in docker-compose?",
    "Tags": "<docker><docker-compose>",
    "AnswerCount": "5",
    "CommentCount": "1",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 4.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p>According to the <a href='https://docs.docker.com/compose/compose-file/' rel='nofollow noreferrer'>docker-compose reference</a>,</p> <h1><a href='https://docs.docker.com/compose/compose-file/compose-file-v3/#ports' rel='nofollow noreferrer'>Ports</a> is defined as:</h1> <blockquote> <p>Expose <strong>ports</strong>. Either specify both ports (HOST:CONTAINER), or just the container port (a random host port will be chosen).</p> </blockquote> <ul> <li>Ports mentioned in docker-compose.yml will be shared among different services started by the docker-compose.</li> <li>Ports will be exposed to the host machine to a random port or a given port.</li> </ul> <p>My <code>docker-compose.yml</code> looks like:</p> <pre><code>mysql:   image: mysql:5.7   ports:     - &quot;3306&quot; </code></pre> <p>If I do <code>docker-compose ps</code>, it will look like:</p> <pre><code>  Name                     Command               State            Ports -------------------------------------------------------------------------------------   mysql_1       docker-entrypoint.sh mysqld      Up      0.0.0.0:32769-&gt;3306/tcp </code></pre> <h1><a href='https://docs.docker.com/compose/compose-file/compose-file-v3/#expose' rel='nofollow noreferrer'>Expose</a> is defined as:</h1> <blockquote> <p>Expose ports without publishing them to the host machine - they\u2019ll only be accessible to linked services. Only the internal port can be specified.</p> </blockquote> <p>Ports are not exposed to host machines, only exposed to other services.</p> <pre><code>mysql:   image: mysql:5.7   expose:     - &quot;3306&quot; </code></pre> <p>If I do <code>docker-compose ps</code>, it will look like:</p> <pre><code>  Name                  Command             State    Ports ---------------------------------------------------------------  mysql_1      docker-entrypoint.sh mysqld   Up      3306/tcp </code></pre> <h2>Edit</h2> <p>In recent versions of Dockerfile, <code>EXPOSE</code> <a href='https://docs.docker.com/engine/reference/builder/#expose' rel='nofollow noreferrer'>typically</a> doesn't have any operational impact anymore, it is just informative. (<a href='https://stackoverflow.com/a/65785558/5075502'>see also</a>)</p> "
  },
  {
    "Id": "48727548",
    "PostTypeId": "1",
    "AcceptedAnswerId": "48730146",
    "CreationDate": "2018-02-11T02:03:45.517",
    "Score": "95",
    "ViewCount": "215950",
    "Body": "<p>I'm trying to connect two containers with a docker-compose-yml, but it isn't working. This is my docker-compose.yml file:</p> <pre class='lang-yaml prettyprint-override'><code>version: &quot;3&quot; services:     datapower:         build: .         ports:             - &quot;9090:9090&quot;         depends_on:             - db     db:         image: &quot;microsoft/mssql-server-linux:2017-latest&quot;         environment:             SA_PASSWORD: &quot;your_password&quot;             ACCEPT_EULA: &quot;Y&quot;         ports:         - &quot;1433:1433&quot; </code></pre> <p>When I make:</p> <blockquote> <p>docker-compose up</p> </blockquote> <p>This up my two containers. Then I stop one container and then I run the same container stoped independiently like:</p> <blockquote> <p>docker-compose run -u root --name nameofcontainer 'name of container named in docker-compose.yml'</p> </blockquote> <p>With this, the connection of the containers works. Exists a method to configure my docker-compose.yml to connect my containers like root without stop a container and run independently?</p> ",
    "OwnerUserId": "7288685",
    "LastEditorUserId": "1402846",
    "LastEditDate": "2020-07-16T10:08:25.307",
    "LastActivityDate": "2022-04-05T10:58:56.513",
    "Title": "How to configure docker-compose.yml to up a container as root",
    "Tags": "<docker><docker-compose>",
    "AnswerCount": "2",
    "CommentCount": "1",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 4.0",
    "accepted_answer": "<p><strong>Update:</strong></p> <p>There exists the <code>user</code> property that can be set in the compose file. This is documented in <a href='https://docs.docker.com/compose/compose-file/compose-file-v3/#domainname-hostname-ipc-mac_address-privileged-read_only-shm_size-stdin_open-tty-user-working_dir' rel='noreferrer'>docker-compose file reference</a>.</p> <pre class='lang-yaml prettyprint-override'><code>... services:     datapower:         build: .         user: root         ports:             - &quot;9090:9090&quot;         depends_on:             - db ... </code></pre> ",
    "highest_rated_answer": "<h1>Setting both a User <em>AND</em> a Group in <code>docker-compose.yml</code>:</h1> <p>Discovered another way to set not only the <em><strong>user</strong></em> but also the <em><strong>group</strong></em> in a docker-compose.yml file which is NOT documented in the Docker Compose File Reference @yamenk helpfully provides in the accepted answer.</p> <p>I needed to raise a container expressly setting both a user <strong>AND</strong> a group and found that the <code>user:</code> parameter in <code>docker-compose.yml</code> can be populated as a <strong>UID:GID</strong> mapping delimited by a colon.</p> <p>Below is a snippet from my docker-compose.yml file where this form was tested and found to work correctly:</p> <pre><code>services:  zabbix-agent:   image: zabbix/zabbix-agent2:ubuntu-6.0-latest   container_name: DockerHost1-zabbix-agent2   user: 0:0 &lt;SNIP&gt; </code></pre> <p>Reference:</p> <p><a href='https://github.com/zabbix/zabbix-docker/issues/710' rel='noreferrer'>https://github.com/zabbix/zabbix-docker/issues/710</a></p> <p>Hope this saves others wasted cycles looking for this!</p> "
  },
  {
    "Id": "69054921",
    "PostTypeId": "1",
    "CreationDate": "2021-09-04T11:37:58.750",
    "Score": "94",
    "ViewCount": "137218",
    "Body": "<p>I want to run a docker container for <code>Ganache</code> on my MacBook M1, but get the following error:</p> <pre><code>The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested </code></pre> <p>After this line nothing else will happen anymore and the whole process is stuck, although the qemu-system-aarch64 is running on 100% CPU according to Activity Monitor until I press <kbd>CTRL</kbd>+<kbd>C</kbd>.</p> <p>My docker-files come from <a href='https://github.com/unlock-protocol/unlock/blob/master/docker/docker-compose.override.yml' rel='noreferrer'>this repository</a>. After running into the same issues there I tried to isolate the root cause and came up with the smallest setup that will run into the same error.</p> <p>This is the output of <code>docker-compose up --build</code>:</p> <pre><code>Building ganache Sending build context to Docker daemon  196.6kB Step 1/17 : FROM trufflesuite/ganache-cli:v6.9.1  ---&gt; 40b011a5f8e5 Step 2/17 : LABEL Unlock &lt;ops@unlock-protocol.com&gt;  ---&gt; Using cache  ---&gt; aad8a72dac4e Step 3/17 : RUN apk add --no-cache git openssh bash  ---&gt; Using cache  ---&gt; 4ca6312438bd Step 4/17 : RUN apk add --no-cache   python   python-dev   py-pip   build-base   &amp;&amp; pip install virtualenv  ---&gt; Using cache  ---&gt; 0be290f541ed Step 5/17 : RUN npm install -g npm@6.4.1  ---&gt; Using cache  ---&gt; d906d229a768 Step 6/17 : RUN npm install -g yarn  ---&gt; [Warning] The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested  ---&gt; Running in 991c1d804fdf </code></pre> <p><strong>docker-compose.yml:</strong></p> <pre><code>version: '3.2' services:   ganache:     restart: always     build:       context: ./development       dockerfile: ganache.dockerfile     env_file: ../.env.dev.local     ports:       - 8545:8545    ganache-standup:     image: ganache-standup     build:       context: ./development       dockerfile: ganache.dockerfile     env_file: ../.env.dev.local     entrypoint: ['node', '/standup/prepare-ganache-for-unlock.js']     depends_on:       - ganache </code></pre> <p><strong>ganache.dockerfile:</strong></p> <p>The ganache.dockerfile can be found <a href='https://github.com/unlock-protocol/unlock/blob/master/docker/development/ganache.dockerfile' rel='noreferrer'>here</a>.</p> <p>Running the whole project on an older iMac with Intel-processor works fine.</p> ",
    "OwnerUserId": "6727976",
    "LastEditorUserId": "7487335",
    "LastEditDate": "2022-05-31T22:48:23.297",
    "LastActivityDate": "2023-06-07T08:46:01.750",
    "Title": "Docker on Mac M1 gives: 'The requested image's platform (linux/amd64) does not match the detected host platform'",
    "Tags": "<docker><docker-compose><arm><apple-m1>",
    "AnswerCount": "8",
    "CommentCount": "4",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 4.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p>If you're planning to run the image in your laptop, you need to build it for the cpu architecture of that particular machine. You can provide the <code>--platform</code> option to docker build (or even to <code>docker-compose</code>) to define the target platform you want to build the image for.</p> <p>For example:</p> <pre><code>docker build --platform linux/arm64  . </code></pre> "
  },
  {
    "Id": "32360687",
    "PostTypeId": "1",
    "AcceptedAnswerId": "32361238",
    "CreationDate": "2015-09-02T18:34:18.940",
    "Score": "93",
    "ViewCount": "148754",
    "Body": "<p>I have a docker mysql image running, following is what the docker-compose.yml file looks like:</p>  <pre><code>db:   image: mysql   environment:     MYSQL_ROOT_PASSWORD: ''     MYSQL_ALLOW_EMPTY_PASSWORD: yes   ports:     - '3306:3306' </code></pre>  <p>This works fine.</p>  <p>My question is: How can I connect to the MySQL instance running on that container from the command line mysql client on my the host (my macbook)?</p>  <p>To clarify:</p>  <ul> <li>I have a macbook with Docker installed</li> <li>I have a docker container with mysql</li> <li>I want to connect to the mysql instance running on the aforementioned container from the Terminal on my macbook</li> <li>I do NOT want to user a <code>docker</code> command to make this possible. Rather, I want to use the <code>mysql</code> client directly from the Terminal (without tunneling in through a docker container).</li> </ul>  <p>I don't have MySQL running locally, so port 3306 should be open and ready to use.</p>  <p>The command I am using to start the container is: <code>docker-compose run</code></p> ",
    "OwnerUserId": "507151",
    "LastEditorUserId": "507151",
    "LastEditDate": "2015-09-04T22:34:54.073",
    "LastActivityDate": "2021-01-07T07:22:15.477",
    "Title": "Connect to Docker MySQL container from localhost?",
    "Tags": "<mysql><macos><docker><docker-compose>",
    "AnswerCount": "8",
    "CommentCount": "3",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<h1>Using <code>docker-compose up</code></h1>  <p>Since you published port <code>3306</code> on your <strong>docker host</strong>, from that host itself you would connect to <code>127.0.0.1:3306</code>. </p>  <h1>Using <code>docker-compose run</code></h1>  <p>In that case the port mapping section of the <code>docker-compose.yml</code> file is ignored. To have the port mapping section considered, you have to add the <a href='https://docs.docker.com/compose/reference/run/#/run'><code>--service-ports</code></a> option: </p>  <pre><code>docker-compose run --service-ports db </code></pre>  <h1>Additional note</h1>  <p>Beware that by default, the mysql client tries to connect using a unix socket when you tell it to connect to <code>localhost</code>. So do use <code>127.0.0.1</code> and not <code>localhost</code>:</p>  <pre><code> $ mysql -h 127.0.0.1 -P 3306 -u root </code></pre>  <blockquote>   <p>Welcome to the MySQL monitor.  Commands end with ; or \\g.   Your MySQL connection id is 1   Server version: 5.6.26 MySQL Community Server (GPL)</p>      <p>Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.</p>      <p>Oracle is a registered trademark of Oracle Corporation and/or its   affiliates. Other names may be trademarks of their respective   owners.</p>      <p>Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.</p>      <p>mysql></p> </blockquote>  <pre><code>$ mysql -h localhost -P 3306 -u root </code></pre>  <blockquote>   <p>ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (2)</p> </blockquote> ",
    "highest_rated_answer": "<p>A simple way to login to MySQL inside a Docker image is:</p> <p><code>sudo docker exec -it &lt;CONTAINER_ID&gt; mysql -u root -p</code></p> <p>for mySQL's <code>root</code> account by default password is not set, its BLANK, just press enter/return key, unless you have changed root password.</p> <p>On successful execution, above command gives you mysql prompt.</p> <p>Cheers!</p> "
  },
  {
    "Id": "39204142",
    "PostTypeId": "1",
    "AcceptedAnswerId": "39210054",
    "CreationDate": "2016-08-29T10:35:24.407",
    "Score": "93",
    "ViewCount": "135544",
    "Body": "<p>I'm trying to figure out how to implement docker using docker-compose.yml with 2 databases imported from sql dumps.</p>  <pre><code>httpd:     container_name: webserver     build: ./webserver/     ports:         - 80:80     links:         - mysql         - mysql2     volumes_from:         - app  mysql:     container_name: sqlserver     image: mysql:latest     ports:         - 3306:3306     volumes:         - ./sqlserver:/docker-entrypoint-initdb.d     environment:         MYSQL_ROOT_PASSWORD: root         MYSQL_DATABASE: dbname1         MYSQL_USER: dbuser         MYSQL_PASSWORD: dbpass  mysql2:     extends: mysql     container_name: sqlserver2     environment:         MYSQL_ROOT_PASSWORD: root         MYSQL_DATABASE: dbname2         MYSQL_USER: dbuser         MYSQL_PASSWORD: dbpass  app:     container_name: webdata     image: php:latest     volumes:         - ../php:/var/www/html     command: 'true' </code></pre>  <p>The above returns the following:</p>  <pre><code>Kronos:mybuild avanche$ ./run.sh  Creating sqlserver Creating webdata Creating sqlserver2  ERROR: for mysql2  driver failed programming external connectivity on endpoint sqlserver2 (6cae3dfe7997d3787a8d59a95c1b5164f7431041c1394128c14e5ae8efe647a8): Bind for 0.0.0.0:3306 failed: port is already allocated Traceback (most recent call last):   File '&lt;string&gt;', line 3, in &lt;module&gt;   File 'compose/cli/main.py', line 63, in main AttributeError: 'ProjectError' object has no attribute 'msg' docker-compose returned -1 </code></pre>  <p>Basically, I'm trying to get my whole stack setup in a single docker compose file, create 2 databases and import the respective sql dumps. Anyone have any suggestions?</p> ",
    "OwnerUserId": "1158019",
    "LastEditorUserId": "1158019",
    "LastEditDate": "2016-08-29T11:09:53.443",
    "LastActivityDate": "2024-01-13T04:27:29.530",
    "Title": "docker-compose with multiple databases",
    "Tags": "<database><docker><docker-compose>",
    "AnswerCount": "13",
    "CommentCount": "2",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>Just as an update to anyone else who may look into this. </p>  <p>I solved this by removing:</p>  <pre><code>MYSQL_DATABASE: dbname  </code></pre>  <p>from <strong>docker-compose.yml</strong> and adding the relevant create database statements directly to the sql file being passed to <strong>docker-entrypoint-initdb.d</strong>. </p>  <p>At that stage, sql commands are performed under root, so you'll also need to add a statement to grant relevant permissions to the database user you want to use.</p> ",
    "highest_rated_answer": "<h3>Multiple databases in a single Docker container</h3> <p>The answers elsewhere on this page set up a dedicated container for each database, but a single MySQL server is capable of hosting multiple databases. Whether you should <a href='https://stackoverflow.com/questions/38762709/one-or-multiple-databases-per-docker-container'>is a different question</a>, but if you want multiple databases in a single container, <a href='https://github.com/abagayev/docker-bootstrap-collection/tree/master/mysql-few-databases' rel='noreferrer'>here's an example</a>.</p> <p><em>docker-compose.yml:</em></p> <pre><code>version: '3'  volumes:   db:     driver: local  services:   db:     image: mysql:5.7     command: mysqld --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci     volumes:         - ./docker/provision/mysql/init:/docker-entrypoint-initdb.d     environment:       MYSQL_ROOT_PASSWORD: local </code></pre> <p><em>docker/provision/mysql/init/01-databases.sql:</em></p> <pre><code># create databases CREATE DATABASE IF NOT EXISTS `primary`; CREATE DATABASE IF NOT EXISTS `secondary`;  # create root user and grant rights CREATE USER 'root'@'localhost' IDENTIFIED BY 'local'; GRANT ALL PRIVILEGES ON *.* TO 'root'@'%'; </code></pre> <h3>How does this work?</h3> <p>This works because the <a href='https://github.com/docker-library/mysql' rel='noreferrer'>MySQL Docker project</a> has an <a href='https://github.com/docker-library/mysql/blob/333935aa6612376d58737a8cab0e3f5df370585a/8.0/docker-entrypoint.sh#L197' rel='noreferrer'>entrypoint script</a> that will run through all files in the <code>/docker-entrypoint-initdb.d</code> folder, if it exists. This is useful for setting up databases and initializing their schema and data. In docker-compose, we're using <code>volumes</code> to map that virtual folder to a folder on the host system.</p> "
  },
  {
    "Id": "45109398",
    "PostTypeId": "1",
    "CreationDate": "2017-07-14T18:18:36.217",
    "Score": "93",
    "ViewCount": "130371",
    "Body": "<p>In recent versions <code>docker-compose</code> automatically creates a new network for the services it creates. Basically, every <code>docker-compose</code> setup is getting its own IP range, so that in theory I could call my services on the network's IP address with the predefined ports. This is great when developing multiple projects at the same time, since there is then no need to change the ports in <code>docker-compose.yml</code> (i.e. I can run multiple <code>nginx</code> projects at the same time on port 8080 on different interfaces)</p>  <p>However, this does not work as intended: every exposed port is still exposed on 0.0.0.0 and thus there are port conflicts with multiple projects. It is possible to put the bind IP into <code>docker-compose.yml</code>, however this is a killer for portability -- not every developer on the team uses the same OS or works on the same projects, therefore it's not clear which IP to configure.</p>  <p>It's be great to define the IP to bind the containers to in terms of the network created for this particular project. <code>docker-compose</code> should both know which network it created as well as its IP, so this shouldn't be a problem, however I couldn't find an easy way to do it. Is there a way or is this something yet to be implemented?</p>  <p>EDIT: An example of a port conflict: imagine two projects, each with an application server running on port 8080 and a MySQL database running on port 3306, both respectively exposed as '8080:8080' and '3306:3306'. Running the first one with <code>docker-compose</code> creates a network called something like <code>app1_network</code> with an IP range of 172.18.0.0/16. Every exposed port is exposed on 0.0.0.0, i.e. on 127.0.0.1, on the WAN address, on the default bridge (172.17.0.0/16) and also on the 172.18.0.0/16. In this case I can reach my application server of all of 127.0.0.1:8080, 172.17.0.1:8080, 172.18.0.1:8080 and als on <code>$WAN_IP:8080</code>. If I start the second application now, it starts a second network <code>app2_network</code> 172.19.0.0/16, but still tries to bind every exposed port on all interfaces. Those ports are of course already taken (except for 172.19.0.1). If there had been a possibility to restrict each application to its network, application 1 would have available at 172.18.0.1:8080 and the second at 172.19.0.1:8080 and I wouldn't need to change port mappings to 8081 and 3307 respectively to run both applications at the same time. </p> ",
    "OwnerUserId": "6460",
    "LastEditorUserId": "6460",
    "LastEditDate": "2017-07-14T19:09:04.970",
    "LastActivityDate": "2021-07-05T07:26:48.393",
    "Title": "How can I make docker-compose bind the containers only on defined network instead of 0.0.0.0?",
    "Tags": "<docker><docker-compose>",
    "AnswerCount": "3",
    "CommentCount": "6",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p>In your service configuration, in docker-compose.yml:</p> <pre><code>ports:  - &quot;127.0.0.1:8001:8001&quot; </code></pre> <p>Reference: <a href='https://github.com/compose-spec/compose-spec/blob/master/spec.md#ports' rel='noreferrer'>https://github.com/compose-spec/compose-spec/blob/master/spec.md#ports</a></p> "
  },
  {
    "Id": "53093487",
    "PostTypeId": "1",
    "AcceptedAnswerId": "53101932",
    "CreationDate": "2018-10-31T23:59:03.717",
    "Score": "93",
    "ViewCount": "81032",
    "Body": "<p>How can I specify multi-stage build with in a <code>docker-compose.yml</code>?</p>  <p>For each variant (e.g. dev, prod...) I have a multi-stage build with 2 docker files:</p>  <ul> <li>dev: <code>Dockerfile.base</code> + <code>Dockerfile.dev</code></li> <li>or prod: <code>Dockerfile.base</code> + <code>Dockerfile.prod</code></li> </ul>  <p>File <code>Dockerfile.base</code> (common for all variants):</p>  <pre><code>FROM python:3.6 RUN apt-get update &amp;&amp; apt-get upgrade -y RUN pip install pipenv pip COPY Pipfile ./ # some more common configuration... </code></pre>  <p>File <code>Dockerfile.dev</code>:</p>  <pre><code>FROM flaskapp:base RUN pipenv install --system --skip-lock --dev ENV FLASK_ENV development ENV FLASK_DEBUG 1 </code></pre>  <p>File <code>Dockerfile.prod</code>:</p>  <pre><code>FROM flaskapp:base RUN pipenv install --system --skip-lock ENV FLASK_ENV production </code></pre>  <p>Without docker-compose, I can build as:</p>  <pre><code># Building dev docker build --tag flaskapp:base -f Dockerfile.base . docker build --tag flaskapp:dev -f Dockerfile.dev . # or building prod docker build --tag flaskapp:base -f Dockerfile.base . docker build --tag flaskapp:dev -f Dockerfile.dev . </code></pre>  <p>According to the <a href='https://docs.docker.com/compose/compose-file/#build' rel='noreferrer'>compose-file doc</a>, I can specify a Dockerfile to build.</p>  <pre><code># docker-compose.yml version: '3' services:   webapp:     build:       context: ./dir       dockerfile: Dockerfile-alternate </code></pre>  <p>But how can I specify 2 Dockerfiles in <code>docker-compose.yml</code> (for multi-stage build)?</p> ",
    "OwnerUserId": "3858883",
    "LastActivityDate": "2020-04-30T16:06:48.007",
    "Title": "multi-stage build in docker compose?",
    "Tags": "<docker><docker-compose>",
    "AnswerCount": "2",
    "CommentCount": "2",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 4.0",
    "accepted_answer": "<p>As mentioned in the comments, a multi-stage build involves a single Dockerfile to perform multiple stages. What you have is a common base image.</p>  <p>You could convert these to a non-traditional multi-stage build with a syntax like (I say non-traditional because you do not perform any copying between the layers and instead use just the from line to pick from a prior stage):</p>  <pre><code>FROM python:3.6 as base RUN apt-get update &amp;&amp; apt-get upgrade -y RUN pip install pipenv pip COPY Pipfile ./ # some more common configuration...  FROM base as dev RUN pipenv install --system --skip-lock --dev ENV FLASK_ENV development ENV FLASK_DEBUG 1  FROM base as prod RUN pipenv install --system --skip-lock ENV FLASK_ENV production </code></pre>  <p>Then you can build one stage or another using the <code>--target</code> syntax to build, or a compose file like:</p>  <pre><code># docker-compose.yml version: '3.4' services:   webapp:     build:       context: ./dir       dockerfile: Dockerfile       target: prod </code></pre>  <p>The biggest downside is the current build engine will go through every stage until it reaches the target. Build caching can mean that's only a sub-second process. And BuildKit which is coming out of experimental in 18.09 and will need upstream support from docker-compose will be more intelligent about only running the needed commands to get your desired target built.</p>  <p>All that said, I believe this is trying to fit a square peg in a round hole. The docker-compose developer is encouraging users to move away from doing the build within the compose file itself since it's not supported in swarm mode. Instead, the recommended solution is to perform builds with a CI/CD build server, and push those images to a registry. Then you can run the same compose file with <code>docker-compose</code> or <code>docker stack deploy</code> or even some k8s equivalents, without needing to redesign your workflow.</p> ",
    "highest_rated_answer": "<p>you can use as well concating of docker-compose files, with including both <code>dockerfile</code> pointing to your existing dockerfiles and run <code>docker-compose -f docker-compose.yml -f docker-compose.prod.yml build</code></p> "
  },
  {
    "Id": "70256928",
    "PostTypeId": "1",
    "AcceptedAnswerId": "70257438",
    "CreationDate": "2021-12-07T08:24:21.110",
    "Score": "93",
    "ViewCount": "203823",
    "Body": "<p>I had setup Docker Desktop with Windows WSL integration version 2 and I run into issue when execute certain <code>docker compose</code> command with following errors</p> <pre><code>docker compose logs no configuration file provided: not found </code></pre> <p>However, there were no problem found when executing the following</p> <pre><code>docker compose up </code></pre> <p>and image built and fired up successfully.</p> <p>Is there anyone who can help with this?</p> <hr /> <h3>Output of <code>docker info</code></h3> <pre><code>Client:  Context:    default  Debug Mode: false  Plugins:   buildx: Docker Buildx (Docker Inc., v0.7.1)   compose: Docker Compose (Docker Inc., v2.2.1)   scan: Docker Scan (Docker Inc., 0.9.0)  Server:  Containers: 3   Running: 3   Paused: 0   Stopped: 0  Images: 4  Server Version: 20.10.11  Storage Driver: overlay2   Backing Filesystem: extfs   Supports d_type: true   Native Overlay Diff: true   userxattr: false  Logging Driver: json-file  Cgroup Driver: cgroupfs  Cgroup Version: 1  Plugins:   Volume: local   Network: bridge host ipvlan macvlan null overlay   Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog  Swarm: inactive  Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc  Default Runtime: runc  Init Binary: docker-init  containerd version: 7b11cfaabd73bb80907dd23182b9347b4245eb5d  runc version: v1.0.2-0-g52b36a2  init version: de40ad0  Security Options:   seccomp    Profile: default  Kernel Version: 5.10.60.1-microsoft-standard-WSL2  Operating System: Docker Desktop  OSType: linux  Architecture: x86_64  CPUs: 8  Total Memory: 24.95GiB  Name: docker-desktop  ID: FUMA:ZOXR:BA4L:YSOZ:4NQT:HHIZ:ASAD:EJGA:NJRG:SO4S:GXN3:JG5H  Docker Root Dir: /var/lib/docker  Debug Mode: false  Registry: https://index.docker.io/v1/  Labels:  Experimental: false  Insecure Registries:   127.0.0.0/8  Live Restore Enabled: false  WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support </code></pre> <hr /> <h1>UPDATED</h1> <p>It was my mistake that I should execute the <code>docker compose</code> command under the correct directory where <code>docker-compose.yml</code> file located at.</p> <p>The issue is resolved.</p> ",
    "OwnerUserId": "7928825",
    "LastEditorUserId": "814702",
    "LastEditDate": "2023-10-10T19:09:36.553",
    "LastActivityDate": "2024-02-05T11:30:23.623",
    "Title": "Docker Compose prompted error: no configuration file provided: not found",
    "Tags": "<docker><docker-compose><windows-subsystem-for-linux><docker-desktop>",
    "AnswerCount": "11",
    "CommentCount": "1",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 4.0",
    "accepted_answer": "<p>execute docker-compose command where docker-compose.yml file located at should resolved it.</p> <p>or specify the docker-compose.yml file as bellow</p> <pre><code>docker-compose -f &lt;docker-compose.yml&gt; logs  </code></pre> <p>as suggested</p> ",
    "highest_rated_answer": "<p>For anyone who's getting</p> <pre><code>no configuration file provided: not found </code></pre> <p>I was getting the same message on Mac trying to execute</p> <pre><code>docker-compose up </code></pre> <p>The problem was with docker-compose.yml file. I saved it from Visual Studio Code as a text file though OS determined it like a proper YAML file. Look at the screenshot: left file is wrong but seems the same.</p> <p><a href='https://i.stack.imgur.com/sCtLLm.png' rel='noreferrer'><img src='https://i.stack.imgur.com/sCtLLm.png' alt='enter image description here' /></a></p> <p>Make sure you select YAML format when you saving it.</p> <p><a href='https://i.stack.imgur.com/9PZcUm.png' rel='noreferrer'><img src='https://i.stack.imgur.com/9PZcUm.png' alt='enter image description here' /></a></p> "
  },
  {
    "Id": "33799885",
    "PostTypeId": "1",
    "AcceptedAnswerId": "41841714",
    "CreationDate": "2015-11-19T09:30:17.317",
    "Score": "91",
    "ViewCount": "37389",
    "Body": "<p>Up until recently, when one was doing <code>docker-compose up</code> for a bunch of containers and one of the started containers stopped, all of the containers were stopped. This is not the case anymore since <a href='https://github.com/docker/compose/issues/741' rel='noreferrer'>https://github.com/docker/compose/issues/741</a> and this is a really annoying for us: We use docker-compose to run selenium tests which means starting application server, starting selenium hub + nodes, starting tests driver, then exiting when tests driver stops.</p>  <p>Is there a way to get back old behaviour?</p> ",
    "OwnerUserId": "137871",
    "LastEditorUserId": "11248508",
    "LastEditDate": "2020-06-20T09:05:06.897",
    "LastActivityDate": "2020-06-20T09:05:06.897",
    "Title": "How to stop all containers when one container stops with docker-compose?",
    "Tags": "<docker><docker-compose>",
    "AnswerCount": "4",
    "CommentCount": "0",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>You can use:</p>  <blockquote>   <p>docker-compose up --abort-on-container-exit</p> </blockquote>  <p>Which will stop all containers if one of your containers stops</p> ",
    "highest_rated_answer": "<p>In your docker compose file, setup your <em>test driver</em> container to depend on other containers (with <code>depends_on</code> parameter). Your docker compose file should look like this:</p>  <pre><code>services:   application_server:      ...   selenium:      ...   test_driver:     entry_point: YOUR_TEST_COMMAND     depends_on:       - application_server       - selenium </code></pre>  <p>With dependencies expressed this way, run:</p>  <pre><code>docker-compose run test_driver </code></pre>  <p>and all the other containers will shut down when the <code>test_driver</code> container is finished.</p>  <p><br><br> This solution is an alternative to the <code>docker-compose up --abort-on-container-exit</code> <a href='https://stackoverflow.com/a/41841714/3937850'>answer</a>. The latter will also shut down all other containers if any of them exits (not only the <em>test driver</em>). It depends on your use case which one is more adequate. </p> "
  },
  {
    "Id": "29061026",
    "PostTypeId": "1",
    "CreationDate": "2015-03-15T13:17:51.473",
    "Score": "90",
    "ViewCount": "156707",
    "Body": "<p>I want to create a <strong>docker-compose</strong> file that is able to run on different servers. </p>  <p>For that I have to be able to specify the host-ip or hostname of the server (where all the containers are running) in several places in the <strong>docker-compose.yml</strong>. </p>  <p>E.g. for a consul container where I want to define how the server can be found by fellow consul containers.</p>  <pre><code>consul:   image: progrium/consul   command: -server -advertise 192.168.1.125 -bootstrap </code></pre>  <p>I don't want to hardcode 192.168.1.125 obviously.</p>  <p>I could use  <strong><a href='https://docs.docker.com/compose/env-file/' rel='noreferrer'>env_file</a>:</strong> to specify the hostname or ip and adopt it on every server, so I have that information in one place and use that in docker-compose.yml. But this can only be used to specifiy environment variables and not for the advertise parameter.</p>  <p>Is there a better solution?</p> ",
    "OwnerUserId": "2425939",
    "LastEditorUserId": "598513",
    "LastEditDate": "2019-01-08T10:47:12.313",
    "LastActivityDate": "2023-09-04T15:32:34.940",
    "Title": "Using the host ip in docker-compose",
    "Tags": "<docker><docker-compose>",
    "AnswerCount": "7",
    "CommentCount": "1",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 4.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p>docker-compose allows to use environment variables from the environment running the compose command.</p>  <p>See documentation at <a href='https://docs.docker.com/compose/compose-file/#variable-substitution' rel='noreferrer'>https://docs.docker.com/compose/compose-file/#variable-substitution</a></p>  <p>Assuming you can create a wrapper script, like @balver suggested, you can set an environment variable called <code>EXTERNAL_IP</code> that will include the value of <code>$(docker-machine ip)</code>.</p>  <p>Example:</p>  <pre><code>#!/bin/sh export EXTERNAL_IP=$(docker-machine ip) exec docker-compose $@ </code></pre>  <p>and</p>  <pre><code># docker-compose.yml version: '2' services:   consul:     image: consul     environment:       - 'EXTERNAL_IP=${EXTERNAL_IP}'     command: agent -server -advertise ${EXTERNAL_IP} -bootstrap </code></pre>  <p>Unfortunately if you are using random port assignment, there is no way to add <code>EXTERNAL_PORT</code>, so the ports must be linked statically.</p>  <p>PS: Something very similar is enabled by default in HashiCorp Nomad, also includes mapped ports. Doc: <a href='https://www.nomadproject.io/docs/jobspec/interpreted.html#interpreted_env_vars' rel='noreferrer'>https://www.nomadproject.io/docs/jobspec/interpreted.html#interpreted_env_vars</a></p> "
  },
  {
    "Id": "30040708",
    "PostTypeId": "1",
    "CreationDate": "2015-05-04T21:58:47.083",
    "Score": "90",
    "ViewCount": "74625",
    "Body": "<p>I am trying to use docker-machine with docker-compose. The file docker-compose.yml has definitions as follows:</p>  <pre><code>web:   build: .   command: ./run_web.sh   volumes:     - .:/app   ports:     - '8000:8000'   links:     - db:db     - rabbitmq:rabbit     - redis:redis </code></pre>  <p>When running <code>docker-compose up -d</code> all goes well until trying to execute the command and an error is produced:</p>  <blockquote>   <p>Cannot start container b58e2dfa503b696417c1c3f49e2714086d4e9999bd71915a53502cb6ef43936d: [8] System error: exec: './run_web.sh': stat ./run_web.sh: no such file or directory</p> </blockquote>  <p>Local volumes are not mounted to the remote machine. Whats the recommended strategy to mount the local volumes with the webapps' code?</p> ",
    "OwnerUserId": "1675422",
    "LastEditorUserId": "7225971",
    "LastEditDate": "2017-01-05T13:21:44.913",
    "LastActivityDate": "2019-12-20T02:49:25.207",
    "Title": "How to mount local volumes in docker machine",
    "Tags": "<docker><dockerfile><docker-compose>",
    "AnswerCount": "12",
    "CommentCount": "2",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p>Docker-machine automounts the users directory... But sometimes that just isn't enough.</p>  <p>I don't know about docker 1.6, but in 1.8 you <em>CAN</em> add an additional mount to docker-machine</p>  <h1>Add Virtual Machine Mount Point (part 1)</h1>  <p><strong>CLI</strong>: (Only works when machine is stopped)</p>  <p><code>VBoxManage sharedfolder add &lt;machine name/id&gt; --name &lt;mount_name&gt; --hostpath &lt;host_dir&gt; --automount</code></p>  <p>So an example in windows would be</p>  <pre><code>/c/Program\\ Files/Oracle/VirtualBox/VBoxManage.exe sharedfolder add default --name e --hostpath 'e:\\' --automount </code></pre>  <p><strong>GUI</strong>: (does NOT require the machine be stopped)</p>  <ol> <li>Start 'Oracle VM VirtualBox Manager'</li> <li>Right-Click <code>&lt;machine name&gt;</code> (default)</li> <li>Settings...</li> <li>Shared Folders</li> <li>The Folder+ Icon on the Right (Add Share)</li> <li>Folder Path: <code>&lt;host dir&gt;</code> (e:)</li> <li>Folder Name: <code>&lt;mount name&gt;</code> (e)</li> <li>Check on 'Auto-mount' and 'Make Permanent' (Read only if you want...) (The auto-mount is sort of pointless currently...)</li> </ol>  <h1>Mounting in boot2docker (part 2)</h1>  <p><strong>Manually mount in boot2docker</strong>:</p>  <ol> <li>There are various ways to log in, use 'Show' in 'Oracle VM VirtualBox Manager', or ssh/putty into docker by IP address <code>docker-machine ip default</code>, etc...</li> <li><code>sudo mkdir -p &lt;local_dir&gt;</code></li> <li><code>sudo mount -t vboxsf -o defaults,uid=`id -u docker`,gid=`id -g docker` &lt;mount_name&gt; &lt;local_dir&gt;</code></li> </ol>  <p>But this is only good until you restart the machine, and then the mount is lost...</p>  <p><strong>Adding an automount to boot2docker</strong>: </p>  <p>While logged into the machine</p>  <ol> <li>Edit/create (as root) <code>/mnt/sda1/var/lib/boot2docker/bootlocal.sh</code>, sda1 may be different for you...</li> <li><p>Add</p>  <pre><code>mkdir -p &lt;local_dir&gt; mount -t vboxsf -o defaults,uid=`id -u docker`,gid=`id -g docker` &lt;mount_name&gt; &lt;local_dir&gt; </code></pre></li> </ol>  <p>With these changes, you should have a new mount point. This is one of the few files I could find that is called on boot and is persistent. Until there is a better solution, this should work.</p>  <hr>  <p>Old method: <strong>Less recommended</strong>, but left as an alternative</p>  <ul> <li>Edit (as root) <code>/mnt/sda1/var/lib/boot2docker/profile</code>, sda1 may be different for you...</li> <li><p>Add</p>  <pre><code>add_mount() {   if ! grep -q 'try_mount_share $1 $2' /etc/rc.d/automount-shares ; then     echo 'try_mount_share $1 $2' &gt;&gt; /etc/rc.d/automount-shares   fi }  add_mount &lt;local dir&gt; &lt;mount name&gt; </code></pre></li> </ul>  <p>As a <strong>last resort</strong>, you can take the slightly more tedious alternative, and you can just modify the boot image.</p>  <ul> <li><code>git -c core.autocrlf=false clone https://github.com/boot2docker/boot2docker.git</code></li> <li><code>cd boot2docker</code></li> <li><code>git -c core.autocrlf=false checkout v1.8.1</code> #or your appropriate version</li> <li>Edit <code>rootfs/etc/rc.d/automount-shares</code></li> <li><p>Add <code>try_mount_share &lt;local_dir&gt; &lt;mount_name&gt;</code> line right before fi at the end. For example</p>  <pre><code>try_mount_share /e e </code></pre>  <p>Just be sure not to set the  to anything the os needs, like /bin, etc...</p></li> <li><code>docker build -t boot2docker .</code> #This will take about an hour the first time :(</li> <li><code>docker run --rm boot2docker &gt; boot2docker.iso</code></li> <li>Backup the old boot2docker.iso and copy your new one in its place, in ~/.docker/machine/machines/</li> </ul>  <p>This does work, it's just long and complicated</p>  <p>docker version 1.8.1, docker-machine version 0.4.0</p> "
  },
  {
    "Id": "38086453",
    "PostTypeId": "1",
    "AcceptedAnswerId": "38095099",
    "CreationDate": "2016-06-28T21:17:12.450",
    "Score": "90",
    "ViewCount": "135166",
    "Body": "<p>I have following docker command to run container </p>  <pre><code>docker run -d --name test -v /etc/hadoop/conf:/etc/hadoop/conf -v /usr/lib/python2.7/dist-packages/hdinsight_common:/usr/lib/python2.7/dist-packages/hdinsight_common -v /etc/hive/conf/:/etc/hive/conf/ -v /etc/tez/conf/:/etc/tez/conf/ -v /usr/hdp/2.4.2.0-258/sqoop/lib/:/usr/hdp/2.4.2.0-258/sqoop/lib/ -i -t hdinsight /bin/bash </code></pre>  <p>This was to complicated so I was trying to create docker-compose file like this</p>  <pre><code>version: '2' services:   hdinsight:     image: hdinsight     container_name: ABC     volumes:      - /etc/hadoop/conf:/etc/hadoop/conf      - /usr/lib/python2.7/dist-packages/hdinsight_common:/usr/lib/python2.7/dist-packages/hdinsight_common      - /etc/hive/conf/:/etc/hive/conf/      - /etc/tez/conf/:/etc/tez/conf/      - /usr/hdp/2.4.2.0-258/sqoop/lib/:/usr/hdp/2.4.2.0-258/sqoop/lib/     entrypoint:      - bash     labels:      - 'HDInsight client VM' </code></pre>  <p>But I am not sure where to pass <code>-d</code>, <code>-i</code> &amp; <code>-t</code> flages from my original docker run command</p>  <p>I was running docker-compose like this </p>  <pre><code>docker-compose -f docker-compose.yml run hdinsight </code></pre>  <p>can anyone point me to right direction here ?</p>  <p>UPDATE after first answer </p>  <p>I tried to run <code>docker-compose up -d</code></p>  <pre><code>root@abc-docker:~/ubuntu# docker-compose up -d Creating ABC root@sbd-docker:~/ubuntu# docker ps -a CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                     PORTS               NAMES ffa4c359abf7        hdinsight           '/bin/bash'         5 seconds ago       Exited (0) 5 seconds ago                       ABC root@sbd-docker:~/ubuntu# </code></pre>  <p>Dont know why its in <code>Exited</code> status</p>  <p>Any idea ?</p>  <p>Thanks</p> ",
    "OwnerUserId": "3579198",
    "LastEditorUserId": "3579198",
    "LastEditDate": "2016-06-29T12:27:54.210",
    "LastActivityDate": "2022-01-14T21:46:02.897",
    "Title": "docker-compose for Detached mode",
    "Tags": "<docker><docker-compose>",
    "AnswerCount": "5",
    "CommentCount": "3",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>You should scour the Compose file <a href='https://docs.docker.com/compose/compose-file/' rel='noreferrer'>docs</a>.</p>  <p>Most <em>docker run</em> commands have a compose equivalent, and they should all be listed there.</p>  <p>The background flag -d goes after <em>run</em> or <em>up</em>.</p>  <p>The tty flag -t and interactive flag -i are not required as docker-compose run <a href='https://docs.docker.com/compose/reference/run/' rel='noreferrer'>does this by default</a>. You can add tty to individual containers in the compose file with -t, but you cannot use interactive mode since you may start multiple containers at once and you can't interact with them all.</p>  <p>In regard to your situation the command you're using should be working. If you add -d after the run command it will run in the background. But I recommend using <em>up</em> instead of <em>run</em>, as it will simply start all containers in the file rather than you having to specify hdinsight.</p> ",
    "highest_rated_answer": "<p>As said by Anand Suthar, you have to use <code>tty: true</code> and <code>stdin_open: true</code>. Here is a minimal example:</p>  <pre><code>version: '3' services:   alpine1:     image: alpine     tty: true     stdin_open: true </code></pre>  <p>Start with:</p>  <pre><code>docker-compose up -d </code></pre>  <p>Attach to a container with:</p>  <pre><code>docker attach 268bcfb650fb </code></pre>  <p>and detach with <code>^P^Q</code>.</p> "
  },
  {
    "Id": "47223280",
    "PostTypeId": "1",
    "AcceptedAnswerId": "47336776",
    "CreationDate": "2017-11-10T12:55:06.133",
    "Score": "90",
    "ViewCount": "111645",
    "Body": "<p><strong>Issue</strong>: Can not stop docker containers, whenever I try to stop containers I get the following Error message,</p>  <pre><code>ERROR: for yattyadocker_web_1  cannot stop container: 1f04148910c5bac38983e6beb3f6da4c8be3f46ceeccdc8d7de0da9d2d76edd8: Cannot kill container 1f04148910c5bac38983e6beb3f6da4c8be3f46ceeccdc8d7de0da9d2d76edd8: rpc error: code = PermissionDenied desc = permission denied </code></pre>  <p><strong>OS Version/build:</strong> Ubuntu 16.04 | Docker Version 17.09.0-ce, build afdb6d4 | Docker Compose version 1.17.1, build 6d101fb</p>  <p><strong>Steps to reproduce:</strong></p>  <ul> <li>Created a rails project with Dockerfile and docker-compose.yml. docker-compose.yml is of version 3.</li> <li>Image is built successfully with either <code>docker build -t &lt;project name&gt; .</code> or <code>docker-compose up --build</code></li> <li>Containers boots up and runs successfully.</li> <li>Try to stop docker compose with docker-compose down.</li> </ul>  <p><strong>What I tried:</strong>: </p>  <ul> <li>I have to run <code>sudo service docker restart</code> and then the containers can be removed.</li> <li>Uninstalled docker, removed docker directory and then re installed everything. Still facing same issue. </li> </ul>  <p><strong>Note</strong>: This configuration was working correctly earlier, but somehow file permissions might have changed and I am seeing this error. I have to run <code>sudo service docker restart</code> and then the containers can be removed. But this is highly inconvenient and I don't know how to troubleshoot this.</p>  <p><strong>Reference Files:</strong></p>  <pre><code># docker-compose.yml version: '3' volumes:   db-data:     driver: local   redis-data:     driver: local   services:   db:     image: postgres:9.4.1     volumes:       - db-data:/var/lib/postgresql/data     ports:       - '5432:5432'     env_file: local_envs.env   web:     image: yattya_docker:latest     command: bundle exec puma -C config/puma.rb     tty: true     stdin_open: true     ports:       - '3000:3000'     links:       - db       - redis       - memcached     depends_on:       - db       - redis       - memcached     env_file: local_envs.env   redis:     image: redis:3.2.4-alpine     ports:       # We'll bind our host's port 6379 to redis's port 6379, so we can use       # Redis Desktop Manager (or other tools) with it:       - 6379:6379     volumes:       # We'll mount the 'redis-data' volume into the location redis stores it's data:       - redis-data:/var/lib/redis     command: redis-server --appendonly yes   memcached:     image: memcached:1.5-alpine     ports:       - '11211:11211'   clock:     image: yattya_docker:latest     command: bundle exec clockwork lib/clock.rb     links:       - db     depends_on:       - db     env_file: local_envs.env   worker:     image: yattya_docker:latest     command: bundle exec rake jobs:work     links:        - db     depends_on:        - db     env_file: local_envs.env </code></pre>  <p>And Dockerfile:</p>  <pre><code># Dockerfile FROM ruby:2.4.1  RUN apt-get update &amp;&amp; apt-get install -y nodejs --no-install-recommends &amp;&amp; rm -rf /var/lib/apt/lists/*  ENV APP_HOME /app RUN mkdir -p $APP_HOME WORKDIR $APP_HOME  ADD Gemfile* $APP_HOME/ RUN bundle install  ADD . $APP_HOME  RUN mkdir -p ${APP_HOME}/log RUN cat /dev/null &gt; '$APP_HOME/log/development.log'  RUN mkdir -p ${APP_HOME}/tmp/cache \\     &amp;&amp; mkdir -p ${APP_HOME}/tmp/pids \\     &amp;&amp; mkdir -p ${APP_HOME}/tmp/sockets  EXPOSE 3000 </code></pre> ",
    "OwnerUserId": "4933185",
    "LastEditorUserId": "4933185",
    "LastEditDate": "2017-11-16T11:46:07.410",
    "LastActivityDate": "2024-01-29T12:36:52.313",
    "Title": "Docker Containers can not be stopped or removed - permission denied Error",
    "Tags": "<ruby-on-rails><docker><docker-compose><docker-swarm>",
    "AnswerCount": "10",
    "CommentCount": "7",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>I was able to fix the issue. Apparmor service in ubuntu was not working normally due to some unknown issue. The problem was similar to the issue reported in moby project <a href='https://github.com/moby/moby/issues/20554' rel='noreferrer'>https://github.com/moby/moby/issues/20554</a>. </p>  <p>The <code>/etc/apparmor.d/tunables</code> folder was empty, and <a href='https://github.com/mlaventure' rel='noreferrer'>https://github.com/mlaventure</a> suggested to <strong>purge/reinstall apparmor to get it to the initial state.</strong></p>  <p>So I reinstalled apparmor, and <strong>after restarting</strong> the problem was solved.</p>  <p>Hope this helps.</p> ",
    "highest_rated_answer": "<p>I installed Docker from the snap package and after a while I decided to move to apt repository installation. </p>  <p>I was facing the same problem and using <code>sudo aa-remove-unknown</code> worked for me.</p>  <p>So no reinstallation of Apparmor was needed.</p> "
  },
  {
    "Id": "59296801",
    "PostTypeId": "1",
    "AcceptedAnswerId": "59297469",
    "CreationDate": "2019-12-12T02:21:39.550",
    "Score": "90",
    "ViewCount": "235927",
    "Body": "<p>When I run the following command, I expect the exit code to be 0 since my <code>combined</code> container runs a test that successfully exits with an exit code of 0.</p>  <pre><code>docker-compose up --build --exit-code-from combined </code></pre>  <p>Unfortunately, I consistently receive an exit code of 137 even when the tests in my <code>combined</code> container run successfully and I exit that container with an exit code of 0 (more details on how that happens are specified below).</p>  <p>Below is my docker-compose version:</p>  <pre><code>docker-compose version 1.25.0, build 0a186604 </code></pre>  <p>According to this <a href='https://success.docker.com/article/what-causes-a-container-to-exit-with-code-137' rel='noreferrer'>post</a>, the exit code of 137 can be due to two main issues.</p>  <ol> <li>The container received a <code>docker stop</code> and the app is not gracefully handling SIGTERM</li> <li>The container has run out of memory (OOM).</li> </ol>  <p><em>I know the 137 exit code is not because my container has run out of memory.</em> When I run <code>docker inspect &lt;container-id&gt;</code>, I can see that 'OOMKilled' is false as shown in the snippet below. I also have 6GB of memory allocated to the Docker Engine which is plenty for my application.</p>  <pre><code>[     {         'Id': 'db4a48c8e4bab69edff479b59d7697362762a8083db2b2088c58945fcb005625',         'Created': '2019-12-12T01:43:16.9813461Z',         'Path': '/scripts/init.sh',         'Args': [],         'State': {             'Status': 'exited',             'Running': false,             'Paused': false,             'Restarting': false,             'OOMKilled': false, &lt;---- shows container did not run out of memory             'Dead': false,             'Pid': 0,             'ExitCode': 137,             'Error': '',             'StartedAt': '2019-12-12T01:44:01.346592Z',             'FinishedAt': '2019-12-12T01:44:11.5407553Z'         }, </code></pre>  <p><em>My container doesn't exit from a <code>docker stop</code> so I don't think the first reason is relevant to my situation either.</em></p>  <p><strong>How my Docker containers are set up</strong></p>  <p>I have two Docker containers:</p>  <ol> <li><strong>b-db</strong> - contains my database</li> <li><strong>b-combined</strong> - contains my web application and a series of tests, which run once the container is up and running.</li> </ol>  <p>I'm using a docker-compose.yml file to start both containers.</p>  <pre><code>version: '3' services:     db:         build:             context: .             dockerfile: ./docker/db/Dockerfile         container_name: b-db         restart: unless-stopped         volumes:                  - dbdata:/data/db         ports:             - '27017:27017'         networks:             - app-network      combined:         build:             context: .             dockerfile: ./docker/combined/Dockerfile         container_name: b-combined         restart: unless-stopped         env_file: .env         ports:             - '5000:5000'             - '8080:8080'         networks:             - app-network         depends_on:             - db  networks:     app-network:         driver: bridge  volumes:     dbdata:     node_modules: </code></pre>  <p>Below is the Dockerfile for the <code>combined</code> service in <code>docker-compose.yml</code>.</p>  <pre><code>FROM cypress/included:3.4.1  WORKDIR /usr/src/app  COPY package*.json ./  RUN npm install  COPY . .  EXPOSE 5000  RUN npm install -g history-server nodemon  RUN npm run build-test  EXPOSE 8080  COPY ./docker/combined/init.sh /scripts/init.sh  RUN ['chmod', '+x', '/scripts/init.sh']  ENTRYPOINT [ '/scripts/init.sh' ] </code></pre>  <p>Below is what is in my <code>init.sh</code> file.</p>  <pre><code>#!/bin/bash # Start front end server history-server dist -p 8080 &amp; front_pid=$!  # Start back end server that interacts with DB nodemon -L server &amp; back_pid=$!  # Run tests NODE_ENV=test $(npm bin)/cypress run --config video=false --browser chrome  # Error code of the test test_exit_code=$?  echo 'TEST ENDED WITH EXIT CODE OF: $test_exit_code'  # End front and backend server kill -9 $front_pid kill -9 $back_pid  # Exit with the error code of the test echo 'EXITING SCRIPT WITH EXIT CODE OF: $test_exit_code' exit '$test_exit_code' </code></pre>  <p>Below is the Dockerfile for my <code>db</code> service. All its doing is copying some local data into the Docker container and then initialising the database with this data.</p>  <pre><code>FROM  mongo:3.6.14-xenial  COPY ./dump/ /tmp/dump/  COPY mongo_restore.sh /docker-entrypoint-initdb.d/  RUN chmod 777 /docker-entrypoint-initdb.d/mongo_restore.sh </code></pre>  <p>Below is what is in <code>mongo_restore.sh</code>.</p>  <pre><code>#!/bin/bash # Creates db using copied data mongorestore /tmp/dump </code></pre>  <p>Below are the last few lines of output when I run <code>docker-compose up --build --exit-code-from combined; echo $?</code>.</p>  <pre><code>... b-combined | user disconnected b-combined | Mongoose disconnected b-combined | Mongoose disconnected through Heroku app shutdown b-combined | TEST ENDED WITH EXIT CODE OF: 0 =========================== b-combined | EXITING SCRIPT WITH EXIT CODE OF: 0 ===================================== Aborting on container exit... Stopping b-combined   ... done 137 </code></pre>  <p><strong>What is confusing as you can see above, is that the test and script ended with exit code of 0 since all my tests passed successfully but the container still exited with an exit code of 137.</strong> </p>  <p><strong>What is even more confusing is that when I comment out the following line (which runs my Cypress integration tests) from my <code>init.sh</code> file, the container exits with a 0 exit code as shown below.</strong></p>  <pre><code>NODE_ENV=test $(npm bin)/cypress run --config video=false --browser chrome </code></pre>  <p>Below is the output I receive when I comment out / remove the above line from <code>init.sh</code>, which is a command that runs my Cypress integration tests.</p>  <pre><code>... b-combined | TEST ENDED WITH EXIT CODE OF: 0 =========================== b-combined | EXITING SCRIPT WITH EXIT CODE OF: 0 ===================================== Aborting on container exit... Stopping b-combined   ... done 0 </code></pre>  <p><strong>How do I get docker-compose to return me a zero exit code when my tests run successfully and a non-zero exit code when they fail?</strong></p>  <p><strong>EDIT:</strong> </p>  <p>After running the following docker-compose command in debug mode, I noticed that b-db seems to have some trouble shutting down and potentially is receiving a SIGKILL signal from Docker because of that.</p>  <pre><code>docker-compose --log-level DEBUG up --build --exit-code-from combined; echo $? </code></pre>  <p>Is this indeed the case according to the following output?</p>  <pre><code>... b-combined exited with code 0 Aborting on container exit... http://localhost:None 'GET /v1.25/containers/196f3e622847b4c4c82d8d761f9f19155561be961eecfe874bbb04def5b7c9e5/json HTTP/1.1' 200 None http://localhost:None 'GET /v1.25/containers/json?limit=-1&amp;all=1&amp;size=0&amp;trunc_cmd=0&amp;filters=%7B%22label%22%3A+%5B%22com.docker.compose.project%3Db-property%22%2C+%22com.docker.compose.oneoff%3DFalse%22%5D%7D HTTP/1.1' 200 3819 http://localhost:None 'GET /v1.25/containers/196f3e622847b4c4c82d8d761f9f19155561be961eecfe874bbb04def5b7c9e5/json HTTP/1.1' 200 None http://localhost:None 'GET /v1.25/containers/0626d6bf49e5236440c82de4e969f31f4f86280d6f8f555f05b157fa53bae9b8/json HTTP/1.1' 200 None http://localhost:None 'GET /v1.25/containers/196f3e622847b4c4c82d8d761f9f19155561be961eecfe874bbb04def5b7c9e5/json HTTP/1.1' 200 None http://localhost:None 'GET /v1.25/containers/json?limit=-1&amp;all=0&amp;size=0&amp;trunc_cmd=0&amp;filters=%7B%22label%22%3A+%5B%22com.docker.compose.project%3Db-property%22%2C+%22com.docker.compose.oneoff%3DFalse%22%5D%7D HTTP/1.1' 200 4039 http://localhost:None 'POST /v1.25/containers/196f3e622847b4c4c82d8d761f9f19155561be961eecfe874bbb04def5b7c9e5/attach?logs=0&amp;stdout=1&amp;stderr=1&amp;stream=1 HTTP/1.1' 101 0 http://localhost:None 'GET /v1.25/containers/196f3e622847b4c4c82d8d761f9f19155561be961eecfe874bbb04def5b7c9e5/json HTTP/1.1' 200 None http://localhost:None 'GET /v1.25/containers/196f3e622847b4c4c82d8d761f9f19155561be961eecfe874bbb04def5b7c9e5/json HTTP/1.1' 200 None http://localhost:None 'GET /v1.25/containers/0626d6bf49e5236440c82de4e969f31f4f86280d6f8f555f05b157fa53bae9b8/json HTTP/1.1' 200 None Stopping b-combined   ... Stopping b-db         ... Pending: {&lt;Container: b-db (0626d6)&gt;, &lt;Container: b-combined (196f3e)&gt;} Starting producer thread for &lt;Container: b-combined (196f3e)&gt; http://localhost:None 'GET /v1.25/containers/196f3e622847b4c4c82d8d761f9f19155561be961eecfe874bbb04def5b7c9e5/json HTTP/1.1' 200 None http://localhost:None 'GET /v1.25/containers/196f3e622847b4c4c82d8d761f9f19155561be961eecfe874bbb04def5b7c9e5/json HTTP/1.1' 200 None Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} http://localhost:None 'GET /v1.25/containers/196f3e622847b4c4c82d8d761f9f19155561be961eecfe874bbb04def5b7c9e5/json HTTP/1.1' 200 None Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} Pending: {&lt;Container: b-db (0626d6)&gt;} http://localhost:None 'POST /v1.25/containers/196f3e622847b4c4c82d8d761f9f19155561be961eecfe874bbb04def5b7c9e5/wait HTTP/1.1' 200 32 http://localhost:None 'POST /v1.25/containers/196f3e622847b4c4c82d8d761f9f19155561be961eecfe874bbb04def5b7c9e5/stop?t=10 HTTP/1.1' 204 0 http://localhost:None 'GET /v1.25/containers/196f3e622847b4c4c82d8d761f9f19155561be961eecfe874bbb04def5b7c9e5/json HTTP/1.1' 200 None http://localhost:None 'POST /v1.25/containers/196f3e622847b4c4c82d8d761f9f19155561bStopping b-combined   ... done Finished processing: &lt;Container: b-combined (196f3e)&gt; Pending: {&lt;Container: b-db (0626d6)&gt;} Starting producer thread for &lt;Container: b-db (0626d6)&gt; http://localhost:None 'GET /v1.25/containers/196f3e622847b4c4c82d8d761f9f19155561be961eecfe874bbb04def5b7c9e5/json HTTP/1.1' 200 None http://localhost:None 'GET /v1.25/containers/0626d6bf49e5236440c82de4e969f31f4f86280d6f8f555f05b157fa53bae9b8/json HTTP/1.1' 200 None Pending: set() Pending: set() Pending: set() Pending: set() Pending: set() Pending: set() http://localhost:None 'GET /v1.25/containers/0626d6bf49e5236440c82de4e969f31f4f86280d6f8f555f05b157fa53bae9b8/json HTTP/1.1' 200 None http://localhost:None 'POST /v1.25/containers/0626d6bf49e5236440c82de4e969f31f4f86280d6f8f555f05b157fa53bae9b8/stop?t=10 HTTP/1.1' 204 0 http://localhost:None 'POST /v1.25/containers/0626d6bf49e5236440c82de4e969f31f4f86280d6f8f555f05b157fa53bae9b8/wait HTTP/1.1' 200 30 Stopping b-db         ... done Pending: set() http://localhost:None 'GET /v1.25/containers/0626d6bf49e5236440c82de4e969f31f4f86280d6f8f555f05b157fa53bae9b8/json HTTP/1.1' 200 None http://localhost:None 'GET /v1.25/containers/196f3e622847b4c4c82d8d761f9f19155561be961eecfe874bbb04def5b7c9e5/json HTTP/1.1' 200 None 137 </code></pre> ",
    "OwnerUserId": "7489488",
    "LastEditorUserId": "7489488",
    "LastEditDate": "2019-12-12T04:43:07.133",
    "LastActivityDate": "2023-07-19T15:35:57.000",
    "Title": "Docker-compose exit code is 137 when there is no OOM exception",
    "Tags": "<docker><docker-compose><cypress>",
    "AnswerCount": "9",
    "CommentCount": "0",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 4.0",
    "accepted_answer": "<p>The error message strikes me as: <code>Aborting on container exit...</code></p> <p>From <a href='https://docs.docker.com/compose/reference/up/' rel='noreferrer'>docker-compose docs</a>:</p> <blockquote> <p><strong>--abort-on-container-exit</strong>  Stops all containers if any container was stopped.</p> </blockquote> <p>Are you running docker-compose with this flag? If that is the case, think about what it means.</p> <p>Once <code>b-combined</code> is finished, it simply exits. That means, container <code>b-db</code> will be forced to stop as well. Even though <code>b-combined</code> returned with exit code 0, <code>b-db</code> forced shutdown was likely not handled gracefully by mongodb.</p> <p>EDIT: I just realized you have <code>--exit-code-from</code> in the command line. That implies <code>--abort-on-container-exit</code>.</p> <p><strong>Solution</strong>: <code>b-db</code> needs more time to exit gracefully. Using <code>docker-compose up --timeout 600</code> avoids the error.</p> ",
    "highest_rated_answer": "<p>Docker exit code 137 may imply Docker doesn't have enough RAM to finish the work.</p> <p>Unfortunately Docker consumes a lot of RAM.</p> <p>Go to Docker Desktop app &gt; Preferences &gt; Resources &gt; Advanced and increase the MEMORY - best to double it.</p> "
  },
  {
    "Id": "59715622",
    "PostTypeId": "1",
    "AcceptedAnswerId": "59715738",
    "CreationDate": "2020-01-13T11:20:17.247",
    "Score": "90",
    "ViewCount": "94001",
    "Body": "<p>I have a question. I am pretty new to docker, so what I am trying to do is create a docker-compose file that on compose command will also create the database. Problem is that does not create DB as I ask it nicely. So my docker-compose looks like:</p>  <pre><code>version: '3'  services:   db:     image: postgres:latest     restart: always     ports:       - 5432:5432     environment:       POSTGRES_PASSWORD: 'postgres'     volumes:       - database_data:/var/lib/postgresql/data       - ./init.sql:/docker-entrypoint-initdb.d/init.sql  volumes:   database_data:     driver: local </code></pre>  <p>And when I start docker-compose up in log I can see</p>  <pre><code>db_1  | PostgreSQL Database directory appears to contain a database; Skipping initialization db_1  | db_1  | 2020-01-13 11:14:36.259 UTC [1] LOG:  starting PostgreSQL 12.1 (Debian 12.1-1.pgdg100+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 8.3.0-6) 8.3.0, 64-bit db_1  | 2020-01-13 11:14:36.259 UTC [1] LOG:  listening on IPv4 address '0.0.0.0', port 5432 db_1  | 2020-01-13 11:14:36.260 UTC [1] LOG:  listening on IPv6 address '::', port 5432 db_1  | 2020-01-13 11:14:36.264 UTC [1] LOG:  listening on Unix socket '/var/run/postgresql/.s.PGSQL.5432' db_1  | 2020-01-13 11:14:36.277 UTC [29] LOG:  database system was shut down at 2020-01-13 11:10:57 UTC db_1  | 2020-01-13 11:14:36.280 UTC [1] LOG:  database system is ready to accept connections </code></pre>  <p>In my init SQL there is only 1 line:</p>  <pre><code>CREATE DATABASE 'MyDataBase'; </code></pre>  <p>As I list DB is Postgres container my DB is nowhere to be found. What could be the source root of this problem?</p> ",
    "OwnerUserId": "1662139",
    "LastEditorUserId": "6499760",
    "LastEditDate": "2020-01-13T11:56:29.130",
    "LastActivityDate": "2023-06-29T09:21:21.600",
    "Title": "docker-compose and create db in Postgres on init",
    "Tags": "<docker><docker-compose>",
    "AnswerCount": "7",
    "CommentCount": "0",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 4.0",
    "accepted_answer": "<p>According to the <a href='https://registry.hub.docker.com/_/postgres/' rel='noreferrer'>documentation of postgres docker image</a> you did everything correct.</p>  <blockquote>   <p>If you would like to do additional initialization in an image derived   from this one, add one or more *.sql, *.sql.gz, or *.sh scripts under   /docker-entrypoint-initdb.d (creating the directory if necessary).   After the entrypoint calls initdb to create the default postgres user   and database, it will run any *.sql files, run any executable *.sh   scripts, and source any non-executable *.sh scripts found in that   directory to do further initialization before starting the service.</p> </blockquote>  <p>But, there is a catch which I think you missed based on log that you posted above.</p>  <blockquote>   <p>Warning: <strong>scripts in /docker-entrypoint-initdb.d are only run if you   start the container with a data directory that is empty</strong>; any   pre-existing database will be left untouched on container startup.</p> </blockquote>  <p>So, I would give it a try to empty <code>database_data</code> directory and run again <code>docker-compose up</code>.</p> ",
    "highest_rated_answer": "<p>If, when you start your Docker Compose, you're getting:</p> <pre><code>PostgreSQL Database directory appears to contain a database; Skipping initialization </code></pre> <p>you need to proactively remove the volumes which were set up to store the database.</p> <p>The command <a href='https://docs.docker.com/compose/reference/down/' rel='noreferrer'><code>docker-compose down</code></a> doesn't do this automatically.</p> <p>You can request removal of volumes like this:</p> <pre><code>docker-compose down --volumes </code></pre> "
  },
  {
    "Id": "29564268",
    "PostTypeId": "1",
    "AcceptedAnswerId": "29565333",
    "CreationDate": "2015-04-10T14:40:40.403",
    "Score": "9",
    "ViewCount": "11652",
    "Body": "<p>From my understanding of docker compose / fig, creating a link between two services/images is one main reason if you do not want to exposes ports to others.</p>  <p>like here <strong>db</strong> does not expose any ports and is only linked:</p>  <pre><code>web:   build: .   links:    - db   ports:    - '8000:8000'    db:   image: postgres </code></pre>  <p>Does <strong>web</strong> thinks <strong>db</strong> runs on its localhost? Would i connect from a script/program in <strong>web</strong> to localhost:5432 (standard port from postgresql) to get a database connection? </p>  <p>And if this is correct, how can you change port 5432 to 6432, without exposing? would i just run postgresql on a different port?</p>  <p>Update:</p>  <p>useful links after some input:</p>  <p><a href='http://docs.docker.com/userguide/dockerlinks/'>http://docs.docker.com/userguide/dockerlinks/</a></p>  <p><a href='https://docs.docker.com/compose/yml/#links'>https://docs.docker.com/compose/yml/#links</a></p> ",
    "OwnerUserId": "1584115",
    "LastEditorUserId": "1584115",
    "LastEditDate": "2015-04-11T11:18:50.467",
    "LastActivityDate": "2016-11-22T10:12:20.007",
    "Title": "Understanding ports and links in docker compose",
    "Tags": "<docker><fig><docker-compose>",
    "AnswerCount": "2",
    "CommentCount": "0",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p><strong>web</strong> thinks <strong>db</strong> runs on the host pointed to by the env variable DOCKER_DB or something like that.  Your services should point to that variable (host), not localhost.  </p>  <p>The db container exposes ports (via EXPOSE) to its linked containers, again in variables.  You can run the db on whatever port you want, as long as it's EXPOSEd.</p> ",
    "highest_rated_answer": "<p><code>docker-compose</code> / <code>fig</code> is mainly a workhorse for starting/managing multiple images at once which are somehow dependent on one another.</p>  <p>To fully understand the <strong>links</strong> between containers you should know that there is a host entry created, usually inside <code>/etc/hosts</code> file, which maps that container hostname within docker managed network to a specific ip address. So if you want to access <em>postgres</em> db you have to point to the <code>db</code> hostname instead of <code>localhost</code>.</p>  <p>Regarding exposed ports, most of the images already have some ports exposed, so it might be the case that you just use an image and that port is exposed, you can always override it to something else.</p>  <p><strong>Update in Docker Compose 1.6.0+</strong></p>  <blockquote>   <p>By default Compose sets up a single network for your app. Each container for a service joins the default network and is both reachable by other containers on that network, and discoverable by them at a hostname identical to the container name.   <a href='https://docs.docker.com/compose/networking/#/updating-containers' rel='nofollow noreferrer'>source</a></p> </blockquote>  <p>You can define links between containers only when you want to define an alias to a container, like so:</p>  <pre><code>version: '2' services:   web:     build: .     links:       - 'db:database'   db:     image: postgres </code></pre> "
  },
  {
    "Id": "29729537",
    "PostTypeId": "1",
    "CreationDate": "2015-04-19T11:57:35.533",
    "Score": "9",
    "ViewCount": "7017",
    "Body": "<p>I am trying to use <code>docker-compose up</code> the way you can use <code>docker run [APP_CONTAINER_NAME] [APP_OPTIONS]</code>.</p> ",
    "OwnerUserId": "4793737",
    "LastActivityDate": "2017-02-09T18:00:00.290",
    "Title": "How can I send command line options to my dockerized program that I start with 'docker-compose up'?",
    "Tags": "<ubuntu><docker><fig><docker-compose>",
    "AnswerCount": "3",
    "CommentCount": "2",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p>The point of Docker Compose is that you don't have to remember all your command line switches.</p>  <p>If you want to change environment variables for different contexts, I suggest you create a base <code>common.yml</code> file for Compose. You can then create a new yml file for each different context, inheriting from the <code>common.yml</code> file with the <code>extends</code> instruction. You can then use the <code>-f</code> flag to <code>docker compose</code> to switch between contexts.</p>  <p>Also note that Compose should not 'rebuild' anything if you just change a variable in the yml, and that you can use an external file for environment variables if that works better for you.</p> "
  },
  {
    "Id": "33061464",
    "PostTypeId": "1",
    "AcceptedAnswerId": "33061840",
    "CreationDate": "2015-10-11T04:17:04.123",
    "Score": "9",
    "ViewCount": "3869",
    "Body": "<p>I have a Rails app. In the development and test environments, I want the Rails app to connect to a dockerized Postgres. The Rails app itself will not be in a container though - just Postgres.</p>  <p>What should my database.yml look like?</p>  <p>I have a docker <code>default</code> machine running. I created docker-compose.yml:</p>  <pre><code>postgres:   image: postgres   ports:     - '5432:5432'   environment:     - POSTGRES_USER=timbuktu     - POSTGRES_PASSWORD=mysecretpassword </code></pre>  <p>I ran <code>docker-compose up</code> to get Postgres running.</p>  <p>Then I ran <code>docker-machine ip default</code> to get the IP address of the Docker virtual machine, and I updated database.yml accordingly:</p>  <pre><code>... development:    adapter: postgresql   host: 192.168.99.100   port: 5432   database: timbuktu_development   username: timbuktu   password: mysecretpassword   encoding: unicode   pool: 5 ... </code></pre>  <p>So all is well and I can connect to Postgres in its container. </p>  <p>But, if someone else pulls the repo, they won't be able to connect to Postgres using my database.yml, because the IP address of their Docker default machine will be different from mine. </p>  <p>So how can I change my database.yml to account for this?</p>  <p>One idea I have is to ask them to get the IP address of their Docker default machine by running <code>docker-machine env default</code>, and pasting the env DOCKER_HOST line into their bash_rc. For example,</p>  <pre><code>export DOCKER_HOST='tcp://192.168.99.100:2376' </code></pre>  <p>Then my database.yml host can include the line </p>  <pre><code>host: &lt;%= ENV['DOCKER_HOST'].match(/tcp:\\/\\/(.+):\\d{3,}/)[1] %&gt; </code></pre>  <p>But this feels ugly and hacky. Is there a better way?</p> ",
    "OwnerUserId": "5331394",
    "LastActivityDate": "2024-01-10T09:37:48.147",
    "Title": "How to setup database.yml to connect to Postgres Docker container?",
    "Tags": "<ruby-on-rails><postgresql><docker><docker-compose>",
    "AnswerCount": "2",
    "CommentCount": "1",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>2015: You could set a correct environment variable first, and <a href='https://stackoverflow.com/a/8686483/6309'>access it from your <code>database.yml</code></a>:</p> <pre><code>host: &lt;%= ENV['POSTGRES_IP'] %&gt; </code></pre> <p>With a bashrc like (using <a href='https://stackoverflow.com/a/16623897/6309'>bash substring removal</a>):</p> <pre><code>export DOCKER_HOST=$(docker-machine env default) export POSTGRES_IP=${DOCKER_HOST#tcp://} </code></pre> <hr /> <p>2023: Reminder, the <code>docker machine</code> command, which was part of the Docker Machine tool, has been replaced by <a href='https://www.docker.com/products/docker-desktop/' rel='nofollow noreferrer'>Docker Desktop</a>'s built-in features and <a href='https://docs.docker.com/compose/' rel='nofollow noreferrer'>Docker Compose</a>.<br /> Docker Machine was primarily used to provision and manage Docker hosts (or machines) on virtual environments.</p> <p>The <code>docker-machine env default</code> command was used to set up the shell environment for interacting with a specific Docker machine, named <code>default</code> in this case. That command was particularly useful when managing multiple Docker hosts or when working with Docker running on a virtual machine.</p> <p>In the modern Docker ecosystem, especially with Docker Desktop, the need for such a command is greatly reduced. Docker Desktop runs Docker directly on your host (either Windows or macOS), negating the need for a separate virtual machine to host Docker. So the Docker client is configured by default to communicate with Docker Desktop's daemon without additional environment setup.</p> <p>However, if you are looking to replicate a similar setup where Docker runs in a virtual machine or a remote host, you would configure your Docker client to communicate with that Docker daemon. That is commonly done through environment variables, particularly <code>DOCKER_HOST</code>, <code>DOCKER_CERT_PATH</code>, and <code>DOCKER_TLS_VERIFY</code>.</p> <p>You would need to:</p> <ul> <li><p>set environment variables manually:<br /> Determine the IP address and the port of your remote Docker daemon.<br /> Set the <code>DOCKER_HOST</code> environment variable, and if necessary, <code>DOCKER_CERT_PATH</code> and <code>DOCKER_TLS_VERIFY</code>.</p> <pre class='lang-bash prettyprint-override'><code>export DOCKER_HOST=&quot;tcp://[IP_ADDRESS]:[PORT]&quot; export DOCKER_CERT_PATH=&quot;[path/to/cert]&quot; export DOCKER_TLS_VERIFY=1 </code></pre> </li> <li><p>use SSH to connect to remote docker:<br /> Docker also supports using SSH to connect to a remote Docker daemon.</p> <pre class='lang-bash prettyprint-override'><code>export DOCKER_HOST=&quot;ssh://user@remote-host&quot; </code></pre> </li> <li><p>use <a href='https://docs.docker.com/engine/context/working-with-contexts/' rel='nofollow noreferrer'>Docker contexts</a>:<br /> Docker contexts can be used to manage different Docker endpoints.<br /> Create a new context for your remote Docker daemon:</p> <pre class='lang-bash prettyprint-override'><code>docker context create my-remote-docker --docker &quot;host=tcp://[IP_ADDRESS]:[PORT]&quot; </code></pre> <p>And use the context:</p> <pre class='lang-bash prettyprint-override'><code>docker context use my-remote-docker </code></pre> </li> </ul> <p>Again, in a modern setup, especially for local development, Docker Desktop eliminates the need for most of this configuration.<br /> The steps above are more relevant if you are working with Docker in remote or specialized environments. However, with the evolution of Docker's ecosystem, especially the enhancements in Docker Desktop and Docker Compose, the functionalities of Docker Machine became redundant and less relevant.</p> <p>Docker Desktop now includes features that cover most use cases of Docker Machine. It allows you to manage Docker containers, images, networks, and volumes, as well as build and run multi-container applications defined with Docker Compose. Docker Desktop's integration with cloud providers and its ability to handle local Kubernetes clusters further extend its capabilities beyond what Docker Machine offered.</p> ",
    "highest_rated_answer": "<p>I found a simpler way:</p>  <pre><code>host: &lt;%= `docker-machine ip default` %&gt; </code></pre> "
  },
  {
    "Id": "33230871",
    "PostTypeId": "1",
    "CreationDate": "2015-10-20T07:46:55.680",
    "Score": "9",
    "ViewCount": "11473",
    "Body": "<p>I build 2 dockers, one docker with apache, one docker with php5, and I use docker-compose to start.</p>  <p>apache2 Dockerfile in directoy apache2:</p>  <pre><code>FROM debian:latest RUN apt-get update &amp;&amp; apt-get install -y apache2 ADD test.php /var/www/html  CMD ['/usr/sbin/apache2ctl', '-D', 'FOREGROUND'] </code></pre>  <p>and test.php:</p>  <pre><code>&lt;?php phpinfo(); ?&gt; </code></pre>  <p>php5 Dorckerfile in directory php:</p>  <pre><code>FROM debian:latest RUN apt-get update &amp;&amp; apt-get install -y php5 </code></pre>  <p>docker-compose.yml:</p>  <pre><code>apache:     build: ./apache2     container_name: apache     ports:       - '80:80'     links:       - 'php5'  php5:     build: ./php     container_name: php </code></pre>  <p>then I run:</p>  <pre><code>docker-compose up </code></pre>  <p>apache2 server start successfully. Then I access this server by <a href='http://server_ip' rel='noreferrer'>http://server_ip</a>, then I get index of debian.But when I access <a href='http://server_ip/test.php' rel='noreferrer'>http://server_ip/test.php</a>, just occur this:</p>  <pre><code>&lt;?php phpinfo(); ?&gt; </code></pre>  <p>php just doesn't work.And I don't why.</p> ",
    "OwnerUserId": "4432999",
    "LastActivityDate": "2017-12-03T19:55:43.117",
    "Title": "php docker link apache docker",
    "Tags": "<php><apache><docker><docker-compose>",
    "AnswerCount": "3",
    "CommentCount": "3",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p>You can separate Apache and PHP with PHP-FPM. It is however that the DocumentRoot must be mounted on both containers.</p>  <p>Apache must be able to access the files locally (inside its container) as well as the PHP-FPM server.</p>  <p>I am currently working on the same, have a look at my docker-compose.yml here</p>  <p><a href='https://github.com/cytopia/devilbox/blob/master/docker-compose.yml' rel='noreferrer'>https://github.com/cytopia/devilbox/blob/master/docker-compose.yml</a></p>  <p>Both volumes (in PHP and apache) are mounted to <code>/shared/httpd</code></p> "
  },
  {
    "Id": "34343025",
    "PostTypeId": "1",
    "AcceptedAnswerId": "34345794",
    "CreationDate": "2015-12-17T19:46:08.223",
    "Score": "9",
    "ViewCount": "9980",
    "Body": "<p>When you are running a <strong>docker-compose up -d</strong> command, by default the build image is going to take the name of the current folder.</p>  <p>So for example for this <strong>docker-compose.yml</strong> file my image will be named: <strong>dockersymfony_php</strong> because my current folder is named <strong>docker-symfony</strong> and I indicated <strong>php</strong> for my image in my yaml file</p>  <pre><code>php:     build: ./docker/php     container_name: symfony </code></pre>  <p>Is there a way to override that ? Like <strong>container_name</strong> to renaming the container...</p>  <p>I know that the command <strong>docker build -t  .</strong> can do it but I need to use my <strong>docker-compose.yml</strong> file.</p> ",
    "OwnerUserId": "3676576",
    "LastActivityDate": "2018-01-14T04:14:49.083",
    "Title": "Docker Compose - Image name",
    "Tags": "<docker><docker-compose>",
    "AnswerCount": "2",
    "CommentCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>Currently all you can do is customize the project name (the first part of the name before the underscore) using <code>-p</code> flag:</p>  <p><code>docker-compose -p COMPOSE_PROJECT_NAME</code></p>  <p>In the next release you'll be able to use <code>image</code> to set the name: <a href='https://github.com/docker/compose/issues/2092' rel='noreferrer'>https://github.com/docker/compose/issues/2092</a></p> ",
    "highest_rated_answer": "<p>As of Jan 2018, with version 3 of Docker compose you can use container_name to specify the name. An example can be found here <a href='https://www.handsonarchitect.com/2018/01/docker-compose-tip-how-to-avoid-sql.html' rel='nofollow noreferrer'>https://www.handsonarchitect.com/2018/01/docker-compose-tip-how-to-avoid-sql.html</a></p> "
  },
  {
    "Id": "34441975",
    "PostTypeId": "1",
    "AcceptedAnswerId": "34445379",
    "CreationDate": "2015-12-23T18:52:28.110",
    "Score": "9",
    "ViewCount": "7789",
    "Body": "<p>When my Dockerfile ends with </p>  <pre><code>CMD node . </code></pre>  <p>docker runs that container with the command <code>/bin/sh -c 'node .'</code> instead of simply <code>node .</code> (I know, I could do that with <code>CMD ['node', '.']</code>).</p>  <p>I thought that this behavior is actually nice, since it means that inside the container <code>PID1</code> is <code>/bin/sh</code> and not my humble node script. </p>  <p>If I understand correctly <code>PID1</code> is responsible for reaping orphaned zombie processes, and I don't really wan't to be responsible for that... So if <code>/bin/sh</code> could do that, that would be nice. (I actually thought that this is the reason why docker does rewrite my <code>CMD</code>).</p>  <p>The problem is that when I send a <code>SIGTERM</code> to the container (started with <code>/bin/sh -c 'node .'</code>), either via <code>docker-composer stop</code> or <code>docker-composer kill -s SIGTERM</code>, the signal doesn't reach my <code>node</code> process and therefore it get's forcefully killed everytime with a <code>SIGKILL</code> after the 10 seconds grace period. Not nice.</p>  <p>Is there a way to have someone manage my zombies and have my node instance receive the signals sent by docker?</p> ",
    "OwnerUserId": "913761",
    "LastActivityDate": "2015-12-28T12:05:53.990",
    "Title": "SIGTERM does not reach node script when docker runs it with `/bin/sh -c`",
    "Tags": "<linux><node.js><docker><docker-compose>",
    "AnswerCount": "3",
    "CommentCount": "0",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>There are tools designed to solve this problem:</p>  <ul> <li><a href='https://github.com/yelp/dumb-init' rel='nofollow'>https://github.com/yelp/dumb-init</a></li> <li><a href='https://github.com/krallin/tini' rel='nofollow'>https://github.com/krallin/tini</a></li> </ul>  <p>I think if you only have a single process, all you need to do is explicitly handle the signal with a signal handler, which bash doesn't do for you.</p>  <p>Using the <code>['node', '.']</code> syntax, you could use <a href='https://nodejs.org/api/process.html#process_signal_events' rel='nofollow'>https://nodejs.org/api/process.html#process_signal_events</a> and just have it exit on SIGTERM. I believe that would be enough.</p>  <p>Or using a bash script you can use <code>trap 'exit 0' TERM</code></p>  <p>You could also use a process supervisor like <a href='http://skarnet.org/software/s6/' rel='nofollow'>http://skarnet.org/software/s6/</a></p> ",
    "highest_rated_answer": "<p>I think you have to understand the roles of <code>ENTRYPOINT</code> and <code>CMD</code>, and use the <code>ENTRYPOINT</code>(exec form) way in your <code>Dockerfile</code>.</p>  <p><code>ENTRYPOINT</code>, which specifies the starting executable of the container, is the core part of a Docker container. Every container <strong>MUST</strong> have an entrypoint to decide where to start. By default the value is <code>/bin/bash -c</code>. In addition, everything set by <code>CMD</code> would be appended to <code>ENTRYPOINT</code> as arguments. </p>  <p>Therefore, if you failed to specify <code>ENTRYPOINT</code> in your <code>Dockerfile</code>, the actual entrypoint would be <code>/bin/bash -c {your_command_in_CMD}</code>, which unfortunately <strong>DOES NOT</strong> pass signals.</p>  <p><code>ENTRYPOINT</code> have two forms: <em>exec form</em> and <em>shell form</em></p>  <ul> <li>exec form: ENTRYPOINT ['executable', 'param1', 'param2']</li> <li>shell form: command param1 param2</li> </ul>  <p>As the <a href='https://docs.docker.com/engine/reference/builder/#entrypoint' rel='noreferrer'>Docker reference</a> pointed out: exec form is recommended, and shell form has the disadvantage that <em>command</em> is executed by <code>/bin/bash -c</code>, which might not work well with signals:</p>  <blockquote>   <p>The <em>shell form</em> prevents any <code>CMD</code> or <code>run</code> command line arguments from being used, but has the disadvantage that your <code>ENTRYPOINT</code> will be started as a subcommand of <code>/bin/sh -c</code>, which does not pass signals. This means that the executable will not be the container\u2019s <code>PID 1</code> - and <em>will not</em> receive Unix signals - so your executable will not receive a <code>SIGTERM</code> from <code>docker stop &lt;container&gt;</code>.</p> </blockquote> "
  },
  {
    "Id": "34458042",
    "PostTypeId": "1",
    "CreationDate": "2015-12-24T21:26:13.417",
    "Score": "9",
    "ViewCount": "2472",
    "Body": "<p>I have spent the past few day working on creating a docker swarm on Digtital Ocean. Note: I don't want to use <code>-link</code> to communicate with the other apps/containers becasue they are technically considered deprecated and don't work well with docker swarm (i.e. I can't add more app instances to the load balancer without re composing the entire swarm)</p>  <p>I am using one server as a kv-store server running console according to <a href='https://docs.docker.com/engine/userguide/networking/get-started-overlay/'>this guide</a>. Becasue i'm on Digital Ocean, i'm using private networking on DO so the machines can communicate with each other.</p>  <p>I then create a hive master and slave, and start the overlay network, which is running on all machines. Here is my docker-compose.yml</p>  <pre><code>proxy:     image: tutum/haproxy      ports:         - '1936:1936'         - '80:80'  web:     image: tutum/hello-world     expose:         - '80' </code></pre>  <p>So when I do this it creates the 2 containers. HAProxy is running because I can access the stats at port 1936 at <code>http://&lt;ip-address&gt;:1936</code>, however, when I try to go to the web server/load balancer at port 80 I get connection refused. I everything <em>seems</em> to be connected though, when I run <code>docker-compose ps</code>:</p>  <pre><code>       Name                      Command               State                                 Ports -------------------------------------------------------------------------------------------------------------------------------- splashcloud_proxy_1   python /haproxy/main.py          Up      104.236.109.58:1936-&gt;1936/tcp, 443/tcp, 104.236.109.58:80-&gt;80/tcp splashcloud_web_1     /bin/sh -c php-fpm -d vari ...   Up      80/tcp </code></pre>  <p>The only thing I can think of is that it's not linking to the web container, but i'm not sure how to troubleshoot this.</p>  <p>I'd appreciate any help on this.</p> ",
    "OwnerUserId": "1530554",
    "LastEditorUserId": "1530554",
    "LastEditDate": "2015-12-27T05:44:41.327",
    "LastActivityDate": "2016-06-18T04:26:47.777",
    "Title": "Docker Swarm HAProxy Not Load Balancing w/ Overlay Networking",
    "Tags": "<docker><haproxy><docker-compose><docker-swarm>",
    "AnswerCount": "2",
    "CommentCount": "7",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p>you cannot use the tutum haproxy version here unfortunately. This image is specifically tailored around links. You do need some scripted way of passing the web server ip to haproxy I fear.</p>  <p>But this isn't all that hard :) I would suggest you start from this example: First setup the docker-compose.yml => lets use two nodes, just so you can make sure what you're doing makes sense and actually load balances along the way :)</p>  <pre><code>proxy:     build: ./haproxy/     ports:         - '1936:1936'         - '80:80' web1:     container_name: web1     image: tutum/hello-world     expose:         - '80' web2:     container_name: web2     image: tutum/hello-world     expose:         - '80' </code></pre>  <p>Now with haproxy you need to setup your own Dockerfile according to the official images documentation: <a href='https://hub.docker.com/_/haproxy/'>https://hub.docker.com/_/haproxy/</a></p>  <p>I did this in the haproxy subfolder using the suggested file:</p>  <pre><code>FROM haproxy COPY haproxy.cfg /usr/local/etc/haproxy/haproxy.cfg </code></pre>  <p>then for the haproxy config file haproxy.cfg I tested this:</p>  <pre><code>global     stats socket /var/run/haproxy.stat mode 660 level admin     stats timeout 30s     user root     group root  defaults     mode    http     timeout connect 5000     timeout client  50000     timeout server  50000  frontend localnodes     bind *:80     mode http     default_backend nodes  backend nodes     mode http     balance roundrobin     option forwardfor     http-request set-header X-Forwarded-Port %[dst_port]     http-request add-header X-Forwarded-Proto https if { ssl_fc }     option httpchk HEAD / HTTP/1.1\\r\\nHost:localhost     server web01 172.17.0.2:80     server web02 172.17.0.3:80  listen stats      bind *:1936     mode http     stats enable     stats uri /     stats hide-version     stats auth someuser:password </code></pre>  <p>Obviously the IPs here will only work in the default setup I'm fully aware of this :) You need to do something about those 2 lines:</p>  <pre><code>server web01 172.17.0.2:80 server web02 172.17.0.3:80 </code></pre>  <p>I think you're in luck here working with Digital Ocean :) As far as I understand you do have private IP addresses at your disposal with DO under which you are planning to run the swarm nodes. I suggest to simply put those node IPs instead of my example IPs and run your web servers on them and you're good :)</p> "
  },
  {
    "Id": "34511336",
    "PostTypeId": "1",
    "AcceptedAnswerId": "34513016",
    "CreationDate": "2015-12-29T12:43:17.907",
    "Score": "9",
    "ViewCount": "5315",
    "Body": "<p>I'm having trouble in configuring persistent data with <code>Mariadb</code>. I'm using <code>docker-compose</code>, with each service in a single container (<code>Nginx</code>, <code>PHP-FPM</code> and <code>Mariadb</code>). Everything is working, except <code>Mariadb</code> doesn't store data. Every time I restart the container, I lose all the data. Then I found out that I can use another container just to keep data, and it doesn't even have to be running.</p>  <p>So I'm using, in <code>Mariadb</code> container <code>volume_from</code> content container. But when I do that, when I try to map the volume <code>/var/lib/mysql</code>, the Container <code>MariaDb</code> doesn't start.</p>  <p><strong>Error</strong></p>  <blockquote>   <p>2015-12-29 12:16:40 7f2f02e4a780<br />   InnoDB: Operating system error number 13 in a file operation.<br />   InnoDB: The error means mysqld does not have the access rights to<br />   InnoDB: the directory.</p> </blockquote>  <p>The error refers to a problem about volume permissions, but I've tried to set permissions through <code>Dockerfile</code> in both containers, and the problem persists. I'm a bit lost. I'm using OSX, so I believe this is an OSX problem. Can anyone help me on this?</p>  <p>This is my code:</p>  <p><strong>My Docker Compose</strong></p>  <pre> content:   build: containers/content   container_name: content   hostname: content   volumes:     - /var/lib/mysql mariadb:   build: containers/mariadb   container_name: mariadb   hostname: mariadb   ports:     - '3306:3306'   volumes_from:     - content   environment:     - MYSQL_ROOT_PASSWORD=mariadb     - TERM=xterm     - PORT=3306 </pre>  <p><strong>MariaDB Dockerfile</strong><br></p>  <pre> FROM debian:jessie  RUN apt-get update && apt-get install -y  mariadb-server  EXPOSE 3306 </pre>  <p><strong>Content Dockerfile</strong><br></p>  <pre> FROM debian:jessie  VOLUME /var/lib/mysql  CMD ['true'] </pre> ",
    "OwnerUserId": "5723090",
    "LastEditorUserId": "3393505",
    "LastEditDate": "2015-12-31T20:36:07.067",
    "LastActivityDate": "2016-09-22T18:55:07.623",
    "Title": "Docker-Compose Persistent Data Trouble",
    "Tags": "<macos><docker><mariadb><docker-compose>",
    "AnswerCount": "1",
    "CommentCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>The way i do it is that I use busybox for all data stored and shared with mariadb. Then use <code>--volumes-from</code> in mariadb to link that directories. Please have a look into my simpified <code>compose.yml</code> file.</p>  <pre><code>db-data:   container_name: db-data   image: busybox:latest   volumes:     - /data/mysql:/var/lib/mysql  db:   container_name: db   image: million12/mariadb   restart: always   volumes_from:     - db-data   environment:     - MARIADB_USER=admin     - MARIADB_PASS=my_pass </code></pre>  <p>Now all database files are accessible on host os too and there shouldn't be any permissions issues. </p>  <p>Update for docker-compose 2.0 </p>  <pre><code>version: '2' volumes:   database:  services:   db:     container_name: db     image: million12/mariadb     restart: always     volumes_from:        - database      environment:        - MARIADB_USER=admin        - MARIADB_PASS=my_pass </code></pre>  <p>You can see where docker is storing that volume on your hard drive by running command:<br> <code>docker volume inspect docker_database</code></p>  <pre><code>[ {     'Name': 'docker_database',     'Driver': 'local',     'Mountpoint': '/var/lib/docker/volumes/docker_database/_data',     'Labels': null,     'Scope': 'local' } </code></pre>  <p>]</p> ",
    "highest_rated_answer": null
  },
  {
    "Id": "34537168",
    "PostTypeId": "1",
    "CreationDate": "2015-12-30T20:59:53.813",
    "Score": "9",
    "ViewCount": "26573",
    "Body": "<p>I'm trying to set up a development environment using docker-compose and my container does not seem to have permissions to the host directory that is mounted to the container, i'm getting this error when running a grunt task that tries to modify folders inside the volume:</p>  <pre><code>app_1                   | Warning: Unable to delete '.tmp' file (EACCES, permission denied '.tmp'). Use --force to continue. </code></pre>  <p>here's my docker file:</p>  <pre><code>FROM node:0.10  RUN mkdir -p /usr/src/app WORKDIR /usr/src/app  RUN apt-get update \\     &amp;&amp; apt-get install -y --no-install-recommends ruby-sass \\     &amp;&amp; rm -rf /var/lib/apt/lists/* \\     &amp;&amp; apt-get clean -y \\     &amp;&amp; apt-get autoremove -y  RUN npm install -g grunt-cli bower  RUN groupadd -r node \\ &amp;&amp;  useradd -r -m -g node node  RUN chown -R node:node /usr/src/app  USER node  EXPOSE 8080 </code></pre>  <p>and my docker-compose file:</p>  <pre><code>app:   build: .   dockerfile: Dockerfile.dev   ports:    - '9000:9000'   env_file:    - ./server/config/env/development.env   volumes:    - ./:/usr/src/app:Z   command: bash -c 'npm install &amp;&amp; bower install &amp;&amp; grunt dev &amp;&amp; npm start'  db:   ports:    - '27017:27017' </code></pre>  <ul> <li>I'm running ubuntu 15.10 with docker-compose version 1.5.2, build 7240ff3</li> <li>Note that I am using the :Z permission</li> </ul> ",
    "OwnerUserId": "2458504",
    "LastActivityDate": "2015-12-31T08:38:14.743",
    "Title": "Adding permissions to host directory with docker-compose",
    "Tags": "<docker><docker-compose>",
    "AnswerCount": "1",
    "CommentCount": "4",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p>It's just a file permissions thing.</p>  <p>The first thing to realise is that the volume you are mounting has different permissions to the folder you create and chown in the Dockerfile. The node user presumably doesn't have permissions to access this folder. You can fix this by running something like:</p>  <pre><code>$ docker run -u root -v $(pwd):/usr/src/app:Z my_app_image chown -R node:node /usr/src/app </code></pre>  <p>This will change the permissions of the folder on the host.</p>  <p>Alternatively, if you need to be root to run the <code>npm install &amp;&amp; bower install</code>, you could leave the root user as the default user then change to the node user to run the application. Something like:</p>  <pre><code>npm install &amp;&amp; bower install &amp;&amp; gosu node npm start </code></pre>  <p>Here I've used the <a href='https://github.com/tianon/gosu' rel='nofollow'>gosu</a> tool, which you will need to install in the image. It's a little nicer than sudo, as it doesn't start a second process.</p> "
  },
  {
    "Id": "34833640",
    "PostTypeId": "1",
    "AcceptedAnswerId": "34835504",
    "CreationDate": "2016-01-17T00:13:10.040",
    "Score": "9",
    "ViewCount": "5137",
    "Body": "<p>My docker-compose.yml looks something like this:</p>  <pre><code>django:   build: .   user: django   links:     # LINK TO AMAZON RDS?   command: /gunicorn.sh   env_file: config/settings/.env  nginx:   build: ./compose/nginx   links:     - django   ports:     - '0.0.0.0:80:80' </code></pre>  <p>How do I link the django container to the Amazon RDS, which has an url like: <code>example.blahblahblah.eu-west-1.rds.amazonaws.com:5432</code> </p> ",
    "OwnerUserId": "1051624",
    "LastActivityDate": "2016-01-17T05:42:14.307",
    "Title": "How to link from docker-compose to Amazon RDS",
    "Tags": "<django><amazon-web-services><docker><docker-compose>",
    "AnswerCount": "1",
    "CommentCount": "0",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>In that case, you don't have to define a 'link'; the database service is already running, so all you need to do, is configure your django app to connect to that host.</p>  <p>I don't have experience with django, but based on the <a href='https://docs.docker.com/compose/django/#connect-the-database'>example in the docker-compose documentation</a>, it would look something like;</p>  <pre><code>DATABASES = {     'default': {         'ENGINE': 'django.db.backends.postgresql_psycopg2',         'NAME': 'postgres',         'USER': 'postgres',         'HOST': 'example.blahblahblah.eu-west-1.rds.amazonaws.com',         'PORT': 5432,     } } </code></pre> ",
    "highest_rated_answer": null
  },
  {
    "Id": "35760760",
    "PostTypeId": "1",
    "CreationDate": "2016-03-03T00:12:41.613",
    "Score": "9",
    "ViewCount": "31008",
    "Body": "<p>I have a docker setup with some websites for localhost. I use Smarty as my template engine and it requires to have a writable templates_c folder. Any idea how I can make this folder writable?</p>  <p>The error is as following:</p>  <pre><code>PHP Fatal error:  Smarty error: unable to write to $compile_dir  '/var/www/html/sitename.local/httpdocs/templates_c'.  Be sure $compile_dir is writable by the web server user. in  /var/www/html/sitename.local/httpdocs/libs/Smarty.class.php on  line 1093 </code></pre>  <p>I know this could be set manually with linux but I am looking for an automatic <strong><em>global</em></strong> solution since I have many websites who have this issue</p>  <p>Also worth mentioning I am using a pretty clean docker-compose.yml</p>  <pre><code>php56:   build: .   dockerfile: /etc/docker/dockerfile_php_56   volumes:     - ./sites:/var/www/html     - ./etc/php:/usr/local/etc/php     - ./etc/apache2/apache2.conf:/etc/apache2/conf-enabled/apache2.conf     - ./etc/apache2/hosts.conf:/etc/apache2/sites-enabled/hosts.conf   ports:     - '80:80'     - '8080:8080'   links:     - mysql   mysql:   image: mysql   ports:     - '3306:3306'   environment:     - MYSQL_ROOT_PASSWORD=MY_PASSWORD     - MYSQL_DATABASE=YOUR_DATABASE_NAME   volumes:      - ./etc/mysql:/docker-entrypoint-initdb.d </code></pre>  <p>With a small dockerfile for basics:</p>  <pre><code>FROM php:5.6-apache  RUN /usr/local/bin/docker-php-ext-install mysqli mysql RUN    docker-php-ext-configure mysql --with-libdir=lib/x86_64-linux-gnu/ \\     &amp;&amp; docker-php-ext-install mysql RUN a2enmod rewrite </code></pre>  <p><a href='https://github.com/wesleyd85/docker-php7-httpd-apache2-mysql' rel='noreferrer'>https://github.com/wesleyd85/docker-php7-httpd-apache2-mysql</a> (but then with php 5.6)</p> ",
    "OwnerUserId": "5322269",
    "LastEditorUserId": "5322269",
    "LastEditDate": "2016-03-03T00:24:14.830",
    "LastActivityDate": "2016-11-13T13:21:50.190",
    "Title": "Writable folder permissions in docker",
    "Tags": "<docker><docker-compose>",
    "AnswerCount": "1",
    "CommentCount": "7",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p>I solved the same problem with the solution here: <a href='https://stackoverflow.com/questions/34031397/running-docker-on-ubuntu-mounted-host-volume-is-not-writable-from-container'>Running docker on Ubuntu: mounted host volume is not writable from container</a></p>  <p>Just need to add:</p>  <pre><code>RUN chmod a+rwx -R project-dir/smarty.cache.dir </code></pre>  <p>to Dockerfile</p> "
  },
  {
    "Id": "35955853",
    "PostTypeId": "1",
    "AcceptedAnswerId": "35956974",
    "CreationDate": "2016-03-12T09:20:41.043",
    "Score": "9",
    "ViewCount": "26481",
    "Body": "<p>my OS is : ubuntu:15.10</p>  <p>And i want to use the <strong>official docker-hub redis</strong> , but met problems .</p>  <p>my docker-compose.yml </p>  <pre><code>version: '2' services:    redis:     image: redis     ports:       - '6379:6379'     volumes:       -  ~/dbdata/redis_conf/redis.conf:/usr/local/etc/redis/redis.conf     volumes_from:       -  redisdata     environment:       - REDIS_PASSWORD: 29c4181fb842b5f24a3103dbd2ba17accb1f7e3c8f19868955401ab921     command: redis-server /usr/local/etc/redis/redis.conf  redisdata:     image: redis     volumes:       - /home/keryhu/dbdata/redisdb:/data     command: --break-redis </code></pre>  <p>I copy the default redis.conf to the '~/dbdata/redis_conf/redis.conf' directory . And just modify the 'requirepass' to '29c4181fb842b5f24a3103dbd2ba17accb1f7e3c8f19868955401ab921'</p>  <p>when i start the container ,i met an error -</p>  <pre><code>*** FATAL CONFIG FILE ERROR *** Reading the configuration file, at line 103 &gt;&gt;&gt; 'logfile /var/log/redis/redis-server.log' Can't open the log file: No such file or directory </code></pre>  <p>Can help me ?</p> ",
    "OwnerUserId": "5097551",
    "LastActivityDate": "2018-12-12T08:04:17.470",
    "Title": "docker redis -Can't open the log file: No such file or directory",
    "Tags": "<docker><redis><docker-compose>",
    "AnswerCount": "2",
    "CommentCount": "0",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>As I can see <a href='http://download.redis.io/redis-stable/redis.conf' rel='noreferrer'>here</a>, the log file is not specified and all logs output to the stdout. I wouldn't change this behaviour because in that way docker will manage logs on his own which is standard and even more flexible way.</p>  <p>If you want to redirect logs output somewhere I would suggest using <a href='https://docs.docker.com/compose/compose-file/#logging' rel='noreferrer'><code>logging</code> directive</a> of the composer.</p> ",
    "highest_rated_answer": "<p>I happened to have encountered this problem too.Finally, i touched the redis.log on my host, then mount it on docker. <code> step1: touch /yourself's location/redis-server.log docker run -d -v /yourself's location/redis-server.log:/var/log/redis/redis-server.log -v /your/redis.config:/usr/local/etc/redis/redis.conf --name myredis redis redis-server /usr/local/etc/redis/redis.conf </code> if you want to mount a file or dir,they must exist firstly,otherwise,it occours permission denied or empty dir.</p> "
  },
  {
    "Id": "36120679",
    "PostTypeId": "1",
    "AcceptedAnswerId": "36130772",
    "CreationDate": "2016-03-20T22:58:48.947",
    "Score": "9",
    "ViewCount": "16684",
    "Body": "<p>I run Docker 1.8.1 in OSX 10.11 via an local docker-machine VM.</p>  <p>I have the following docker-compose.yml:</p>  <pre><code>web:     build: docker/web     ports:         - 80:80         - 8080:8080     volumes:         - $PWD/cms:/srv/cms </code></pre>  <p>My Dockerfile looks like this:</p>  <pre><code>FROM alpine  # install nginx and php RUN apk add --update \\     nginx \\     php \\     php-fpm \\     php-pdo \\     php-json \\     php-openssl \\     php-mysql \\     php-pdo_mysql \\     php-mcrypt \\     php-ctype \\     php-zlib \\     supervisor \\     wget \\     curl \\     &amp;&amp; rm -rf /var/cache/apk/*  RUN mkdir -p /etc/nginx &amp;&amp; \\     mkdir -p /etc/nginx/sites-enabled &amp;&amp; \\     mkdir -p /var/run/php-fpm &amp;&amp; \\     mkdir -p /var/log/supervisor &amp;&amp; \\     mkdir -p /srv/cms  RUN rm /etc/nginx/nginx.conf ADD nginx.conf /etc/nginx/nginx.conf ADD thunder.conf /etc/nginx/sites-enabled/thunder.conf  ADD nginx-supervisor.ini /etc/supervisor.d/nginx-supervisor.ini  WORKDIR '/srv/cms' VOLUME '/srv/cms'  EXPOSE 80 EXPOSE 8080 EXPOSE 22  CMD ['/usr/bin/supervisord'] </code></pre>  <p>When I run everything with <code>docker-compose up</code> everything works fine, my volumes are mounted at the correct place.</p>  <p>But the permissions in the mounted folder /srv/cms look wrong. The user is '1000' and the group is '50' in the container. The webserver could not create any files in this folder, because it runs with the user 'root'.</p> ",
    "OwnerUserId": "651764",
    "LastActivityDate": "2023-03-26T09:35:59.530",
    "Title": "Wrong permissions in volume in Docker container",
    "Tags": "<macos><docker><docker-compose><docker-machine>",
    "AnswerCount": "3",
    "CommentCount": "1",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>1) <strong>General idea</strong>: Docker it is not Vagrant. It is wrong to put two different services into one container! Split it into two different images and link them together. Don't do this shitty image.</p>  <p>Check and follow <a href='https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/' rel='noreferrer'>https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/</a></p>  <ul> <li>Avoid installing unnecessary packages</li> <li>Run only one process per container</li> <li>Minimize the number of layers</li> </ul>  <p>If you do it:</p>  <ul> <li>you will remove your supervisor</li> <li>your can decrease numbers of layers</li> </ul>  <p>It should be something like (example):</p>  <pre><code>FROM alpine  RUN apk add --update \\     wget \\     curl RUN apk add --update \\     php \\     php-fpm \\     php-pdo \\     php-json \\     php-openssl \\     php-mysql \\     php-pdo_mysql \\     php-mcrypt \\     php-ctype \\     php-zlib RUN usermod -u 1000 www-data RUN rm -rf /var/cache/apk/*  EXPOSE 9000 </code></pre>  <p>For nginx it is enough to use default image and mount configs. docker-compose file like: </p>  <pre><code>nginx:   image: nginx   container_name: site.dev   volumes:     - ./myconf1.conf:/etc/nginx/conf.d/myconf1.conf     - ./myconf2.conf:/etc/nginx/conf.d/myconf2.conf     - $PWD/cms:/srv/cms   ports:     - '80:80'   links:    - phpfpm phpfpm:   build: ./phpfpm/   container_name: phpfpm.dev   command: php5-fpm -F --allow-to-run-as-root   volumes:     - $PWD/cms:/srv/cms </code></pre>  <p>2) Add <code>RUN usermod -u 1000 www-data</code> into Dockerfile for php container, it will fix problem with permission.</p> ",
    "highest_rated_answer": "<p>For alpine version you need to use:</p>  <pre><code>RUN apk add shadow &amp;&amp; usermod -u 1000 www-data &amp;&amp; groupmod -g 1000 www-data </code></pre> "
  },
  {
    "Id": "36179087",
    "PostTypeId": "1",
    "CreationDate": "2016-03-23T13:08:29.230",
    "Score": "9",
    "ViewCount": "1985",
    "Body": "<p>Currently I'm setting up my app using docker. I've got a minimal rails app, with 1 controller. You can get my setup by running these:</p>  <pre><code>rails new app --database=sqlite --skip-bundle cd app rails generate controller --skip-routes Home index echo 'Rails.application.routes.draw { root 'home#index' }' &gt; config/routes.rb echo 'gem 'foreman'' &gt;&gt; Gemfile echo 'web: rails server -b 0.0.0.0' &gt; Procfile echo 'port: 3000' &gt; .foreman </code></pre>  <p>And I have the following setup:</p>  <p><code>Dockerfile</code>:</p>  <pre><code>FROM ruby:2.3  # Install dependencies RUN apt-get update &amp;&amp; apt-get install -y \\       nodejs \\       sqlite3 \\       --no-install-recommends \\       &amp;&amp; rm -rf /var/lib/apt/lists/*  # Configure bundle RUN bundle config --global frozen 1 RUN bundle config --global jobs 7  # Expose ports and set entrypoint and command EXPOSE 3000 CMD ['foreman', 'start']  # Install Gemfile in different folder to allow caching WORKDIR /tmp COPY ['Gemfile', 'Gemfile.lock', '/tmp/'] RUN bundle install --deployment  # Set environment ENV RAILS_ENV production ENV RACK_ENV production  # Add files ENV APP_DIR /app RUN mkdir -p $APP_DIR COPY . $APP_DIR WORKDIR $APP_DIR  # Compile assets RUN rails assets:precompile VOLUME '$APP_DIR/public' </code></pre>  <p>Where <code>VOLUME '$APP_DIR/public'</code> is creating a volume that's shared with the Nginx container, which has this in the <code>Dockerfile</code>:</p>  <pre><code>FROM nginx  ADD nginx.conf /etc/nginx/nginx.conf </code></pre>  <p>And then <code>docker-compose.yml</code>:</p>  <pre><code>version: '2'  services:   web:     build: config/docker/web     volumes_from:       - app     links:       - app:app     ports:       - 80:80       - 443:443   app:     build: .     environment:       SECRET_KEY_BASE: 'af3...ef0'     ports:       - 3000:3000 </code></pre>  <p>This works, but only the first time I build it. If I change any assets, and build the images again, they're not updated. Possibly because volumes are not updated on image build, I think because how Docker handles caching.</p>  <p>I want the assets to be updated every time I run <code>docker-compose built &amp;&amp; docker-compose up</code>. Any idea how to accomplish this?</p> ",
    "OwnerUserId": "2059127",
    "LastActivityDate": "2016-03-25T16:40:15.030",
    "Title": "Serving Rails' precompiled assets using nginx in Docker",
    "Tags": "<ruby-on-rails><docker><docker-compose>",
    "AnswerCount": "1",
    "CommentCount": "0",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p><a href='https://docs.docker.com/compose/overview/#preserve-volume-data-when-containers-are-created' rel='nofollow'>Compose preserves volumes on recreate</a>.</p>  <p>You have a couple options:</p>  <ol> <li>don't use volumes for the assets, instead build the assets and <code>ADD</code> or <code>COPY</code> them into the web container during build</li> <li><code>docker-compose rm app</code> before running <code>up</code> to remove the old container and volumes.</li> </ol> "
  },
  {
    "Id": "36312699",
    "PostTypeId": "1",
    "AcceptedAnswerId": "36321403",
    "CreationDate": "2016-03-30T15:01:54.303",
    "Score": "9",
    "ViewCount": "37749",
    "Body": "<p>I have the following example</p>  <pre><code>version: '2'  services:   proxy:     container_name: proxy     hostname: proxy     image: nginx     ports:       - 80:80       - 443:443     volumes:       - proxy_conf:/etc/nginx       - proxy_htdocs:/usr/share/nginx/html  volumes:   proxy_conf: {}   proxy_htdocs: {} </code></pre>  <p>which works fine. When I run <code>docker-compose up</code> it creates those named volumes in <code>/var/lib/docker/volumes</code> and all is good. However, from the host, I can only access <code>/var/lib/docker</code> as root, because it's <code>root:root</code> (makes sense). I was wondering if there is a way of <code>chown</code>ing the host's directories to something more sensible/safe (like, my relatively unprivileged user that I use to do most things on the host) or if I just have to suck it up and <code>chown</code> them manually. I'm starting to have a number of scripts already to work around other issues, so having an extra couple of lines won't be much of a problem, but I'd really like to keep my self-written automation minimal, if I can -- fewer chances for stupid mistakes.</p>  <p>By the way, no: if I mount host directories instead of creating volumes, they get overlaid, meaning that if they start empty, they stay empty, and I don't get the default configuration (or whatever) from inside the container.</p>  <p>Extra points: can I just move the volumes to a more convenient location? Say, <code>/home/myuser/myserverstuff/volumes</code>?</p> ",
    "OwnerUserId": "554491",
    "LastActivityDate": "2017-05-17T15:48:03.617",
    "Title": "chown docker volumes on host (possibly through docker-compose)",
    "Tags": "<docker><docker-compose><chown><docker-volume>",
    "AnswerCount": "1",
    "CommentCount": "0",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>It's best to not try to access files inside <code>/var/lib/docker</code> directly. Those directories are meant to be managed by the docker daemon, and not to be messed with.</p>  <p>To access the data inside a volume, there's a number of options;</p>  <ul> <li>use a bind-mounted directory (you considered that, but didn't fit your use case).</li> <li>use a 'service' container that uses the same volume and makes it accessible through that container, for example a container running <code>ssh</code> (to use <code>scp</code>) or a SAMBA container (such as <a href='https://hub.docker.com/r/svendowideit/samba/' rel='noreferrer'>svendowideit/samba</a>)</li> <li>use a <a href='https://docs.docker.com/engine/extend/plugins/' rel='noreferrer'>volume-driver plugin</a>. there's various plugins around that offer all kind of options. For example, the <a href='https://github.com/CWSpear/local-persist' rel='noreferrer'>local persist plugin</a> is a really simple plug-in that allows you to specify <em>where</em> docker should store the volume data (so outside of <code>/var/lib/docker</code>)</li> </ul> ",
    "highest_rated_answer": null
  },
  {
    "Id": "36623457",
    "PostTypeId": "1",
    "AcceptedAnswerId": "36624612",
    "CreationDate": "2016-04-14T12:41:22.950",
    "Score": "9",
    "ViewCount": "3145",
    "Body": "<p>I have 2 containers: <code>web</code> and <code>nginx</code>. When I build <code>web</code> container, static assets for frontend are generated within the container.</p>  <p>Now, I want to share those assets between <code>web</code> and <code>nginx</code> without using a volume on the host machine. Otherwise, I'll have to build those static assets on the host side and then include as a volume into the <code>web</code> container and share it with <code>nginx</code> container. This is undesirable from my build system's standpoint.</p>  <p>Is there a way to build static assets in the <code>web</code> container and then share them with <code>nginx</code>?</p> ",
    "OwnerUserId": "1360544",
    "LastActivityDate": "2016-04-14T13:27:54.820",
    "Title": "Docker: Is it possible to share data between 2 containers without a volume?",
    "Tags": "<nginx><docker><docker-compose>",
    "AnswerCount": "1",
    "CommentCount": "0",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<blockquote>   <p>Otherwise, I'll have to build those static assets on the host side and then include as a volume into the web container and share it with nginx container.</p> </blockquote>  <p>This statement seems incorrect.</p>  <p>If the static assets are generated as part of the build process, then just mount a volume on top of that directory at runtime.  Docker will take care of copying the underlying content into the volume, after which you can access it in your nginx container using <code>--volumes-from</code>.</p>  <p>For example, if I start with this <code>Dockerfile</code> for my web container:</p>  <pre><code>FROM alpine  RUN apk add --update darkhttpd COPY assets /assets CMD ['darkhttpd', '/assets'] </code></pre>  <p>I now have a directory <code>/assets</code> that contains my static assets.  If I run this image as:</p>  <pre><code>docker run -v /assets --name web web </code></pre>  <p>Then <code>/assets</code> will (a) be a volume and (b) contain the contents of the <code>/assets</code> directory.</p>  <p>Now you can start an nginx container and share this data with it:</p>  <pre><code>docker run --volumes-from web nginx </code></pre>  <p>The nginx container will have a <code>/assets</code> directory that contains your static assets.</p>  <p>I've put together a small example <a href='https://github.com/larsks/irc-example-bob' rel='nofollow'>here</a>.</p> ",
    "highest_rated_answer": null
  },
  {
    "Id": "37153053",
    "PostTypeId": "1",
    "CreationDate": "2016-05-11T04:18:40.177",
    "Score": "9",
    "ViewCount": "6422",
    "Body": "<p>I am new to docker and have just started exploring it. What I am trying to achieve is to publish a Docker image, which would just have some configuration files. These files would be shared across multiple projects.</p>  <p><strong>So the image I want to publish is 'my-config-image'.</strong></p>  <hr>  <p><strong>This is how the directory structure is:</strong></p>  <pre><code>/my-config-settings      /folder1/multiple files      /folder2/multiple files      file1.txt      file2.txt </code></pre>  <hr>  <p>After reading online and going through the docker quick start I have got some understanding.</p>  <p>I decided to use Create a simple base image using <code>scratch</code>. referred from (<a href='https://docs.docker.com/engine/userguide/eng-image/baseimages/' rel='noreferrer'>https://docs.docker.com/engine/userguide/eng-image/baseimages/</a>)</p>  <p><strong>This is how my Dockerfile looks.</strong></p>  <pre><code>FROM scratch ADD folder1 / folder2 / </code></pre>  <p>This is the command I am using to publish an image.</p>  <pre><code>/my-config-settings/docker build -t my-config-image . Sending build context to Docker daemon  5.12 kB Step 1 : FROM scratch  ---&gt;  Step 2 : ADD folder1 / folder2 /  ---&gt; Using cache  ---&gt; cc9c3f338f51 Successfully built cc9c3f338f51 </code></pre>  <p>When I want to check if the image was created locally I am confirming it in this way by executing</p>  <pre><code>docker images   REPOSITORY                                          TAG                 IMAGE ID            CREATED             SIZE my-config-image                                     latest              cc9c3f338f51        2 minutes ago       39 B </code></pre>  <p>My main query is how can I check if this is working, Also did this add the file1.txt and file2.txt into the image, if not how do I specify to add these files in the image.</p>  <p>Is there a way I can add all the files and directories recursively into the image I am trying to create?</p>  <p>Also how to I access this image, means how can I check if this image actually has all the folders and files. Is there a way to cd into this?</p>  <p>I am okay with suggestions and looking at another docker approach, if what I am trying to do doesn't make sense.</p>  <p>Thank you for reading.</p> ",
    "OwnerUserId": "3277169",
    "LastEditorUserId": "3277169",
    "LastEditDate": "2016-05-11T04:38:56.257",
    "LastActivityDate": "2016-05-11T04:38:56.257",
    "Title": "Using docker image to store data files",
    "Tags": "<image><docker><docker-compose><dockerfile><docker-registry>",
    "AnswerCount": "1",
    "CommentCount": "10",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p>You're trying to address a configuration management problem with an application management approach. For shared configurations, you generally want to take the 'centralized location approach'. Basic examples of this would be a git repository or an S3 bucket. Both solutions have native document storage and can be appropriately shared between services with fine-grained access control.</p>  <p>A docker image isn't the docker approach to store/share configuration. By having an image, you can basically do two things:<br> 1. Base other images off your initial image<br> 2. Run a container  </p>  <p>Given that your image here is just <code>Scratch</code> and files, there are no executables and nothing to run. The hello world example copied a directory with an actual script in it, which is why they ended up with a runnable container.  </p>  <p>As a base image, configuration doesn't make sense coming before things like dependencies.  </p>  <p>If you really want to use a docker tool for this, you're looking for docker volumes. That can persist a file system and is easy to share around different containers. <a href='https://docs.docker.com/engine/userguide/containers/dockervolumes/' rel='nofollow'>https://docs.docker.com/engine/userguide/containers/dockervolumes/</a></p> "
  },
  {
    "Id": "37463543",
    "PostTypeId": "1",
    "AcceptedAnswerId": "37468342",
    "CreationDate": "2016-05-26T14:20:17.010",
    "Score": "9",
    "ViewCount": "27038",
    "Body": "<p>I want to get started with docker and created a simple container environment with an nginx container, a PHP-FPM container and a MySQL container. </p>  <p>While the link between the nginx and PHP-FPM container works well I can't seem to link the PHP application server with the database server.</p>  <p>I use docker-compose to minimize the manual terminal work. My docker-compose.yml looks like this:</p>  <pre><code>web:   image: tutorial/nginx   ports:     - '8080:80'   volumes:     - ./src:/var/www     - ./src/vhost.conf:/etc/nginx/sites-enabled/vhost.conf   links:     - php php:   image: nmcteam/php56   volumes:     - ./src/php-fpm.conf:/etc/php5/fpm/php-fpm.conf     - ./src:/var/www   links:     - db db:   image: sameersbn/mysql   volumes:    - /var/lib/mysql   environment:    - DB_NAME=demoDb    - DB_USER=demoUser    - DB_PASS=demoPass </code></pre>  <p>While I try to connect to the DB with the following statement:</p>  <pre><code>$db = new \\PDO('mysql:host=db;dbname=demoName', 'demoUser', 'demoPass'); </code></pre>  <p>The MySQL container itself is working as I can connect to the containers bash and use the MySQL CLI:</p>  <pre><code>mysql&gt; show databases;  +--------------------+ | Database           | +--------------------+ | information_schema | | demoDb             | | mysql              | | performance_schema | +--------------------+ </code></pre>  <p>I just get a 500 error and can't find a reason why this wouldn't work. Any help or suggestion of what I might have missed is more than appreciated.</p> ",
    "OwnerUserId": "6290835",
    "LastActivityDate": "2021-07-24T21:06:08.517",
    "Title": "Can't connect to MySQL docker container created via docker-compose",
    "Tags": "<mysql><docker><docker-compose>",
    "AnswerCount": "4",
    "CommentCount": "10",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>This is not a Docker issue but a code issue:  </p>  <p>You have: <code>$db = new \\PDO('mysql:host=db;dbname=demoName', 'demoUser', 'demoPass');</code>   </p>  <p>It should be: <code>$db = new \\PDO('mysql:host=db;port=3306;dbname=demoDb', 'demoUser', 'demoPass');</code></p> ",
    "highest_rated_answer": "<p>If you have <code>0.0.0.0</code> host for your container with MySql and you want to connect with the database from outside docker (WorkBench or SequelPro), just use your docker-machine IP. Example:</p>  <pre><code>user$ docker-machine ls NAME      ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER    ERRORS default   *        virtualbox   Running   tcp://192.168.99.100:2376           v1.12.0 </code></pre>  <p>Then your host to MySQL is <code>192.168.99.100</code> with proper port e.g. <code>3306</code></p> "
  },
  {
    "Id": "38353959",
    "PostTypeId": "1",
    "CreationDate": "2016-07-13T14:04:41.510",
    "Score": "9",
    "ViewCount": "3858",
    "Body": "<p>Does anyone know how (if possible) to run docker-compose commands against a swarm using the new docker 1.12 'swarm mode' swarm? </p>  <p>I know with the previous 'Docker Swarm' you could run docker-compose commands directly against the swarm by updating the DOCKER_HOST to point to the swarm master :<br> <code>export DOCKER_HOST='tcp://123.123.123.123:3375'</code><br> and then simply execute commands as if you were running them against a single instance of Docker engine.</p>  <p>OR is this functionality something that <code>docker-compose bundle</code> is replacing?</p> ",
    "OwnerUserId": "4066662",
    "LastEditorUserId": "4066662",
    "LastEditDate": "2016-07-13T14:06:27.243",
    "LastActivityDate": "2016-09-09T12:36:58.513",
    "Title": "Docker-Compose with Docker 1.12 'Swarm Mode'",
    "Tags": "<docker><docker-compose><docker-swarm>",
    "AnswerCount": "3",
    "CommentCount": "3",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p>I realized my question was vaguely worded and actually has two parts to it. Eventually however, I was able to figure out solutions to both issues. </p>  <h3>1) Can you run commands directly 'against' a swarm / swarm-mode in Docker 1.12 running on a remote machine?</h3>  <p>While you can't really run commands 'against' a swarm you CAN run <code>docker service</code> commands on the master node of a swarm in order to run services on that swarm.<br> You can also configure the Docker daemon (the docker daemon that is the master node of the swarm) to listen on TCP ports in order to externally expose the Docker API. </p>  <h3>2) Can you still use docker-compose files to start services in Docker 1.12 swarm-mode?</h3>  <p>Yes, although these features are currently part of Docker's 'experimental' features. This means you must download/install the version that includes the experimental features (check the github).<br> You essentially follow these instructions  <a href='https://github.com/docker/docker/blob/master/experimental/docker-stacks-and-bundles.md' rel='noreferrer'>https://github.com/docker/docker/blob/master/experimental/docker-stacks-and-bundles.md</a><br> to go from the docker-compose.yml file to a distributed application bundle and then to an application stack (this is when your services are actually run).<br> <code>$ docker-compose bundle</code><br> <code>$ docker deploy [OPTIONS] STACK</code></p>  <h3>Here's what I did:</h3>  <ol> <li><p>On my remote swarm manager node I started docker with the following options: </p>  <p><code>docker daemon -D -H unix:///var/run/docker.sock -H tcp://0.0.0.0:2375 &amp;</code>   </p>  <p>This configures Docker daemon to listen on the standard docker socket <code>unix:///var/run/docker.sock</code> AND on <code>localhost:2375</code>.<br> WARNING : I'm not enabling TLS here just for simplicity</p></li> <li><p>On my local machine I update the docker host environment variable to point at my swarm master node.<br> <code>$ export DOCKER_HOST='tcp://XX.XX.XX.XX:2377'</code> (populate with your IP)  </p></li> <li><p>Navigate to the directory of my docker-compose.yml file  </p></li> <li><p>Create a bundle file from my docker-compose.yml file. Make sure to include the <code>.dab</code> extension. <code>docker-compose bundle --fetch-digests -o myNewBundleFile.dab</code>  </p></li> <li><p>Create an application stack from the bundle file. Do not specify the <code>.dab</code> extension here.<br> <code>$ docker deploy myNewBundleFile</code></p></li> </ol>  <p>Now I'm still experiencing some networking related issues but I have successfully gotten my service up and running from my unmodified docker-compose.yml files. The network issues I'm experiencing is documented here : <a href='https://github.com/docker/docker/issues/23901' rel='noreferrer'>https://github.com/docker/docker/issues/23901</a></p> "
  },
  {
    "Id": "38663834",
    "PostTypeId": "1",
    "CreationDate": "2016-07-29T16:50:16.247",
    "Score": "9",
    "ViewCount": "17341",
    "Body": "<p>I try to build containers with a docker-compose.yml file :</p>  <pre><code>version: '2'  services:      geonode:         build:              context: .         hostname: geonode         container_name: geonode         ports:             - 8000:8000         volumes:             - .:/geonode/         entrypoint:             - /usr/bin/python         command: manage.py runserver 0.0.0.0:8000         network_mode: host </code></pre>  <p>In my Dockerfile I run <code>apt-get update</code> after <code>FROM ubuntu:14.04</code> but it fails : <code>Could not resolve 'archive.ubuntu.com'</code></p>  <p>I tried <code>docker run -i -t --net=host ubuntu:14.04 /bin/bash</code> and then run <code>apt-get update</code> and it works. So it seems to me that the network_mode in docker-compose and the <code>--net=host</code> with docker run don't work the same way.</p>  <p>Does somebody has an explanation?</p> ",
    "OwnerUserId": "2886075",
    "LastActivityDate": "2022-04-05T14:27:58.487",
    "Title": "docker-compose network_mode: 'host' doesn't seem to work",
    "Tags": "<docker><docker-compose>",
    "AnswerCount": "2",
    "CommentCount": "1",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p>It works at least in version 3.7 when doing:</p> <pre><code>services:     my-app:         container_name: my-app         build:              context: .             network: host         network_mode: host         command: /app/my-app </code></pre> <p>The ports are obsolete as all ports are &quot;exposed&quot;.</p> "
  },
  {
    "Id": "39326602",
    "PostTypeId": "1",
    "AcceptedAnswerId": "39329600",
    "CreationDate": "2016-09-05T08:31:28.837",
    "Score": "9",
    "ViewCount": "17206",
    "Body": "<p>Trying to run cluster application on different virtual machines with use of Swarm stand alone and docker-compose version '2'. Overlay network is set. But want to force certain containers to run on specific hosts. </p>  <p>In documentation there is following advice, but with this parameter I was not able to start any container at all:</p>  <pre><code>environment:   - 'constraint:node==node-1'  ERROR: for elasticsearch1  Cannot create container for service elasticsearch1: Unable to find a node that satisfies the following conditions [available container slots] [node==node-1] </code></pre>  <p>Should we register hosts as node-1 node-2... or it is done by default.</p>  <pre><code>[root@ux-test14 ~]# docker node ls Error response from daemon: 404 page not found [root@ux-test14 ~]# docker run swarm list [root@ux-test14 ~]#    [root@ux-test14 ~]# docker info Containers: 8  Running: 6  Paused: 0  Stopped: 2 Images: 8 Server Version: swarm/1.2.5 Role: primary Strategy: spread Filters: health, port, containerslots, dependency, affinity, constraint Nodes: 2  ux-test16.rs: 10.212.212.2:2375   \u00e2 ID: JQPG:GKFF:KJZJ:AY3N:NHPZ:HD6J:SH36:KEZR:2SSH:XF65:YW3N:W4DG   \u00e2 Status: Healthy   \u00e2 Containers: 4 (4 Running, 0 Paused, 0 Stopped)   \u00e2 Reserved CPUs: 0 / 2   \u00e2 Reserved Memory: 0 B / 3.888 GiB   \u00e2 Labels: kernelversion=3.10.0-327.28.3.el7.x86_64, operatingsystem=CentOS Linux 7 (Core), storagedriver=devicemapper   \u00e2 UpdatedAt: 2016-09-05T11:11:31Z   \u00e2 ServerVersion: 1.12.1  ux-test17.rs: 10.212.212.3:2375   \u00e2 ID: Z27V:T5NU:QKSH:DLNK:JA4M:V7UX:XYGH:UIL6:WFQU:FB5U:J426:7XIR   \u00e2 Status: Healthy   \u00e2 Containers: 4 (2 Running, 0 Paused, 2 Stopped)   \u00e2 Reserved CPUs: 0 / 2   \u00e2 Reserved Memory: 0 B / 3.888 GiB   \u00e2 Labels: kernelversion=3.10.0-327.28.3.el7.x86_64, operatingsystem=CentOS Linux 7 (Core), storagedriver=devicemapper   \u00e2 UpdatedAt: 2016-09-05T11:11:17Z   \u00e2 ServerVersion: 1.12.1 Plugins:  Volume:  Network: Swarm:  NodeID:  Is Manager: false  Node Address: Security Options: Kernel Version: 3.10.0-327.28.3.el7.x86_64 Operating System: linux Architecture: amd64 CPUs: 4 Total Memory: 7.775 GiB Name: 858ac2fdd225 Docker Root Dir: Debug Mode (client): false Debug Mode (server): false WARNING: No kernel memory limit support </code></pre> ",
    "OwnerUserId": "3292147",
    "LastEditorUserId": "3292147",
    "LastEditDate": "2016-09-05T11:36:16.787",
    "LastActivityDate": "2021-10-10T15:24:54.827",
    "Title": "docker-compose swarm: force containers to run on specific hosts",
    "Tags": "<docker><docker-compose><docker-swarm>",
    "AnswerCount": "3",
    "CommentCount": "3",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>My first answer is about 'swarm mode'. You'd since clarified that you're using legacy Swarm and added more info, so here:</p>  <p>The constraint you list assumes that you have a host named <code>node-1</code>. Your hosts are named <code>ux-test16.rs</code> and <code>ux-test17.rs</code>. Just use that instead of <code>node-1</code> in your constraint. Eg:</p>  <pre><code>environment:   - 'constraint:node==ux-test16.rs' </code></pre> ",
    "highest_rated_answer": "<p>The <strong>environment variable</strong> <code>constraint</code> is only valid for the legacy (stand alone) version of Swarm. The newer 'Swarm Mode' uses either <code>mode</code> or <code>constraints</code> options (not environment variables).</p>  <p>To enforce one and only one task (container) per node, use <code>mode=global</code>.</p>  <pre><code>docker service create --name proxy --mode global nginx </code></pre>  <p>The default mode is <code>replicated</code> which means that the swarm manager will create tasks (containers) across all available nodes to meet the number specified in the <code>--replicas</code> option. Eg:</p>  <pre><code>docker service create --name proxy --replicas 5 nginx </code></pre>  <p>To enforce other constraints based on hostname (node), label, role, id's use the <code>--constraint</code> option. Eg:</p>  <pre><code>docker service create --name proxy --constraint 'node.hostname!=node01' nginx </code></pre>  <p>See <a href='https://docs.docker.com/engine/reference/commandline/service_create/#/specify-service-constraints' rel='nofollow noreferrer'>https://docs.docker.com/engine/reference/commandline/service_create/#/specify-service-constraints</a></p>  <p>EDIT sept 2016:</p>  <p>Something else. <code>docker-compose</code> is not currently supported in 'swarm mode'. Swarm mode understands the new <a href='https://blog.docker.com/2016/06/docker-app-bundle/' rel='nofollow noreferrer'><code>dab</code></a> format instead. There is a way to convert docker-compose files to dab but it's experimental and not to be relied on at this point. It's better to create a bash script that calls all the <code>docker service create ...</code> directly.</p>  <p>EDIT March 2017:</p>  <p>As of docker 1.13 (17.03), docker-compose can now be used to provision swarm environments directly without having to deal with the dab step.</p> "
  },
  {
    "Id": "39331665",
    "PostTypeId": "1",
    "CreationDate": "2016-09-05T13:27:29.483",
    "Score": "9",
    "ViewCount": "6837",
    "Body": "<p>I am trying to build a project using maven on teamcity and getting this error during maven build step.</p>  <blockquote>   <p>[Step 2/4] [ERROR] protoc failed output:<br>   [Step 2/4] [ERROR] protoc failed error: /bin/sh: 1: protoc: Permission denied   [Step 2/4]  [13:03:14][Step 2/4] Failed to execute goal   com.google.protobuf.tools:maven-protoc-plugin:0.1.10:compile   (generate-sources) on project unit-protocol-lib: protoc did not exit   cleanly. Review output for more information.</p> </blockquote>  <p>Keep in mind I am using docker-compose for building the teamcity agent (agent running in container) and protoc is added to /usr/local/bin/protoc ($PATH has /usr/local/bin, /usr/local/bin/protoc has rwx permissions).</p>  <p>EDITED for ease</p>  <p>Forget everything above for a while.<br> I logged into the buildagent of teamcity server, access the shell using <code>/bin/sh</code> and execute the command <code>protoc</code> and it returns the error:<br> <code>protoc failed error: /bin/sh: 1: protoc: Permission denied</code>  </p>  <p>Any help??</p> ",
    "OwnerUserId": "1690527",
    "LastEditorUserId": "1690527",
    "LastEditDate": "2016-09-09T10:04:03.913",
    "LastActivityDate": "2022-10-07T11:25:37.037",
    "Title": "Permission denied for protoc on maven build in Teamcity",
    "Tags": "<maven><ubuntu><docker><teamcity><docker-compose>",
    "AnswerCount": "2",
    "CommentCount": "4",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p>I had the same issue.</p>  <p>What I found was that the error message was misleading.</p>  <p>Here's what worked for me:</p>  <p>Try this:</p>  <pre><code>protoc ./proto/hello/hello.proto --go_out=plugins=grpc:./outputDirectory -I ./proto/hello/hello.proto </code></pre>  <p>Parts of the command obviously look redundant, but this was what I had to do to get it working.  I recommend trying this, and see if it runs.  If it does then you can see if you're able to tweak it, but I don't think so.  </p>  <p>if '.' is your output, then do this:</p>  <pre><code>protoc ./proto/hello/hello.proto --go_out=plugins=grpc:. -I ./proto/hello/hello.proto </code></pre>  <p>Notice that you don't need space.</p> "
  },
  {
    "Id": "39439689",
    "PostTypeId": "1",
    "AcceptedAnswerId": "48448277",
    "CreationDate": "2016-09-11T19:09:56.353",
    "Score": "9",
    "ViewCount": "3061",
    "Body": "<p>Quite often, when I start my docker-composed app, I like to check that everything started correctly and everything's fine.</p>  <p>So I do <code>docker-compose up</code>, look at the logs, and then I have to do <code>docker-compose stop</code>, and <code>docker-compose -d up</code>.</p>  <p>Those are too many steps and having to stop the container means downtime on my server.</p>  <p>Ain't there a way to <em>send docker to the background</em>?</p>  <p>I tried <code>Ctrl+Z</code> but then if I try to exit the ssh session, I get <code>There are stopped jobs.</code>, so that's not the correct way to do this.</p>  <p>I use docker-compose, but I'd be curious if this is possible with docker also.</p>  <p>Thanks</p> ",
    "OwnerUserId": "1620081",
    "LastActivityDate": "2022-04-25T12:48:07.827",
    "Title": "Docker (compose) send to daemon mode without restart",
    "Tags": "<docker><daemon><docker-compose>",
    "AnswerCount": "1",
    "CommentCount": "4",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>After <code>Ctrl+z</code>, just use <code>bg</code>, the task will start running on background and you are safe to close the ssh session.</p> ",
    "highest_rated_answer": null
  },
  {
    "Id": "39457241",
    "PostTypeId": "1",
    "AcceptedAnswerId": "39594533",
    "CreationDate": "2016-09-12T19:16:59.323",
    "Score": "9",
    "ViewCount": "3017",
    "Body": "<p>I have a couple of compose files (docker-compose.yml) describing a simple Django application (five containers, three images).</p>  <p>I want to run this stack in production - to have the whole stack begin on boot, and for containers to restart or be recreated if they crash. There aren't any volumes I care about and the containers won't hold any important state and can be recycled at will.</p>  <p>I haven't found much information on using specifically docker-compose in production in such a way. <a href='https://docs.docker.com/compose/production/' rel='noreferrer'>The documentation</a> is helpful but doesn't mention anything about starting on boot, and I am using Amazon Linux so don't (currently) have access to Docker Machine. I'm used to using supervisord to babysit processes and ensure they start on boot up, but I don't think this is the way to do it with Docker containers, as they end up being ultimately supervised by the Docker daemon?</p>  <p>As a simple start I am thinking to just put <code>restart: always</code> on all my services and make an init script to do <code>docker-compose up -d</code> on boot. Is there a recommended way to manage a docker-compose stack in production in a robust way?</p>  <p>EDIT: I'm looking for a 'simple' way to run the equivalent of <code>docker-compose up</code> for my container stack in a robust way. I know upfront that all the containers declared in the stack can reside on the same machine; in this case I don't have need to orchestrate containers from the same stack across multiple instances, but that would be helpful to know as well.</p> ",
    "OwnerUserId": "2387626",
    "LastEditorUserId": "2387626",
    "LastEditDate": "2016-09-18T16:53:29.983",
    "LastActivityDate": "2016-09-24T22:08:22.773",
    "Title": "Recommended way to run a Docker Compose stack in production?",
    "Tags": "<docker><docker-compose><devops>",
    "AnswerCount": "4",
    "CommentCount": "2",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>Compose is a client tool, but when you run <code>docker-compose up -d</code> all the container options are sent to the Engine and stored. If you specify <code>restart</code> as <code>always</code> (or preferably <code>unless-stopped</code> <a href='https://docs.docker.com/engine/reference/run/#/restart-policies-restart' rel='noreferrer'>to give you more flexibility</a>) then you don't need run <code>docker-compose up</code> every time your host boots.</p>  <p>When the host starts, provided you have <a href='https://docs.docker.com/engine/admin/' rel='noreferrer'>configured the Docker daemon</a> to start on boot, Docker will start all the containers that are flagged to be restarted. So you only need to run <code>docker-compose up -d</code> <strong>once</strong> and Docker takes care of the rest.</p>  <p>As to orchestrating containers across multiple nodes in a Swarm - the preferred approach will be to use <a href='https://docs.docker.com/compose/bundles/' rel='noreferrer'>Distributed Application Bundles</a>, but that's currently (as of Docker 1.12) experimental. You'll basically create a bundle from a local Compose file which represents your distributed system, and then deploy that remotely to a Swarm. Docker moves fast, so I would expect that functionality to be available soon.</p> ",
    "highest_rated_answer": "<p>You can find in their <a href='https://docs.docker.com/v1.8/compose/production/' rel='nofollow'>documentation</a> more information about using docker-compose in production. But, as they mention, <code>compose</code> is primarily aimed at development and testing environments.</p>  <p>If you want to use your containers in production, I would suggest you to use a suitable tool to orchestrate containers, as <a href='http://kubernetes.io/' rel='nofollow'>Kubernetes</a>.</p> "
  },
  {
    "Id": "39918762",
    "PostTypeId": "1",
    "CreationDate": "2016-10-07T13:43:43.997",
    "Score": "9",
    "ViewCount": "7154",
    "Body": "<p>What I want to do is to use a dump.rdb that I've taken from a production server, and use it in my development environment, that is defined by a very simple compose file. </p>  <p>For simplicity, assume that my app is the same as this <a href='https://docker.github.io/compose/gettingstarted/' rel='noreferrer'>compose example from the docker docs</a> for redis and flask, so the docker-compose.yml looks like: </p>  <pre><code>version: '2'  services:    web:      build: .      ports:       - '5000:5000'      volumes:       - .:/code      depends_on:       - redis    redis:      image: redis </code></pre>  <p>This persists redis data between restarts, but you just cannot access the redis files as there is no volume mounted for redis in the docker-compose.yml.  So I change my compose file to mount a volume for redis, and I also want to force redis to persist data and the <a href='https://hub.docker.com/_/redis/' rel='noreferrer'>official redis image docs</a> say that happens if I use 'appendonly'.</p>  <pre><code>redis:   image: redis   command: redis-server --appendonly yes   volumes:     - ./redis:/data </code></pre>  <p>If I do this, my data are persisted, as they were in the original example, and I can now see a dump.rdb and and appendonly.aof in the /redis path. The problem is, if I want to restore from a dump.rdb I need to turn off appendonly (for example, see digital ocean's how-to-back-up-and-restore-your-redis-data-on-ubuntu-14-04), and without append-only I cannot see how to get the compose file to write to the volume.</p>  <p>How can I produce a docker compose that will persist redis in a volume where I can switch the dump.rdb files, and therefore insert the production snapshot into my development environment?</p>  <p><strong>Update</strong> The following compose works, but be patient when testing, as the creation of the dump.rdb is not instant (hence it seeming like it failed). Also the redis official image doc, implies you have to use appendonly when you don't:</p>  <pre><code>redis:   image: redis   volumes:     - ./redis:/data </code></pre> ",
    "OwnerUserId": "6937453",
    "LastEditorUserId": "6937453",
    "LastEditDate": "2016-10-07T17:18:09.177",
    "LastActivityDate": "2018-06-15T01:12:12.747",
    "Title": "Restoring redis running on docker from dump.rdb",
    "Tags": "<redis><docker-compose>",
    "AnswerCount": "1",
    "CommentCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p>The appendonly part is just to make sure that you don't lose data, but since you already have the dump.rdb from your server you don't need to worry about that: you can either remove the append only flag or remove 'command' entirely since it will then fall back to the image default which is just 'redis-server'. </p>  <p><s>I have a similar setup <a href='https://github.com/kraftman/filtta/blob/master/docker-compose.yml' rel='nofollow noreferrer'>here</a> and it writes/loads the dump.rdb files fine.</s> (404)</p> "
  },
  {
    "Id": "39991284",
    "PostTypeId": "1",
    "CreationDate": "2016-10-12T05:45:42.040",
    "Score": "9",
    "ViewCount": "13920",
    "Body": "<p>I'm trying to import a large database to a mysql container. I've mounted host directories as volumes for the mysql container. So the data is persistent on host. The importing sql file is <code>14 GB</code>+. The mysql container becomes unresponsive half way of through importing. When I run <code>docker stats</code> I can see the <code>CPU %</code> usage becomes <code>&lt; 1</code> once mysql container ate all the memory available. I tried increasing memory of docker up to <code>10 GB</code> and It creates more tables from import when I allocate more memory to Docker. But I cannot allocate more than <code>10GB</code> from host.</p>  <p>Following is my <code>docker-compose.yml</code> file</p>  <pre><code>mysql:     image: mysql:5.6     environment:         - MYSQL_ROOT_PASSWORD=12345678     volumes:         - ./mysql/lib:/var/lib/mysql         - ./mysql/conf.d:/etc/mysql/conf.d         - ./mysql/log:/var/log/mysql         - /tmp:/tmp     ports:         - '3306:3306' </code></pre>  <p>I'm using <code>Docker for mac</code> which has docker version <code>1.12.1</code></p>  <p>I was using <code>docker exec -it docker_mysql_1 /bin/bash</code> to login to container and import the sql file from <code>/tmp</code></p>  <p>Also I tried the way recommended by mysql repo by mounting sql file to <code>/docker-entrypoint-initdb.d</code>. But that also halt the mysql init.</p>  <p><strong>UPDATE 1</strong>  </p>  <pre><code>$ docker info Containers: 1  Running: 0  Paused: 0  Stopped: 1 Images: 2 Server Version: 1.12.1 Storage Driver: aufs  Root Dir: /var/lib/docker/aufs  Backing Filesystem: extfs  Dirs: 18  Dirperm1 Supported: true Logging Driver: json-file Cgroup Driver: cgroupfs Plugins:  Volume: local  Network: host bridge null overlay Swarm: inactive Runtimes: runc Default Runtime: runc Security Options: seccomp Kernel Version: 4.4.20-moby Operating System: Alpine Linux v3.4 OSType: linux Architecture: x86_64 CPUs: 4 Total Memory: 9.744 GiB Name: moby ID: 43S4:LA5E:6MTG:IFOG:HHJC:HYLX:LYIT:YU43:QGBQ:K5I5:Z6LP:AENZ Docker Root Dir: /var/lib/docker Debug Mode (client): false Debug Mode (server): true  File Descriptors: 16  Goroutines: 27  System Time: 2016-10-12T07:52:58.516469676Z  EventsListeners: 1 No Proxy: *.local, 169.254/16 Registry: https://index.docker.io/v1/ Insecure Registries:  127.0.0.0/8   $ df -h Filesystem      Size   Used  Avail Capacity iused      ifree %iused  Mounted on /dev/disk1     233Gi  141Gi   92Gi    61% 2181510 4292785769    0%   / devfs          193Ki  193Ki    0Bi   100%     668          0  100%   /dev map -hosts       0Bi    0Bi    0Bi   100%       0          0  100%   /net map auto_home    0Bi    0Bi    0Bi   100%       0          0  100%   /home /dev/disk2s2   466Gi   64Gi  401Gi    14%    1857 4294965422    0%   /Volumes/mac /dev/disk2s3   465Gi   29Gi  436Gi     7%  236633    3575589    6%   /Volumes/PORTABLE /dev/disk3s1   100Mi   86Mi   14Mi    86%      12 4294967267    0%   /Volumes/Vagrant </code></pre>  <p>I was using /dev/disk1 directories to mount volumes.</p> ",
    "OwnerUserId": "625144",
    "LastEditorUserId": "625144",
    "LastEditDate": "2016-10-12T07:57:59.670",
    "LastActivityDate": "2020-10-31T10:59:24.700",
    "Title": "unable to import large database to docker mysql container",
    "Tags": "<mysql><docker><docker-compose>",
    "AnswerCount": "3",
    "CommentCount": "4",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p>I solved phpmyadmin-&gt;import of large Database error, by changing Environment variable at docker-compose.yml</p> <blockquote> <p>UPLOAD_LIMIT=1G</p> </blockquote> <pre><code> myadmin:         image: phpmyadmin/phpmyadmin         container_name: phpmyadmin         ports:             - &quot;8083:80&quot;         environment:             - UPLOAD_LIMIT=1G             - PMA_ARBITRARY=1             - PMA_HOST=${MYSQL_HOST}         restart: always         depends_on:             - mysqldb </code></pre> "
  },
  {
    "Id": "40245749",
    "PostTypeId": "1",
    "CreationDate": "2016-10-25T17:06:02.367",
    "Score": "9",
    "ViewCount": "10377",
    "Body": "<p>I created a <strong>docker swarm cluster</strong> with <strong>4 nodes</strong> out of it <strong>2 is swarm manager</strong> (swarm supports multiple manager) I understand if the <strong>current</strong> manager node <strong>goes down</strong> then the <strong>second</strong> manager <strong>takes the role</strong> of being the swarm manager. </p>  <p>In my case I am firing the <strong>rest call</strong> to the swarm manager for creating the services with replicas and so on. </p>  <p>At some point if this manager goes down and the second manager becomes manager how do I know ??</p>  <p>Is there any way it gives notified that the particular node is a manager dynamically ?? </p>  <p>Please give me the clarification on this ??</p> ",
    "OwnerUserId": "4278321",
    "LastEditorUserId": "861127",
    "LastEditDate": "2016-11-21T16:12:05.523",
    "LastActivityDate": "2018-04-05T23:00:55.047",
    "Title": "How to find which node is the currently the manager node in docker swarm in a multiple swarm manager cluster?",
    "Tags": "<docker><docker-compose><docker-machine><docker-swarm>",
    "AnswerCount": "4",
    "CommentCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": "<h2>Using the CLI</h2>  <p>The quick way to (manually) know if the manager changed is through the CLI:</p>  <pre><code>$ docker node ls  ID                           HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS 46aqrk4e473hjbt745z53cr3t    node-5    Ready   Active        Reachable 61pi3d91s0w3b90ijw3deeb2q    node-4    Ready   Active        Reachable a5b2m3oghd48m8eu391pefq5u    node-3    Ready   Active e7p8btxeu3ioshyuj6lxiv6g0    node-2    Ready   Active ehkv3bcimagdese79dn78otj5 *  node-1    Ready   Active        Leader </code></pre>  <p>You can find out which Manager is the current <strong>Leader</strong> under the <code>MANAGER STATUS</code> column.</p>  <p>You can also check individually on each node formatting with the <code>Leader</code> field, for example:</p>  <pre><code>$ docker node inspect &lt;id-node&gt; --format '{{ .ManagerStatus.Leader }}'  true </code></pre>  <h2>Using the Docker Remote API</h2>  <p>You can also check which one of the manager is the leader programmatically. The docker remote API exposes a <a href='https://docs.docker.com/engine/reference/api/docker_remote_api_v1.24/#/list-nodes' rel='noreferrer'>Nodes API endpoint</a>  that you can use to <strong>list nodes</strong>.</p>  <p>You can provide a <em>filter</em> as a parameter which takes the <code>key=value</code> form.</p>  <p>You can use that filter to list manager nodes only (with <code>role=manager</code>), then you can parse and filter the JSON output to keep the node whose <code>Leader</code> field is set at <code>true</code> (under <code>ManagerStatus</code>).</p>  <p>Unfortunately there is no <code>leader</code> filter (<a href='https://github.com/docker/swarmkit/blob/master/manager/controlapi/node.go#L79' rel='noreferrer'>yet</a>) but I assume that this could be a valid enhancement to propose on the swarmkit issue tracker.</p>  <p>There are no events stream on docker swarm mode yet (this is tracked by <a href='https://github.com/docker/swarmkit/issues/491' rel='noreferrer'>this issue</a> on the <em>swarmkit</em> repository). I imagine that in the future, an event will trigger on Leader switch and you will be dynamically notified of the <em>new leader</em> if you are listening to these events (for example if you have a special setup and want to dynamically update entries in consul, nginx or Interlock).</p> "
  },
  {
    "Id": "41221532",
    "PostTypeId": "1",
    "AcceptedAnswerId": "41263454",
    "CreationDate": "2016-12-19T11:25:18.693",
    "Score": "9",
    "ViewCount": "9988",
    "Body": "<p>I'm working on a flask server (in a virtualenv with python 3.5) which is used as a REST API (only for development as suggested for flask). In the beginning it connects to a local sqlite database and it will commit any db changes as soon as possible. Now I wanted to run everything in a docker container and I was wondering how I can access the database because the sqlite file is located in the container.</p>  <p>So I created a volume in a docker-compose file which points to the dockerfile building the application.</p>  <p>Dockerfile:</p>  <pre><code>FROM python:latest ENV HOME /home/parkrep WORKDIR $HOME ADD requirements.txt $HOME/requirements.txt RUN pip install -r requirements.txt ADD . $HOME EXPOSE 80 CMD ['python', 'server.py'] </code></pre>  <p>.dockerignore</p>  <pre><code>__pycache__ venv .gitignore .dockerignore README.md Dockerfile docker-compose.yml </code></pre>  <p>docker-compose.yml</p>  <pre><code>version: '2' services:   parkrep:     build:       context: ./       dockerfile: Dockerfile     volumes:       - ./output:/home/parkrep/output     command: ['python', 'server.py']     ports:       - '80:80' </code></pre>  <p>If I run <code>docker-compose up</code> I get the following </p>  <pre><code>parkrep_1  |   File '/home/parkrep/db_connector.py', line 45, in _connect_db parkrep_1  |     self._connection = sqlite3.connect(path, check_same_thread=False) parkrep_1  | sqlite3.OperationalError: unable to open database file parkrep_parkrep_1 exited with code 1 </code></pre>  <p>If I create the database at output/reports.db and start docker-compose again, it returns the following error:</p>  <pre><code>parkrep_1  | sqlite3.OperationalError: attempt to write a readonly database </code></pre>  <p>So obviously I don't have permissions to write to the file. I tested this behavior by writing to a test file which is mounted like this:</p>  <pre><code>... volumes:   - ./output:/home/parkrep/output   - ./test.txt:/home/parkrep/text.txt command: bash -c 'echo 'hallo' &gt; test.txt' </code></pre>  <p>Error message:</p>  <pre><code>parkrep_1  | bash: text.txt: Permission denied </code></pre>  <p>Let's see who owns this file:</p>  <pre><code>parkrep_1  | drwxr-xr-x 7 root root 4.0K Dec 19 10:45 . parkrep_1  | -rw-rw-r-- 1 root root  143 Dec 12 15:08 config.yaml parkrep_1  | -rw-rw-r-- 1 root root 7.9K Dec 12 14:37 db_connector.py parkrep_1  | drwxrwxr-x 2 4262 4262 4.0K Dec 19 11:10 output parkrep_1  | -rw-rw-r-- 1 root root  144 Dec 12 13:20 requirements.txt parkrep_1  | -rw-rw-r-- 1 root root 2.7K Dec 19 10:14 server.py parkrep_1  | -rw-rw-r-- 1 4262 4262 2.7K Dec 19 10:14 test.txt </code></pre>  <p>It turns out that there is no user 4262 in the container but on the host machine my user account has this id. So I think I know what the problem is now, but I have no clue how to get access to these files. I tried adding ':rw' to the volume definitions but I still don't have write permissions. How can I tell docker to not change the file/directory owner if a volume is defined.</p>  <p>I'm thinking of a problem with my local volume driver, but maybe someone else already had this problem and can tell me how to configure my image to get the required permissions.</p>  <p>Greetings, Thomas</p>  <p><code>docker info</code></p>  <pre><code>     Containers: 1  Running: 0  Paused: 0  Stopped: 1 Images: 19 Server Version: 1.12.5 Storage Driver: aufs  Root Dir: /var/lib/docker/aufs  Backing Filesystem: extfs  Dirs: 19  Dirperm1 Supported: true Logging Driver: json-file Cgroup Driver: cgroupfs Plugins:  Volume: local  Network: host bridge null overlay Swarm: inactive Runtimes: runc Default Runtime: runc Security Options: apparmor seccomp Kernel Version: 4.4.0-53-generic Operating System: Ubuntu 16.04.1 LTS OSType: linux Architecture: x86_64 CPUs: 8 Total Memory: 15.56 GiB Name: de3lxd-107769 ID: PU5F:LZ55:EEK7:W3R7:SYR3:336J:2VRH:35H2:MTLY:6Q6L:BWBP:EM5R Docker Root Dir: /var/lib/docker Debug Mode (client): false Debug Mode (server): false Registry: https://index.docker.io/v1/ WARNING: No swap limit support Insecure Registries:  127.0.0.0/8 </code></pre>  <p><code>docker-compose -v</code></p>  <pre><code>docker-compose version 1.9.0, build 2585387 </code></pre>  <p><code>lsb_release -a</code></p>  <pre><code>No LSB modules are available. Distributor ID: Ubuntu Description:    Ubuntu 16.04.1 LTS Release:    16.04 Codename:   xenial </code></pre> ",
    "OwnerUserId": "4816930",
    "LastActivityDate": "2016-12-21T13:04:36.457",
    "Title": "Docker volume - need permissions to write to database",
    "Tags": "<python><sqlite><docker><docker-compose><docker-volume>",
    "AnswerCount": "1",
    "CommentCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<h2>Hot fix</h2>  <p>I fixed the problem by giving writer permission to everyone.</p>  <pre><code>mkdir output touch output/reports.db output/database.log chmod a+rw output output/* </code></pre>  <p>This will give the user on the host machine and the root user in the docker machine permissions, no matter who owns the files. This is just a dirty fix, because I had to hurry up. Any process could access the and edit/delete the files. It would be better If only the docker user get's writing permission, but I couldn't give writer permission to the root user on the host machine.</p>  <h2>Better approach</h2>  <p>In this <a href='https://stackoverflow.com/a/29251160/1811501'>post</a> they're using another user (www-data) in the container. After building the image they get the id of the user and replace the current file owner with this id. If you start the container as this user (www-data), the mount will copy the files with permissions and owner information, so the user can read and write to these files.</p>  <p>It would be a more secure way because you make sure that only the docker user can change the database/files. Because I couldn't make this work for me (seems to not work for root user with id=0), but I wanted to point out, that there is a better solution.</p>  <p>If you only need the data in the end after the docker stopped you might have a look at <a href='https://docs.docker.com/engine/reference/commandline/cp/' rel='nofollow noreferrer'>docker cp</a>.</p> ",
    "highest_rated_answer": null
  },
  {
    "Id": "41332673",
    "PostTypeId": "1",
    "AcceptedAnswerId": "41332978",
    "CreationDate": "2016-12-26T14:30:36.000",
    "Score": "9",
    "ViewCount": "6522",
    "Body": "<p>I've built a custom Docker image based off an <a href='https://hub.docker.com/_/php/' rel='noreferrer'>official PHP FPM image</a> <code>php:7.0.14-fpm-alpine</code> </p>  <p>I wanted to keep the size of the image small, so I went for the official <code>alpine</code> PHP-FPM version as it weighs only <strong>27 MB</strong>.</p>  <p>I installed only few additional packages through my <code>Dockerfile</code>, and the image grew in size to as much as <strong>277.5 MB</strong>. Here is my Dockerfile:</p>  <pre><code>FROM php:7.0.14-fpm-alpine  COPY ./config/www-pool.conf /usr/local/etc/php-fpm.d/www.conf COPY ./scripts/download-composer.sh /root/download-composer.sh  WORKDIR /root  RUN chmod +x download-composer.sh \\     &amp;&amp; ./download-composer.sh \\     &amp;&amp; mv composer.phar /usr/local/bin/composer  RUN ['mkdir', '/var/log/php-fpm']  RUN apk --update add \\       autoconf g++ make \\       openssl-dev \\       libxml2-dev  RUN pecl install \\         xdebug \\         mongodb  RUN docker-php-ext-enable \\         xdebug.so \\         mongodb.so  RUN  docker-php-ext-install \\         pdo_mysql \\         soap  RUN addgroup sudo RUN adduser -S luqo33 -G sudo RUN echo '%sudo ALL=(ALL) NOPASSWD:ALL' &gt;&gt; /etc/sudoers </code></pre>  <p>277.5 MB is a ten-fold increase in size compared to the base image. Apart from Composer, all I needed were several PHP extensions:</p>  <ul> <li>mongodb</li> <li>xdebug</li> <li>pdo</li> <li>soap</li> </ul>  <p>I'm not sure what contributed the most to increasing the size of my image so much. I suspect that it might be due to the <code>dev</code> dependencies that needed to be installed in order to successfully run <code>pecl</code> (<code>openssl-dev</code>, <code>libxml2-dev</code>), and that might have installed their own tree of dependencies.</p>  <p>Could you please advise on how I can reduce the size of my custom PHP-FPM image and still keep the necessary extensions?</p> ",
    "OwnerUserId": "3785777",
    "LastActivityDate": "2016-12-26T15:05:44.633",
    "Title": "Best way to reduce the size of a custom Docker image",
    "Tags": "<php><docker><docker-compose><dockerfile>",
    "AnswerCount": "1",
    "CommentCount": "1",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>I built the initial part of your image two different ways to test this. A common answer to this question is that the package index takes up extra space. In the case of Alpine Linux (using APK), you can clean up the package index like this:</p>    <pre class='lang-none prettyprint-override'><code>rm -rf /var/cache/apk/* </code></pre>  <p>However, I built the first part of the image both with and without that cleanup. It made hardly any difference (0.8 MB).</p>  <pre class='lang-none prettyprint-override'><code>FROM php:7.0.14-fpm-alpine  WORKDIR /root  RUN ['mkdir', '/var/log/php-fpm']  RUN apk --update add \\       autoconf g++ make \\       openssl-dev \\       libxml2-dev \\   &amp;&amp; rm -rf /var/cache/apk/* </code></pre>  <p>Whether the cleanup command is present or not, the image weighs in at 267MB.</p>  <pre class='lang-none prettyprint-override'><code>REPOSITORY             TAG      IMAGE ID       CREATED          SIZE php-fpm-alpine-test2   latest   b87f5e2d629d   23 seconds ago   267.1 MB php-fpm-alpine-test1   latest   8ff7df8bebea   6 minutes ago    267.9 MB </code></pre>  <p>The space used is simply the packages you're installing.</p>  <pre class='lang-none prettyprint-override'><code>Step 4 : RUN apk --update add       autoconf g++ make       openssl-dev       libxml2-dev   &amp;&amp; rm -rf /var/cache/apk/*  ---&gt; Running in 037a929d9e6a fetch http://dl-cdn.alpinelinux.org/alpine/v3.4/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.4/community/x86_64/APKINDEX.tar.gz (1/23) Installing m4 (1.4.17-r1) (2/23) Installing perl (5.22.2-r0) (3/23) Installing autoconf (2.69-r0) (4/23) Installing libgcc (5.3.0-r0) (5/23) Installing libstdc++ (5.3.0-r0) (6/23) Installing binutils-libs (2.26-r0) (7/23) Installing binutils (2.26-r0) (8/23) Installing gmp (6.1.0-r0) (9/23) Installing isl (0.14.1-r0) (10/23) Installing libgomp (5.3.0-r0) (11/23) Installing libatomic (5.3.0-r0) (12/23) Installing pkgconf (0.9.12-r0) (13/23) Installing pkgconfig (0.25-r1) (14/23) Installing mpfr3 (3.1.2-r0) (15/23) Installing mpc1 (1.0.3-r0) (16/23) Installing gcc (5.3.0-r0) (17/23) Installing musl-dev (1.1.14-r14) (18/23) Installing libc-dev (0.7-r0) (19/23) Installing g++ (5.3.0-r0) (20/23) Installing zlib-dev (1.2.8-r2) (21/23) Installing libxml2-dev (2.9.4-r0) (22/23) Installing make (4.1-r1) (23/23) Installing openssl-dev (1.0.2j-r0) Executing busybox-1.24.2-r11.trigger OK: 220 MiB in 48 packages </code></pre>  <p>As you can see from the summary at the end of this installation, apk has installed 220 MiB of new content.</p>  <p>My best advice would be to run all of your installation and then you can try to remove some of the packages that are only needed for build, not at run-time. For instance you may not need some of the dev packages anymore, or the compiler, automake, etc.</p>  <p>However, you have to keep in mind that each RUN command makes a new layer. To actually save space this way, you would have to run your apk command, all of your other installs, and the post-install cleanup, in a single RUN command, to make a single layer of it. Otherwise, whether you clean up or not, the earlier layers will still have the content and will still contribute to the image size.</p>  <pre class='lang-none prettyprint-override'><code>RUN apk --update add \\       autoconf g++ make \\       openssl-dev \\       libxml2-dev \\     &amp;&amp; pecl install \\         xdebug \\         mongodb \\     &amp;&amp; docker-php-ext-enable \\         xdebug.so \\         mongodb.so \\     &amp;&amp; docker-php-ext-install \\         pdo_mysql \\         soap \\     &amp;&amp; apk del autoconf g++ make openssl-dev libxml2-dev \\     &amp;&amp; rm -rf /var/cache/apk/* </code></pre> ",
    "highest_rated_answer": null
  },
  {
    "Id": "41407452",
    "PostTypeId": "1",
    "CreationDate": "2016-12-31T10:23:21.953",
    "Score": "9",
    "ViewCount": "5182",
    "Body": "<p>I decided to change the project name of a docker composition:</p>  <pre class='lang-sh prettyprint-override'><code>$ docker-compose -p old_name up -d # Before Starting old_name_web_1 $ docker-compose -p new_name up -d # After Creating new_name_web_1 </code></pre>  <p>But I don't wanted to delete my containers, so I renamed them:</p>  <pre class='lang-sh prettyprint-override'><code>$ docker rename old_name_web_1 new_name_web_1 ... </code></pre>  <p>I thought docker-compose was based on container names, but it does not seem to be the case:</p>  <pre class='lang-sh prettyprint-override'><code>$ docker-compose -p new_name up -d </code></pre>  <blockquote>   <p>ERROR: for web  Cannot create container for service web: Conflict. The name '/new_name_web_1' is already in use by container 4930deaabb[...]. You have to remove (or rename) that container to be able to reuse that name.   ERROR: Encountered errors while bringing up the project.</p> </blockquote>  <p>How can I relink my old containers to the new composition ?</p> ",
    "OwnerUserId": "305189",
    "LastEditorUserId": "305189",
    "LastEditDate": "2017-06-01T14:47:16.573",
    "LastActivityDate": "2017-06-01T14:47:16.573",
    "Title": "Rename a project by keeping containers",
    "Tags": "<docker><docker-compose>",
    "AnswerCount": "2",
    "CommentCount": "3",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p>It looks like you are using one of the newer versions of docker compose which tracks containers by labels assigned to them rather than by their names. That is why renaming the container didn't work.</p>  <h3>Updating labels</h3>  <p>You can check container's labels through the <code>docker inspect</code> command.</p>  <pre><code>$ docker inspect --format='{{json .Config.Labels }}' container_name </code></pre>  <p>The project name is the value of the 'com.docker.compose.project' label.</p>  <p>Moving an existing container to a new project is as easy as changing the value of that label. However it is not yet supported by Docker CLI. There is <a href='https://github.com/moby/moby/issues/21721' rel='nofollow noreferrer'>an open issue requesting that feature</a>.</p>  <h3>Workaround</h3>  <p>It still can be achieved by directly editing the configuration file of that particular container. There you will find labels currently assigned to that container.</p>  <pre><code>$ nano /var/lib/docker/containers/$container_id/config.v2.json </code></pre>  <p>Assign the new project name to the 'com.docker.compose.project' label and save the file. Next you have to restart the daemon. Otherwise the changes will not be visible to docker.</p>  <pre><code>$ systemctl daemon-reload </code></pre> "
  },
  {
    "Id": "41423349",
    "PostTypeId": "1",
    "CreationDate": "2017-01-02T08:15:58.300",
    "Score": "9",
    "ViewCount": "87944",
    "Body": "<p>I use this to set up nginx for PHP:</p>  <pre><code>nginx:     image: nginx:latest     ports:         - 8080:80     volumes:         - ./code:/code         - ./site.conf:/etc/nginx/conf.d/site.conf     links:         - php php:     image: php:7-fpm     volumes:         - ./code:/code </code></pre>  <p>But how about Apache? How can I set up Apache + PHP in docker-compose.yml?</p>  <p>Following this <a href='https://www.kinamo.be/en/support/faq/setting-up-a-development-environment-with-docker-compose' rel='noreferrer'>guide</a>:</p>  <pre><code>version: '2'  services:   php:     build: php     ports:       - '80:80'       - '443:443'     volumes:       - ./php/www:/var/www/html </code></pre>  <p>Error:</p>  <pre><code>ERROR: In file './docker-compose.yml' service 'version' doesn't have any configuration options. All top level keys in your docker-compose.yml must map to a dictionary of configuration options. </code></pre>  <p>Any ideas? I'm on Xubuntu 16.04.</p>  <p><strong>EDIT:</strong></p>  <p>After managing to upgrade docker-compose to 1.9, I try with this file below:</p>  <pre><code>version: '2' services:     php:         build: php         expose:             - 9000         volumes:             - ./php/www:/var/www/html      apache2:         image: webdevops/apache:latest         args:             - PHP_SOCKET=php:9000         volumes:             - ./php/www:/var/www/html         ports:             - 80:80             - 443:443         links:             - php </code></pre>  <p>Error:</p>  <pre><code>$ sudo docker-compose up -d Building php ERROR: Cannot locate specified Dockerfile: Dockerfile </code></pre>  <p>Docker is such as pain!</p>  <p>Any ideas how to fix this?</p> ",
    "OwnerUserId": "413225",
    "LastEditorUserId": "413225",
    "LastEditDate": "2017-01-02T10:51:33.317",
    "LastActivityDate": "2023-09-18T10:19:41.760",
    "Title": "Docker - how to set up Apache + PHP in docker-compose.yml",
    "Tags": "<php><apache><docker><docker-compose><ubuntu-16.04>",
    "AnswerCount": "7",
    "CommentCount": "0",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p>Since the example above does not work for me, here is a different approach:</p> <p>Your file <code>docker-compose.yml</code>:</p> <pre><code>version: '3.1'  services:   php:     image: php:apache     ports:       - 80:80     volumes:       - ./php/www:/var/www/html/ </code></pre> <p>Launch the server with</p> <pre><code>docker-compose up </code></pre> <p>This works with a single Docker image and a single service, since there is already an official PHP image based on latest Debian GNU/Linux stable and with Apache HTTPd webserver. Info:</p> <p><a href='https://hub.docker.com/_/php' rel='nofollow noreferrer'>https://hub.docker.com/_/php</a></p> "
  },
  {
    "Id": "41691584",
    "PostTypeId": "1",
    "AcceptedAnswerId": "41711291",
    "CreationDate": "2017-01-17T07:47:17.197",
    "Score": "9",
    "ViewCount": "24188",
    "Body": "<p>I'm trying to use docker-compose to volume my php.ini file so I can make changes on the fly on my local machine to see how it affects the host machine.  Unfortunately the only way I've been able to get the php.ini file into the container is directly during creation in the Dockerfile so far.</p>  <p>Attached is an image of the container running fine with the current settings below.</p>  <p><a href='https://i.stack.imgur.com/qWhvO.png' rel='noreferrer'><img src='https://i.stack.imgur.com/qWhvO.png' alt='enter image description here'></a></p>  <p><strong>My Dockerfile is below:</strong></p>  <pre><code>FROM ubuntu:14.04 MAINTAINER Joe Astrahan &lt;jastrahan@poolservice.software&gt;  VOLUME ['/var/www']   RUN apt-get update &amp;&amp; \\     apt-get install -y software-properties-common &amp;&amp; \\     apt-get update &amp;&amp; \\     apt-get install -y \\       apache2 \\       curl \\       libcurl3 \\       libcurl3-dev \\       php5 \\       php5-cli \\       libapache2-mod-php5 \\       php5-gd \\       php5-json \\       php5-ldap \\       php5-mysqlnd \\       php5-pgsql \\       php5-curl \\       mysql-client  COPY config/php.ini /etc/php5/apache2/php.ini  # install php-5.5.30 COPY config/install_php-5.5.30.sh /tmp/install_php-5.5.30.sh RUN /bin/bash /tmp/install_php-5.5.30.sh   COPY config/apache_default.conf /etc/apache2/sites-available/000-default.conf COPY config/run /usr/local/bin/run  RUN chmod +x /usr/local/bin/run RUN a2enmod rewrite  #This will allow us to modify files in the container for testing if we need to RUN apt-get update &amp;&amp; \\     apt-get install -y vim  EXPOSE 80 CMD ['/usr/local/bin/run'] </code></pre>  <p><strong>My docker-compose.yml file is below:</strong></p>  <pre><code>version: '2' services:     dblive:         image: mysql:5.5.52         volumes:             - ./db_data_live:/var/lib/mysql         restart: always         environment:             MYSQL_ROOT_PASSWORD: ****             MYSQL_DATABASE: ****             MYSQL_USER: ****             MYSQL_PASSWORD: ****      dbdev:         image: mysql:5.5.52         volumes:             - ./db_data_dev:/var/lib/mysql         restart: always         environment:             MYSQL_ROOT_PASSWORD:****             MYSQL_DATABASE: ****             MYSQL_USER: ****             MYSQL_PASSWORD: ****      phpmyadmin:         depends_on:             - dblive             - dbdev         image: phpmyadmin/phpmyadmin         environment:             PMA_ARBITRARY : 1         restart: always         ports:             - '8081:80'      web:         build: ./         depends_on:             - dblive             - dbdev         volumes:             - ./web:/var/www             - ./config/php.ini:/etc/php5/apache2/conf.d/custom.ini             - ./logs/apache_error.log:/var/log/apache2/error.log             - ./logs/apache_access.log:/var/log/apache2/access.log             - ./config/apache_default.conf:/etc/apache2/sites-enabled/000-default.conf         restart: always         ports:              - '80:80'             - '443:443' </code></pre>  <p>I tried following the advice here, <a href='https://stackoverflow.com/questions/39875543/cant-upate-php-ini-file-in-docker-container'>can&#39;t upate php.ini file in Docker container</a>, by creating a custom.ini file and mounting it in that location.  I actually did it correctly I think because if you look at my image I attached for phpinfo(), you can see that under additional .ini files parsed my custom.ini is there at the end.  I did a test though by setting asp_tags = On instead of Off and I can't.  phpinfo() will always show it as off.  Refer to my attached image of it showing it off despite loading my config file.</p>  <p>I'm not even sure if its really honoring any of the commands in there at all?</p>  <p><a href='https://i.stack.imgur.com/jFtAj.png' rel='noreferrer'><img src='https://i.stack.imgur.com/jFtAj.png' alt='enter image description here'></a></p>  <p><a href='https://i.stack.imgur.com/48moq.png' rel='noreferrer'><img src='https://i.stack.imgur.com/48moq.png' alt='enter image description here'></a></p>  <p><strong>Extra Files Used</strong></p>  <p><strong>Run</strong></p>  <pre><code>#!/bin/bash set -e  PHP_ERROR_REPORTING=${PHP_ERROR_REPORTING:-'E_ALL &amp; ~E_DEPRECATED &amp; ~E_NOTICE'} sed -ri 's/^display_errors\\s*=\\s*Off/display_errors = On/g' /etc/php5/apache2/php.ini sed -ri 's/^display_errors\\s*=\\s*Off/display_errors = On/g' /etc/php5/cli/php.ini sed -ri 's/^error_reporting\\s*=.*$//g' /etc/php5/apache2/php.ini sed -ri 's/^error_reporting\\s*=.*$//g' /etc/php5/cli/php.ini echo 'error_reporting = $PHP_ERROR_REPORTING' &gt;&gt; /etc/php5/apache2/php.ini echo 'error_reporting = $PHP_ERROR_REPORTING' &gt;&gt; /etc/php5/cli/php.ini  source /etc/apache2/envvars &amp;&amp; exec /usr/sbin/apache2 -DFOREGROUND </code></pre>  <p><strong>install_php-5.5.30.sh</strong></p>  <pre><code>#!/bin/bash  # install dependencies apt-get -y update &amp;&amp; \\ apt-get install -y \\   build-essential \\   apache2-dev \\   libxml2-dev  # download PHP 5.5.30 source code cd /tmp curl -fsSL http://php.net/get/php-5.5.30.tar.bz2/from/this/mirror | tar xjf - cd php-5.5.30  # configure build options ./configure --prefix=/usr \\             --with-config-file-path=/etc/php5/apache2 \\             --with-config-file-scan-dir=/etc/php5/apache2/conf.d \\             --disable-pdo \\             --disable-json \\             --enable-mbstring \\             --with-apxs2  # compile and install NUM_CORES=`cat /proc/cpuinfo | grep processor | wc -l` make -j $NUM_CORES make install  # configure extension directory echo 'extension_dir='/usr/lib/php5/20121212'' &gt;&gt; /etc/php5/apache2/php.ini  # cleanup rm -rf /tmp/php-5.5.30 /tmp/install_php-5.5.30.sh </code></pre>  <p><strong>My file structure</strong></p>  <p><a href='https://i.stack.imgur.com/9uu5Y.png' rel='noreferrer'><img src='https://i.stack.imgur.com/9uu5Y.png' alt='enter image description here'></a></p> ",
    "OwnerUserId": "1606689",
    "LastEditorUserId": "-1",
    "LastEditDate": "2017-05-23T12:16:41.197",
    "LastActivityDate": "2024-01-22T15:40:06.547",
    "Title": "Docker-Compose won't volume my php.ini file",
    "Tags": "<php><docker><docker-compose><dockerfile><docker-volume>",
    "AnswerCount": "3",
    "CommentCount": "1",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>I found the answer, put a file called custom.php.ini in the config directory (if you are following my directory structure).</p>  <p>Set the volumes like this in my example...</p>  <pre><code>volumes:             - ./web:/var/www             - ./config/custom.php.ini:/etc/php5/apache2/conf.d/custom.php.ini </code></pre>  <p>By default the scan directory for extra php files will look in the conf.d directory.  These files will overwrite the settings of the main php.ini.  I tested this with the asp_tag option turning it Off and On. It works fine as long as you do the following below.</p>  <p>The trick to making this work is to use <strong><code>docker-compose down</code></strong> instead of <strong><code>docker-compose kill</code></strong></p>  <p>This removes the containers and their cache files.  PHP only loads the configuration file once at bootup and the other files so changing the php.ini or the custom file after requires this docker-compose down to force the change.</p> ",
    "highest_rated_answer": "<p>If you are using something like <strong>wodby</strong> (<code>docker4php</code>  or <code>docker4drupal</code>) or <strong>lando</strong> or trying to find an answer &quot;why php.ini doesn't work&quot; (like me), these tools are using their own way to pass configuration into php</p> <p><a href='https://github.com/wodby/php#php-and-php-fpm-configuration' rel='nofollow noreferrer'>https://github.com/wodby/php#php-and-php-fpm-configuration</a></p> <p>I am posting this answer here because I wasted 2 hours to find an answer and I came into this question from google. I want to save some time to others.</p> <p>Mounting of custom.php.ini will not help.</p> "
  },
  {
    "Id": "41871950",
    "PostTypeId": "1",
    "AcceptedAnswerId": "41907584",
    "CreationDate": "2017-01-26T10:55:46.110",
    "Score": "9",
    "ViewCount": "3837",
    "Body": "<p>I'm trying to deploy an app that's built with docker-compose, but it feels like I'm going in completely the wrong direction.</p>  <ol> <li>I have everything working locally\u2014<code>docker-compose up</code> brings up my app with the appropriate networks and hosts in place.</li> <li>I want to be able to run the same configuration of containers and networks on a production machine, just using a different <code>.env</code> file.</li> </ol>  <p>My current workflow looks something like this:</p>  <pre><code>docker save [web image] [db image] &gt; containers.tar zip deploy.zip containers.tar docker-compose.yml rsync deploy.zip user@server  ssh user@server unzip deploy.zip ./ docker load -i containers.tar  docker-compose up </code></pre>  <p>At this point, I was hoping to be able to run <code>docker-compose up</code> again when they get there, but that tries to rebuild the containers as per the <code>docker-compose.yml</code> file.</p>  <p>I'm getting the distinct feeling that I'm missing something. Should I be shipping over my full application then building the images at the server instead? How would you start composed containers if you were storing/loading the images from a registry?</p> ",
    "OwnerUserId": "1088797",
    "LastEditorUserId": "1088797",
    "LastEditDate": "2017-01-26T11:44:41.020",
    "LastActivityDate": "2017-01-28T07:56:27.800",
    "Title": "Deploying docker-compose containers",
    "Tags": "<docker><docker-compose>",
    "AnswerCount": "1",
    "CommentCount": "6",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>The problem was that I was using the same docker-compose.yml file in development and production.</p>  <p>The app service didn't specify a repository name or tag, so when I ran <code>docker-compose up</code> on the server, it just tried to build the Dockerfile in my app's source code directory (which doesn't exist on the server).</p>  <p>I ended up solving the problem by adding an explicit image field to my local docker-compose.yml.</p>  <pre><code>version: '2' services:   web:     image: 'my-private-docker-registry:latest'     build: ./app </code></pre>  <p>Then created an alternative compose file for production:</p>  <pre><code>version: '2' services:   web:     image: 'my-private-docker-registry:latest'     # no build field! </code></pre>  <p>After running <code>docker-compose build</code> locally, the web service image is built with the repository name <code>my-private-docker-registry</code> and the tag <code>latest</code>.</p>  <p>Then it's just a case of pushing the image up to the repository.</p>  <pre><code>docker push 'my-private-docker-registry:latest' </code></pre>  <p>And running <code>docker pull</code>, it's safe to stop and recreate the running containers, with the new images.</p> ",
    "highest_rated_answer": null
  },
  {
    "Id": "42302632",
    "PostTypeId": "1",
    "AcceptedAnswerId": "42302633",
    "CreationDate": "2017-02-17T16:19:08.077",
    "Score": "9",
    "ViewCount": "8005",
    "Body": "<p>I am looking for a way to deploy <code>docker-compose</code> images and / or builds to a remote sever, specifically but not limited to a DigitalOcean VPS.</p>  <p><code>docker-compose</code> is currently working on the CircleCI Continuous Integration service, where it automatically verifies that tests pass. But, it <strong>should deploy automatically on success</strong>.</p>  <p>My <code>docker-compose.yml</code> is looking like this:</p>  <pre><code>version: '2' services:   web:     image: name/repo:latest     ports:       - '3000:3000'     volumes:       - /app/node_modules       - .:/app     depends_on:        - mongo       - redis   mongo:     image: mongo     command: --smallfiles     volumes:       - ./data/mongodb:/data/db   redis:     image: redis     volumes:       - ./data/redis:/data </code></pre>  <p><code>docker-compose.override.yml</code>:</p>  <pre><code>version: '2' services:   web:     build: . </code></pre>  <p><code>circle.yml</code> relevant part:</p>  <pre><code>deployment:   latest:     branch: master     commands:       - docker login -e $DOCKER_EMAIL -u $DOCKER_USER -p $DOCKER_PASS       - docker push name/repo:$CIRCLE_SHA1       - docker push name/repo:latest </code></pre> ",
    "OwnerUserId": "2013580",
    "LastActivityDate": "2018-07-21T00:54:42.407",
    "Title": "Docker Compose Continuous Deployment setup",
    "Tags": "<docker><deployment><docker-compose><continuous-deployment>",
    "AnswerCount": "2",
    "CommentCount": "0",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>Your <code>docker-compose</code> and <code>circle</code> configurations are already looking pretty good. </p>  <p>Your <code>docker-compose.yml</code> is already setup to gather the image from the <a href='http://hub.docker.com' rel='noreferrer'>Docker Hub</a>, which is being uploaded after tests have passed. We will use this image on the remote server, which instead of building the image up every time (which takes a long time), we'll use this already prepared one.</p>  <p>You did well into separating the <code>build: .</code> into a <code>docker-compose.override.yml</code> file, as <a href='https://stackoverflow.com/a/42179585/2013580'>priority issues can arise if we use a <code>docker-compose.prod.yml</code> file</a>.</p>  <h2>Let's get started with the deployment:</h2>  <p>There are various ways of getting your deployment done. The most popular ones are probably <strong>SSH</strong> and Webhooks.</p>  <p>We'll use SSH.</p>  <p>Edit your <code>circle.yml</code> config to take an additional step, which to load our <code>.scripts/deploy.sh</code> bash file:</p>  <pre><code>deployment:   latest:     branch: master     commands:       - docker login -e $DOCKER_EMAIL -u $DOCKER_USER -p $DOCKER_PASS       - docker push name/repo:$CIRCLE_SHA1       - docker push name/repo:latest       - .scripts/deploy.sh </code></pre>  <p><code>deploy.sh</code> will contain a few instructions to connect into our remote server through SSH and update both the repository and Docker images and reload Docker Compose services.</p>  <p>Prior executing it, you should have a remote server that contains your project folder (i.e. <code>git clone https://github.com/zurfyx/my-project</code>), and both <a href='https://docs.docker.com/engine/installation/linux/' rel='noreferrer'>Docker</a> and <a href='https://docs.docker.com/compose/install/' rel='noreferrer'>Docker Compose</a> installed.</p>  <p>deploy.sh</p>  <pre><code>#!/bin/bash  DIR='$( cd '$( dirname '${BASH_SOURCE[0]}' )' &amp;&amp; pwd )'  (   cd '$DIR/..' # Go to project dir.    ssh $SSH_USERNAME@$SSH_HOSTNAME -o StrictHostKeyChecking=no &lt;&lt;-EOF     cd $SSH_PROJECT_FOLDER     git pull     docker-compose pull     docker-compose stop     docker-compose rm -f     docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d EOF ) </code></pre>  <p><em>Notice: last EOF is not indented. That's how bash <a href='https://stackoverflow.com/questions/2500436/how-does-cat-eof-work-in-bash'>HEREDOC</a> works.</em></p>  <p>deploy.sh steps explained:</p>  <ol> <li><code>ssh $SSH_USERNAME@$SSH_HOSTNAME</code>: connects to the remote host through SSH. <code>-o StrictHostChecking=no</code> avoids the SSH asking whether we trust the server.</li> <li><code>cd $SSH_PROJECT_FOLDER</code>: browses to the project folder (the one you did gather through <code>git clone ...</code>)</li> <li><code>git pull</code>: updates project folder. That's important to keep docker-compose / Dockerfile updated, as well as any shared volume that depends on some source code file.</li> <li><code>docker-compose stop</code>: Our remote dependencies have just been downloaded. Stop the docker-compose services which are current running.</li> <li><code>docker-compose rm -f</code>: Remove docker-compose services. This step is really important, otherwise <a href='https://github.com/docker/compose/issues/2127' rel='noreferrer'>we'll reuse old volumes</a>.</li> <li><code>docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d</code>. Execute your <code>docker-compose.prod.yml</code> which extends <code>docker-compose.yml</code> in detached mode.</li> </ol>  <p>On your CI you will need to fill in the following environment variables (that the deployment script uses):</p>  <ul> <li><code>$SSH_USERNAME</code>: your SSH username (i.e. root)</li> <li><code>$SSH_HOSTNAME</code>: your SSH hostname (i.e. stackoverflow.com)</li> <li><code>$SSH_PROJECT_FOLDER</code>: the folder where the project is stored (either relative or absolute to where the <code>$SSH_USERNAME</code> is on login. (i.e. my-project/)</li> </ul>  <p>What about the SSH password? CircleCI in this case offers a way to store SSH keys, so password is no longer needed when logging in through SSH.</p>  <p>Otherwise simply edit the <code>deploy.sh</code> SSH connection to something like this:</p>  <pre><code>sshpass -p your_password ssh user@hostname </code></pre>  <p>More about SSH password <a href='https://serverfault.com/a/512220/313459'>here</a>.</p>  <p>In conclusion, all we had to do was to create a script that connected with our remote server to let it know that the source code had been updated. Well, and to perform the appropriate upgrading steps.</p>  <p><em>FYI, that's similar to how the alternative Webhooks method work.</em></p> ",
    "highest_rated_answer": "<p>WatchTower solves this for you.</p>  <p><a href='https://github.com/v2tec/watchtower' rel='nofollow noreferrer'>https://github.com/v2tec/watchtower</a></p>  <p>Your CI just needs to build the images and push to the registry. Then WatchTower polls the registry every N seconds and automagically restarts your services using the latest and greatest images. It's as simple as adding this code to your compose yaml:</p>  <pre><code>watchtower: image: v2tec/watchtower volumes:   - /var/run/docker.sock:/var/run/docker.sock   - /root/.docker/config.json:/config.json command: --interval 30 </code></pre> "
  },
  {
    "Id": "42398199",
    "PostTypeId": "1",
    "AcceptedAnswerId": "42416093",
    "CreationDate": "2017-02-22T17:19:32.183",
    "Score": "9",
    "ViewCount": "20997",
    "Body": "<p>I am trying to build and run two Docker containers hosting PostgreSQL and Citus extension using <code>ansible-container</code>. I am aware that Citus provides containers, but I want to build my own.</p>  <p>My <code>container.yaml</code> looks as follows:</p>  <pre class='lang-yaml prettyprint-override'><code>version: '2'  services:    database_master:     image: hackermd/ubuntu-trusty-python     user: postgres     expose:       - 5043     entrypoint: ['dumb-init', '--']     command: ['/usr/bin/pg_ctlcluster', '9.6', 'master', 'start']     links:       - database_worker     depends_on:       - database_worker    database_worker:     image: hackermd/ubuntu-trusty-python     user: postgres     expose:     - 9700   entrypoint: ['dumb-init', '--']   command: ['/usr/bin/pg_ctlcluster', '9.6', 'worker', 'start'] </code></pre>  <p>During the build process I can start and stop the cluster via <code>pg_ctlcluster</code> and it finishes successfully. However, when I subsequently run the containers, I get the following error:</p>  <pre class='lang-none prettyprint-override'><code>$ docker logs ansible_database_master_1 Removed stale pid file. Warning: connection to the database failed, disabling startup checks: psql: FATAL:  the database system is starting up </code></pre>  <p>When I build containers with <code>command: []</code> and run <code>ps aux</code> inside the container, I see the following process:</p>  <pre class='lang-none prettyprint-override'><code>postgres    14  1.6  0.1 307504  3480 ?        Ds   16:46   0:00 postgres: 9.6/master: startup process </code></pre>  <p>I've also tried without the <code>dumb-init</code> entrypoint. What am I missing?</p> ",
    "OwnerUserId": "5483098",
    "LastEditorUserId": "5483098",
    "LastEditDate": "2017-02-22T23:14:45.610",
    "LastActivityDate": "2020-05-06T15:00:07.077",
    "Title": "Dockerized PostgreSQL: psql: FATAL: the database system is starting up",
    "Tags": "<postgresql><docker><ansible><docker-compose><ansible-container>",
    "AnswerCount": "2",
    "CommentCount": "0",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>The problem is related to the default shutdown method of the <code>pg_ctl stop</code> mode (<code>pg_ctl</code> gets called by <code>pg_ctlcluster</code>). Stopping the cluster via <code>pg_ctlcluster</code> with the <code>pg_ctl</code> option <code>-m smart</code> during the build process solves this problem:</p>  <pre class='lang-none prettyprint-override'><code>pg_ctlcluster 9.6 master stop -- -m smart </code></pre>  <p>The 'smart' method waits for active clients to disconnect and online backups to finish before shutting down in contrast to the default 'fast' method. This is explained in the documentation of <a href='https://www.postgresql.org/docs/current/static/app-pg-ctl.html' rel='noreferrer'>pg_ctl</a>.</p>  <p>In addition, the container would exit once the <code>pg_ctlcontrol</code> process successfully started the database cluster via <code>postgres</code> (<code>pg_ctlcontrol</code> -> <code>pg_ctl</code> -> <code>postgres</code>). To prevent this, <code>postgres</code> can be called directly. The <code>container.yml</code> file would then look as follows:</p>  <pre class='lang-yaml prettyprint-override'><code>version: '2'  services:    database_master:     image: hackermd/ubuntu-trusty-python     user: postgres     expose:       - 5043     command: ['dumb-init', '/usr/lib/postgresql/9.6/bin/postgres', '-D', '/var/lib/postgresql/9.6/master']     links:       - database_worker     depends_on:       - database_worker    database_worker:     image: hackermd/ubuntu-trusty-python     user: postgres     expose:       - 9700     command: ['dumb-init', '/usr/lib/postgresql/9.6/bin/postgres', '-D', '/var/lib/postgresql/9.6/worker'] </code></pre> ",
    "highest_rated_answer": "<p>My problem was with starting postgres using pg_ctl and right after running tests in my docker container. What fixed it was adding 'smart mode' to my command, i.e:</p>  <pre><code>su - postgres -c 'pg_ctl start -D /var/lib/postgresql/data -l /var/lib/postgresql/log.log -m smart' </code></pre> "
  },
  {
    "Id": "42426378",
    "PostTypeId": "1",
    "CreationDate": "2017-02-23T21:19:09.850",
    "Score": "9",
    "ViewCount": "5383",
    "Body": "<p>I'm creating an entrypoint for a Docker container, and then attempting to run it using a <code>docker-compose.yml</code> file.  <strong>This works just fine in Ubuntu and OS X</strong>, but I get a permissions error (without much additional information) in Windows</p>  <p>Here is the Dockerfile:</p>  <pre><code>FROM node:6.9  MAINTAINER Some Dude &lt;dude@dude.com&gt;  WORKDIR /opt  # Install Compass RUN DEBIAN_FRONTEND=noninteractive apt-get -y dist-upgrade RUN apt-get -y update RUN apt-get -y install gcc rubygems ruby-dev RUN gem update --system RUN gem install compass  # Install Compass Extensions RUN gem install compass-blend-modes compass-import-once  # Install glup globally RUN npm install -g gulp  # Copy the setup file COPY setup/gulp/docker-gulp-setup.sh /usr/local/bin/docker-gulp-setup.sh RUN chmod +x /usr/local/bin/docker-gulp-setup.sh  CMD ['gulp'] </code></pre>  <p>Here is the docker-compose.yml entry:</p>  <pre><code>version: '2'  services:          mycontainer-for-gulp:                 build:                         context: .                         dockerfile: Dockerfile.gulp                 volumes:                         - ./:/opt:Z                 command: /usr/local/bin/docker-gulp-setup.sh </code></pre>  <p>Here is the output when the build command is run on Windows:</p>  <pre><code>\u03bb docker-compose build mycontainer-for-gulp Building mycontainer-for-gulp Traceback (most recent call last):  File '&lt;string&gt;', line 3, in &lt;module&gt;  File 'compose\\cli\\main.py', line 65, in main  File 'compose\\cli\\main.py', line 117, in perform_command  File 'compose\\cli\\main.py', line 223, in build  File 'compose\\project.py', line 300, in build  File 'compose\\service.py', line 742, in build  File 'site-packages\\docker\\api\\build.py', line 55, in build  File 'site-packages\\docker\\utils\\utils.py', line 95, in tar  File 'tarfile.py', line 2023, in add  File 'tarfile.py', line 2052, in addfile  File 'tarfile.py', line 278, in copyfileobj IOError: [Errno 13] Permission denied docker-compose returned -1 </code></pre>  <p>Note that it IS the <code>COPY</code> command that's generating the error, because we tried commenting out both that and the <code>RUN chmod</code> statement individually.</p>  <p>When I run a raw docker build query:</p>  <pre><code>docker build -f Dockerfile.gulp . </code></pre>  <p>Then I get a bunch of error output as it tries to tar up the current directory.  Basically every file add fails.  Full output is here:</p>  <p><a href='https://gist.github.com/danieltalsky/4cb6bddb6534c46b051230bc45578072' rel='noreferrer'>https://gist.github.com/danieltalsky/4cb6bddb6534c46b051230bc45578072</a></p> ",
    "OwnerUserId": "22452",
    "LastEditorUserId": "22452",
    "LastEditDate": "2017-02-24T19:16:21.897",
    "LastActivityDate": "2021-02-23T08:31:13.243",
    "Title": "Docker build fails on COPY or ADD statement for entrypoint, Windows only",
    "Tags": "<docker><docker-compose><file-permissions><docker-for-windows>",
    "AnswerCount": "2",
    "CommentCount": "13",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p>I was facing the same issue when I tried to build the Docker image while the application was still running in the IDE.</p> <p><strong>Make sure to shutdown your application before building your image</strong></p> "
  },
  {
    "Id": "42672171",
    "PostTypeId": "1",
    "AcceptedAnswerId": "42676571",
    "CreationDate": "2017-03-08T13:05:14.477",
    "Score": "9",
    "ViewCount": "11498",
    "Body": "<p>I am having problem sharing a folder between Docker containers running on different nodes of Docker Swarm. My swarm consist of one manager and two workers.</p>  <p>I am using this compose file to deploy applications:</p>  <pre><code>version: '3'  services:    redis:     image:       redis:latest     networks:       - default     ports:       - 6379:6379     volumes:       - test-volume:/test     deploy:       replicas: 1       update_config:         parallelism: 2         delay: 10s       restart_policy:         condition: on-failure       placement:         constraints: [node.role == manager]    logstash:     image: docker.elastic.co/logstash/logstash:5.2.2     networks:       - default     volumes:       - test-volume:/test     deploy:       placement:         constraints: [node.role == worker]  networks:   default:     external: false  volumes:   test-volume: </code></pre>  <p>I can confirm that the folder is successfully mounted in both containers with use of <code>docker exec _id_ ls /test</code>. But when I add a file into this folder  with <code>docker exec _id_ touch /test/file</code> second container does not see created file.</p>  <p>How to configure the swarm so the files are visible in both containers?</p> ",
    "OwnerUserId": "3423236",
    "LastActivityDate": "2017-12-07T12:04:49.477",
    "Title": "Volume is not shared between nodes of Docker Swarm",
    "Tags": "<docker><docker-compose><docker-swarm>",
    "AnswerCount": "1",
    "CommentCount": "1",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>Volumes created in docker swarm via default driver are local to the node. So if you put both containers on the same host they will have a shared volume. But when you put your containers on different nodes, there will be a separate volume created on each node.</p>  <p>Now in order to achieve bind mounts/volumes across multiple nodes you have these options:</p>  <ol> <li><p>Use a cluster filesystem like glusterfs, ceph and ... across swarm nodes, then use bind mounts in your service defenition pointing to shared fs.</p></li> <li><p>Use one of the many storage drivers available to docker that provide shared storage like flocker, ...</p></li> <li><p>Switch to Kubernetes and take advantage of automated volume provisioning using multiple backends via Storage classes and claims.</p></li> </ol>  <p>UPDATE: As @Krishna noted in the comments Flocker has been shut down and there isn't a lot of activity on the <a href='https://github.com/ClusterHQ/flocker' rel='noreferrer'>github repo</a>.</p> ",
    "highest_rated_answer": null
  },
  {
    "Id": "43121897",
    "PostTypeId": "1",
    "AcceptedAnswerId": "43130286",
    "CreationDate": "2017-03-30T15:16:56.700",
    "Score": "9",
    "ViewCount": "14691",
    "Body": "<p>I'm using docker-compose to run my project created by node,mongodb,nginx;</p>  <p>and I have build the project using <code>docker build</code></p>  <p>and then I use <code>docker up -d nginx</code> to start my project. but I haven't found the config to run mongodb image with '--auth', so how to add '--auth' when compose start the mongodb?</p>  <p>here is my docker-compose.yml:</p>  <pre><code>version: '2' services:   mongodb:     image: mongo:latest     expose:         - '27017'     volumes:         - '/home/open/mymongo:/data/db'   nginx:     build: /home/open/mynginx/     ports:         - '8080:8080'         - '80:80'     links:         - node_server:node   node_server:     build: /home/laoqiren/workspace/isomorphic-redux-CNode/      links:         - mongodb:mongo     expose:         - '3000' </code></pre> ",
    "OwnerUserId": "6271247",
    "LastActivityDate": "2017-04-15T19:45:24.233",
    "Title": "how to add --auth for mongodb image when using docker-compose?",
    "Tags": "<node.js><mongodb><nginx><docker><docker-compose>",
    "AnswerCount": "1",
    "CommentCount": "2",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>Supply a <code>command</code> to the container including the <code>--auth</code> option.</p>  <pre><code>  mongodb:     image: mongo:latest     expose:         - '27017'     volumes:         - '/home/open/mymongo:/data/db'     command: mongod --auth </code></pre>  <p>The latest mongo containers come with <a href='https://stackoverflow.com/a/42917632/1318694'>'root' auth initialisation via environment variables</a> too, modelled on the postgres setup. </p> ",
    "highest_rated_answer": null
  },
  {
    "Id": "43281251",
    "PostTypeId": "1",
    "CreationDate": "2017-04-07T14:57:04.983",
    "Score": "9",
    "ViewCount": "2752",
    "Body": "<p>I'm trying to write a compose-file using a service (node) that I'd like to scale to 3 containers. This service has a volume to be bond to my local system for persistence, but I'd like the different tasks having different binding on my system ? I tried to use the template <code>{{.Task.Slot}}</code> inside my docker-compose.yml as shown below, but I cannot figured out out to make it work...</p>  <pre><code>version: '3.2' services:      node:         build:             context: ./node         volumes:             - nodeVol:/data         command: ['myapp']  volumes:     nodeVol:         external:             name: './persistence/{{.Task.Slot}}' </code></pre>  <p>Any idea of what went wrong ?</p> ",
    "OwnerUserId": "7833346",
    "LastActivityDate": "2017-04-07T14:57:04.983",
    "Title": "docker-compose scale service with independent volumes",
    "Tags": "<docker><docker-compose>",
    "AnswerCount": "0",
    "CommentCount": "4",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": null
  },
  {
    "Id": "43517984",
    "PostTypeId": "1",
    "CreationDate": "2017-04-20T11:15:18.620",
    "Score": "9",
    "ViewCount": "9932",
    "Body": "<p>I am using docker-compose version 2. I am starting containers with <code>docker-compose -p some_name up -d</code> and trying to kill them with <code>docker-compose stop</code>. The commands exits with <code>0</code> code but the containers are still up and running. </p>  <p>Is this the expected behaviour for version? If yes, any idea how can I work around it?</p>  <p>my <code>docker-compose.yml</code> file looks like this</p>  <pre><code>version: '2' services:    elasticsearch:     image: docker.elastic.co/elasticsearch/elasticsearch:5.3.0     ports:       - '9200:9200'     environment:       ES_JAVA_OPTS: '-Xmx512m -Xms512m'       xpack.security.enabled: 'false'       xpack.monitoring.enabled: 'false'       xpack.graph.enabled: 'false'       xpack.watcher.enabled: 'false'     ulimits:       memlock:         soft: -1         hard: -1       nofile:         soft: 262144         hard: 262144    kafka-server:     image: spotify/kafka     environment:       - TOPICS=my-topic     ports:      - '9092:9092'    test:     build: .     depends_on:       - elasticsearch       - kafka-server </code></pre>  <p><strong>update</strong></p>  <p>I found that the problem is caused by using the <code>-p</code> parameter and giving explicit prefix to the container. Still looking for the best way to solve it.</p> ",
    "OwnerUserId": "4068678",
    "LastEditorUserId": "4068678",
    "LastEditDate": "2017-04-20T13:11:45.553",
    "LastActivityDate": "2019-01-12T00:29:39.653",
    "Title": "docker-compose stop not working after docker-compose -p <name> up",
    "Tags": "<docker><docker-compose>",
    "AnswerCount": "2",
    "CommentCount": "7",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p><code>docker-compose -p [project_name] stop</code> worked in my case. I had the same problem.</p> "
  },
  {
    "Id": "44166269",
    "PostTypeId": "1",
    "AcceptedAnswerId": "44187181",
    "CreationDate": "2017-05-24T18:41:21.723",
    "Score": "9",
    "ViewCount": "14497",
    "Body": "<p>I have a docker container from which I am trying to run a pyqt app. Everything works well except a chunk of the GUI is not able to render. The docker logs throw this out:</p>  <pre><code>libGL error: failed to load driver: swrast X Error: GLXBadContext 169  Extension:    154 (Uknown extension)  Minor opcode: 6 (Unknown request)  Resource id:  0x6400003 X Error: BadValue (integer parameter out of range for operation) 2  Extension:    154 (Uknown extension)  Minor opcode: 3 (Unknown request)  Resource id:  0x0 ... QGLContext::makeCurrent(): Failed. </code></pre>  <p>In my Dockerfile, I tried installing pretty much all the packages I could find that might be related, including <code>mesa-utils</code>.</p>  <p>In terms of the docker-compose file, here's what it looks like:</p>  <pre><code>version: '2'     services:     gui:         build: .         volumes:         - .:/usr/src         - /tmp/.X11-unix:/tmp/.X11-unix         command: /bin/bash -c 'python start.py'         environment:         - DISPLAY=unix$DISPLAY         - QT_X11_NO_MITSHM=1         devices:         - '/dev/snd:/dev/snd'         - '/dev/dri:/dev/dri'         privileged: true </code></pre>  <p>Any ideas what I might be missing?</p> ",
    "OwnerUserId": "1631331",
    "LastActivityDate": "2017-05-25T18:11:54.837",
    "Title": "libGL error: failed to load driver swrast in docker container",
    "Tags": "<docker><pyqt><docker-compose><dockerfile><nvidia>",
    "AnswerCount": "1",
    "CommentCount": "1",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>Figured it out. I had to build the <code>gui</code> with hardware accelerated OpenGL support. Theres a repo (<a href='https://github.com/gklingler/docker3d' rel='noreferrer'>https://github.com/gklingler/docker3d</a>) that contains docker images with nvidia or other graphics drivers support. </p>  <p>The other catch was, it didn't work for me unless the host and the container had the <strong>exact</strong> same driver. In order to resolve this, you can run the following shell script if you're running on linux:</p>  <pre><code>#!/bin/bash version='$(glxinfo | grep 'OpenGL version string' | rev | cut -d' ' -f1 | rev)' wget http://us.download.nvidia.com/XFree86/Linux-x86_64/'$version'/NVIDIA-Linux-x86_64-'$version'.run mv NVIDIA-Linux-x86_64-'$version'.run NVIDIA-DRIVER.run </code></pre> ",
    "highest_rated_answer": null
  },
  {
    "Id": "44535976",
    "PostTypeId": "1",
    "AcceptedAnswerId": "44551468",
    "CreationDate": "2017-06-14T05:16:32.410",
    "Score": "9",
    "ViewCount": "21652",
    "Body": "<p>Here is the docker-compose.yml</p>  <pre><code>version: \u201c2\u201d services:   web:    build: .    environment:     MONGO_URI='mongodb://ravimongo:27017'    ports:     \u2014 \u201c3000:3000\u201d    links:     \u2014 ravimongo    depends_on:     \u2014 ravimongo   ravimongo:    image: mongo:3.2.6    ports:      \u2014 \u201c27017:27017\u201d </code></pre>  <p>Here is the error:</p>  <pre><code>ERROR: Version in './docker-compose.yml' is unsupported. You might be seeing this error because you're using the wrong Compose file version. Either specify a supported version ('2.0', '2.1', '3.0') and place your service definitions under the `services` key, or omit the `version` key and place your service definitions at the root of the file to use version 1. For more on the Compose file format versions, see https://docs.docker.com/compose/compose-file/ </code></pre>  <p>Version details are as follows: docker-compose version</p>  <pre><code>docker-compose version 1.11.2, build dfed245 docker-py version: 2.1.0 CPython version: 2.7.12 OpenSSL version: OpenSSL 1.0.2j  26 Sep 2016 </code></pre>  <p>docker version</p>  <pre><code>Client:  Version:      17.03.1-ce  API version:  1.27  Go version:   go1.7.5  Git commit:   c6d412e  Built:        Tue Mar 28 00:40:02 2017  OS/Arch:      darwin/amd64  Server:  Version:      17.03.1-ce  API version:  1.27 (minimum version 1.12)  Go version:   go1.7.5  Git commit:   c6d412e  Built:        Fri Mar 24 00:00:50 2017  OS/Arch:      linux/amd64  Experimental: true </code></pre>  <p>I verified the yaml syntax in <a href='http://www.yamllint.com/' rel='noreferrer'>http://www.yamllint.com/</a> and <a href='https://codebeautify.org/yaml-validator' rel='noreferrer'>https://codebeautify.org/yaml-validator</a>. I am unable to find the problem.</p> ",
    "OwnerUserId": "3871015",
    "LastActivityDate": "2019-09-03T07:52:47.820",
    "Title": "ERROR: Version in './docker-compose.yml' is unsupported. You might be seeing this error because you're using the wrong Compose file version",
    "Tags": "<docker><docker-compose>",
    "AnswerCount": "4",
    "CommentCount": "1",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>The YAML is valid. However, you are using a left double quotation mark <code>\u201c</code> like so:</p>  <p><code>version: \u201c2\u201d</code></p>  <p>Based on the error, seems like Docker Compose is not able to parse the version correctly. If you use a <a href='http://graphemica.com/%E2%80%9C' rel='noreferrer'>left double quotation mark</a> instead of a <a href='http://graphemica.com/%22' rel='noreferrer'>quotation mark</a>, the version which will be picked up by Docker compose will be <code>\u201c2\u201d</code> and not <code>2</code>, and hence it shall not be able to equate it to the supported versions (<code>'2.0'</code>, <code>'2.1'</code>, <code>'3.0'</code>). I would suggest changing it to the following:</p>  <p><code>version: '2'</code></p>  <p>Let me know if the errors still persist.</p> ",
    "highest_rated_answer": "<p>Your editor is injecting smart-quotes instead of normal ascii quotes here:</p>  <pre><code>version: \u201c2\u201d </code></pre>  <p>This needs to be:</p>  <pre><code>version: '2' </code></pre>  <p>I'd recommend not writing yml files with that editor to avoid issues in the future.</p> "
  },
  {
    "Id": "44738469",
    "PostTypeId": "1",
    "CreationDate": "2017-06-24T16:20:56.010",
    "Score": "9",
    "ViewCount": "7110",
    "Body": "<p>I have a spring cloud config server and packaged it as a docker image then I have spring cloud eureka server which is also packaged as docker image.</p>  <p>When I run the two using docker compose I get the following error.</p>  <p><code>discovery-service_1  | 2017-06-24 15:36:12.059  INFO 5 --- [           main] c.c.c.ConfigServicePropertySourceLocator : Fetching config from server at: http://config-service:9001 discovery-service_1  | 2017-06-24 15:36:12.997  WARN 5 --- [           main] c.c.c.ConfigServicePropertySourceLocator : Could not locate PropertySource: I/O error on GET request for 'http://config-service:9001/cls-discovery-service/default': Connection refused (Connection refused); nested exception is java.net.ConnectException: Connection refused (Connection refused) </code></p>  <blockquote>   <p>Although the config service is up and running successfully, discover service still does not find it for some reason.</p> </blockquote>  <p><strong>Docker compose file being used here is this</strong>  <code> version: '2' services:         config-service:                 image: cloudsea/cls-config-service                 ports:                 - 9001:9001                 expose:                 - '9001'         discovery-service:                 image: cloudsea/cls-discovery-service                 depends_on:                 - config-service                 environment:                         CLOUD_SEA_CONFIG_SERVER_URI: http://config-service:9001                         EUREKA_DEFAULT_ZONE_URL: http://discovery-service:8761/eureka/                 ports:                 - 8761:8761                 links:                 - config-service:config-service </code></p>  <p>Below is the <strong>bootstrap.properties</strong> for DISCOVERY SERVICE</p>  <p><code>spring.cloud.config.uri = ${CLOUD_SEA_CONFIG_SERVER_URI:http://localhost:9001} spring.application.name = ${SPRING_APPLICATION_NAME:cls-discovery-service} </code></p>  <p>Below is the <strong>cls-discovery-service.properties</strong> for DISCOVERY SERVICE located in github.</p>  <p><code>server.port=${SERVER_PORT:8761} eureka.client.registerWithEureka: false eureka.client.fetchRegistry: false eureka.client.serviceUrl.defaultZone: ${EUREKA_DEFAULT_ZONE_URL:http://localhost:8761/eureka/} eureka.server.eviction-interval-timer-in-ms: 1000 </code></p>  <p>I am assuming something is wrong with my docker-compose.yml but I am not sure.</p>  <p>Any help will I am stick in this for hours ... heading close to days :(</p> ",
    "OwnerUserId": "3058432",
    "LastActivityDate": "2018-02-06T23:23:47.990",
    "Title": "Spring Cloud Config Server not working with Docker compose",
    "Tags": "<spring-boot><docker-compose><spring-cloud><spring-cloud-netflix><spring-cloud-config>",
    "AnswerCount": "3",
    "CommentCount": "2",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p>I solved it by adding this configuration to the <strong>discovery service's bootstrap.yml</strong>.</p>  <pre><code>spring:   cloud:     config:       failFast: true       retry:         initialInterval: 3000         multiplier: 1.3         maxInterval: 5000         maxAttempts: 20 </code></pre>  <p>Then add <strong>spring-boot-starter-aop</strong> and <strong>spring-retry</strong> to the <strong>discovery service's maven dependencies</strong>.</p>  <pre><code>&lt;dependency&gt;     &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;     &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;     &lt;version&gt;${spring-boot-starter-aop.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt;     &lt;groupId&gt;org.springframework.retry&lt;/groupId&gt;     &lt;artifactId&gt;spring-retry&lt;/artifactId&gt;     &lt;version&gt;${spring-retry.version}&lt;/version&gt; &lt;/dependency&gt; </code></pre>  <p>The problem is they're both starting at the same time. But the discovery service depends on the config service.</p>  <p>When you start the discovery service, it's going to say '<strong>Fetching config from server</strong>' again and again until config service is up.</p>  <p><strong>After the config service starts, discovery service is going to get its configuration successfully</strong> and then it's going to start itself.</p> "
  },
  {
    "Id": "44926335",
    "PostTypeId": "1",
    "AcceptedAnswerId": "44926594",
    "CreationDate": "2017-07-05T12:29:33.263",
    "Score": "9",
    "ViewCount": "15090",
    "Body": "<p>I'm trying to run Elasticsearch using docker compose but I'm not sure how to correctly pass the <code>ES_JAVA_OPTS='-Xms512m -Xmx512m'</code> environmental variable. I've tried lots of combinations of single and double quotes but they all result in: <code>Error: Could not find or load main class '-Xms512m</code>.</p>  <p>My docker-compose config is:</p>  <pre><code>elasticsearch:   image: 'docker.elastic.co/elasticsearch/elasticsearch:5.4.3'   ports:    - '6379:6379'   environment:    - 'http.host=0.0.0.0'    - 'transport.host=127.0.0.1'    - 'xpack.security.enabled=false'    - 'ES_JAVA_OPTS='-Xms512m -Xmx512m'' </code></pre>  <p>This environmental variable works just fine when running the container directly with:</p>  <pre><code>docker run --detach \\   --name elasticsearch \\   --publish 9200:9200 \\   --env 'http.host=0.0.0.0' \\   --env 'transport.host=127.0.0.1' \\   --env 'xpack.security.enabled=false' \\   --env 'ES_JAVA_OPTS=''-Xms512m -Xmx512m''' \\   docker.elastic.co/elasticsearch/elasticsearch:5.4.3 </code></pre>  <p>What am I missing here?</p> ",
    "OwnerUserId": "8799",
    "LastActivityDate": "2022-07-25T08:40:06.707",
    "Title": "Passing ES_JAVA_OPTS variable with spaces when using docker compose",
    "Tags": "<elasticsearch><docker><environment-variables><docker-compose>",
    "AnswerCount": "2",
    "CommentCount": "1",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>According to <a href='https://github.com/docker/compose/issues/2854' rel='nofollow noreferrer'>https://github.com/docker/compose/issues/2854</a>, it's an issue with how docker compose will parse your env variables.</p> <p>If you switch to yaml map instead of list, it should work:</p> <pre class='lang-yaml prettyprint-override'><code>elasticsearch:   image: &quot;docker.elastic.co/elasticsearch/elasticsearch:5.4.3&quot;   ports:     - &quot;6379:6379&quot;   environment:     http.host: 0.0.0.0     transport.host: 127.0.0.1     xpack.security.enabled: &quot;false&quot;     ES_JAVA_OPTS: -Xms512m -Xmx512m </code></pre> ",
    "highest_rated_answer": "<p>It's an issue with Docker compose and spaces.</p> <h2>Just replace with</h2> <pre><code>&quot;ES_JAVA_OPTS=-Xmx512m -Xms512m&quot; </code></pre> <p><em>See <a href='https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html' rel='noreferrer'>https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html</a></em></p> "
  },
  {
    "Id": "45160692",
    "PostTypeId": "1",
    "AcceptedAnswerId": "45163491",
    "CreationDate": "2017-07-18T07:55:15.203",
    "Score": "9",
    "ViewCount": "8897",
    "Body": "<p>I have just started out with Docker, and I am currently trying to run <code>docker-compose run --rm setup</code> on a docker-compose.yml file, but whenever I do, I receive the following:</p>  <pre><code>Traceback (most recent call last):   File '/home/wickywills/.local/bin/docker-compose', line 11, in &lt;module&gt;     sys.exit(main())   File '/home/wickywills/.local/lib/python2.7/site-packages/compose/cli/main.py', line 68, in main     command()   File '/home/wickywills/.local/lib/python2.7/site-packages/compose/cli/main.py', line 118, in perform_command     handler(command, command_options)   File '/home/wickywills/.local/lib/python2.7/site-packages/compose/cli/main.py', line 750, in run     run_one_off_container(container_options, self.project, service, options)   File '/home/wickywills/.local/lib/python2.7/site-packages/compose/cli/main.py', line 1136, in run_one_off_container     rescale=False   File '/home/wickywills/.local/lib/python2.7/site-packages/compose/project.py', line 388, in up     warn_for_swarm_mode(self.client)   File '/home/wickywills/.local/lib/python2.7/site-packages/compose/project.py', line 614, in warn_for_swarm_mode     info = client.info()   File '/home/wickywills/.local/lib/python2.7/site-packages/docker/api/daemon.py', line 90, in info     return self._result(self._get(self._url('/info')), True)   File '/home/wickywills/.local/lib/python2.7/site-packages/docker/utils/decorators.py', line 46, in inner     return f(self, *args, **kwargs)   File '/home/wickywills/.local/lib/python2.7/site-packages/docker/api/client.py', line 189, in _get     return self.get(url, **self._set_request_timeout(kwargs))   File '/home/wickywills/.local/lib/python2.7/site-packages/requests/sessions.py', line 515, in get     return self.request('GET', url, **kwargs)   File '/home/wickywills/.local/lib/python2.7/site-packages/requests/sessions.py', line 502, in request     resp = self.send(prep, **send_kwargs)   File '/home/wickywills/.local/lib/python2.7/site-packages/requests/sessions.py', line 612, in send     r = adapter.send(request, **kwargs)   File '/home/wickywills/.local/lib/python2.7/site-packages/requests/adapters.py', line 440, in send     timeout=timeout   File '/home/wickywills/.local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py', line 582, in urlopen     timeout_obj = self._get_timeout(timeout)   File '/home/wickywills/.local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py', line 309, in _get_timeout     return Timeout.from_float(timeout)   File '/home/wickywills/.local/lib/python2.7/site-packages/requests/packages/urllib3/util/timeout.py', line 154, in from_float     return Timeout(read=timeout, connect=timeout)   File '/home/wickywills/.local/lib/python2.7/site-packages/requests/packages/urllib3/util/timeout.py', line 97, in __init__     self._connect = self._validate_timeout(connect, 'connect')   File '/home/wickywills/.local/lib/python2.7/site-packages/requests/packages/urllib3/util/timeout.py', line 127, in _validate_timeout     'int or float.' % (name, value)) ValueError: Timeout value connect was Timeout(connect=60, read=60, total=None), but it must be an int or float. </code></pre>  <h2>Pip Freeze</h2>  <pre><code>backports.ssl-match-hostname==3.5.0.1 boto==2.40.0 cached-property==1.3.0 certifi==2017.4.17 chardet==3.0.4 colorama==0.3.9 cryptography==1.5 docker==2.4.2 docker-compose==1.14.0 docker-pycreds==0.2.1 dockerpty==0.4.1 docopt==0.6.2 duplicity==0.7.6 enum34==1.1.6 functools32==3.2.3.post2 idna==2.5 ipaddress==1.0.18 jsonschema==2.6.0 lockfile==0.12.2 mysql-connector-python==2.1.3 mysql-utilities==1.6.3 ndg-httpsclient==0.4.2 paramiko==2.0.0 pexpect==4.2.0 ptyprocess==0.5.1 pyasn1==0.1.9 pycrypto==2.6.1 pygobject==3.22.0 pyodbc==3.0.10 pyOpenSSL==16.1.0 pysqlite==2.7.0 python-cloudfiles==1.7.10 PyYAML==3.12 requests==2.18.1 six==1.10.0 texttable==0.8.8 urllib3==1.21.1 websocket-client==0.44.0 </code></pre>  <p>Seems like this is a common issue, without a real solution. I am running Ubuntu 16.10, and followed the installation instructions as per the Docker documentation. Can anyone advise?</p> ",
    "OwnerUserId": "2136354",
    "LastEditorUserId": "2136354",
    "LastEditDate": "2017-07-18T10:33:48.150",
    "LastActivityDate": "2023-05-23T17:29:19.713",
    "Title": "Docker - Timeout value connect was Timeout",
    "Tags": "<python><docker><python-requests><docker-compose>",
    "AnswerCount": "3",
    "CommentCount": "5",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>Your error looks similar to <a href='https://github.com/docker/compose/issues/4927' rel='noreferrer'>this issue</a>. The user closed the issue saying it was fixed by a new release of requests, so I would try upgrading <code>requests</code> in your virtual environment:</p>  <pre><code>pip install --upgrade requests </code></pre>  <p>From the comments, it sounds like you actually need to uninstall and then reinstall, rather than just upgrading.</p>  <pre><code>pip uninstall requests pip install requests </code></pre> ",
    "highest_rated_answer": "<p>Just in case someone have the same problem as me (same error message) the previous commands were not enough to correct the problem.</p>  <p>There was an old useless urllib3 install inside requests... So I did:</p>  <pre><code>rm -rf ~/.local/lib/python2.7/site-packages/requests/packages/urllib3/ </code></pre>  <p>And it worked!</p> "
  },
  {
    "Id": "45193939",
    "PostTypeId": "1",
    "CreationDate": "2017-07-19T14:40:57.183",
    "Score": "9",
    "ViewCount": "3745",
    "Body": "<p>In a Docker Compose file, I can easily publish a range of ports using the short-form syntax: </p>  <pre><code>ports:   - '3000-3010:3000-3010/udp' </code></pre>  <p>But in my case, I need those ports as 'mode=host' to bypass the swarm overlay network. The short-form syntax can't express that, so I need to use the long-form:</p>  <pre><code>ports:   - published: '3000-3010'     target: '3000-3010'     protocol: udp     mode: host </code></pre>  <p>However, it seems that Docker doesn't like specifying ranges with the long-form syntax, as I get that error when deploying a stack: </p>  <blockquote>   <p>services.test.ports.0.target must be a integer</p> </blockquote>  <p>Is there a way to do that (except brute-force by specifying each and every port in the range as long-form) ?</p> ",
    "OwnerUserId": "8332359",
    "LastActivityDate": "2018-02-02T00:09:46.267",
    "Title": "How to publish a range of ports with 'mode=host' with a Docker Compose file?",
    "Tags": "<docker><docker-compose><docker-swarm>",
    "AnswerCount": "1",
    "CommentCount": "2",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p>This is not possible at the moment.</p>  <p>As per my discussion about this feature on official docker slack channel, exposing range of ports using long format syntax (which is the only syntax right now which you can use to publish ports in host mode) is not possible.</p>  <p>Having in mind that there is an <a href='https://github.com/docker/compose/issues/5613' rel='noreferrer'>open ticke</a>t related with this matter, I suppose it will be possible in near future.</p> "
  },
  {
    "Id": "45306484",
    "PostTypeId": "1",
    "CreationDate": "2017-07-25T14:37:06.180",
    "Score": "9",
    "ViewCount": "11333",
    "Body": "<p>I'm trying to deploy a very simple Symfony application using nginx &amp; php-fpm via Docker.</p>  <p>Two docker services :<br> 1. web : running nginx<br> 2. php : running php-fpm; containing application source.</p>  <p>I want to build images that can be deployed without any external dependency. That's why I'm copying source code within the <strong>php container</strong>.<br> On development process; i'm overriding <strong>/var/www/html</strong> volume with local path.</p>  <pre><code># file: php-fpm/Dockerfile FROM php:7.1-fpm-alpine  COPY ./vendor /var/www/html COPY . /var/www/html  VOLUME /var/www/html </code></pre>  <p>Now the docker-compose configuration file.</p>  <pre><code># file : docker-compose-prod.yml version: '2' services:   web:     image: 'private/web'     ports:       - 80:80     volumes_from:       - php   php:     image: 'private/php'     ports:       - 9000:9000 </code></pre>  <p>The problem is about permissions.<br> When accessing localhost, Symfony is botting up, but <strong>cache</strong> / <strong>logs</strong> / <strong>sessions</strong> folders are not writable.</p>  <ol> <li>nginx is using <strong>/var/www/html</strong> to serve static files.  </li> <li>php-fpm is using <strong>/var/www/html</strong> to execute php files.</li> </ol>  <p>I'm not sure about the problem.  But how can I be sure about the following:</p>  <ol> <li><strong>/var/www/html</strong> have to be readable for nginx ?  </li> <li><strong>/var/www/html</strong> have to be writable for php-fpm ?  </li> </ol>  <p>Note: I'm building images from MacbookPro; <strong>cache / logs / sessions</strong> are 777.</p> ",
    "OwnerUserId": "1642991",
    "LastActivityDate": "2020-01-28T23:42:26.393",
    "Title": "How to deal with permissions using docker - nginx / php-fpm",
    "Tags": "<php><symfony><nginx><docker><docker-compose>",
    "AnswerCount": "3",
    "CommentCount": "1",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p>docker-compose.yml supports a <a href='https://docs.docker.com/engine/reference/run/#user' rel='noreferrer'><code>user</code></a> directive under services. The docs only mention it in the <code>run</code> command, but it works the same.</p>  <p>I have a similar setup and this is how I do it:</p>  <pre><code># file : docker-compose-prod.yml version: '2' services:   web:     image: 'private/web'     ports:       - 80:80     volumes_from:       - php   php:     image: 'private/php'     ports:       - 9000:9000     user: '$UID' </code></pre>  <p>I have to run <code>export UID</code> before running <code>docker-compose</code> and then that sets the default user to my current user. This allows logging / caching etc. to work as expected.</p> "
  },
  {
    "Id": "45381623",
    "PostTypeId": "1",
    "AcceptedAnswerId": "45381801",
    "CreationDate": "2017-07-28T20:07:23.690",
    "Score": "9",
    "ViewCount": "14438",
    "Body": "<p>I have this <code>docker-compose.yml</code> file</p>  <pre><code>version: '3'  volumes:   jenkins_home:  services:    registry:      image: registry:2      ports:        - '5000:5000'    jenkins:     image: jenkins/jenkins     ports:       - '9090:8080'     volumes:       - jenkins_home:/var/jenkins_home </code></pre>  <p>As you can see, there is a named volume called <code>jenkins_home</code>, now I wonder, where does the data get really persisted?</p>  <p>running <code>docker inspect infra_jenkins</code> i got this:</p>  <pre><code> ...  'Mounts': [     {       'Type': 'volume',       'Source': 'infra_jenkins_home',       'Target': '/var/jenkins_home',       'VolumeOptions': {       'Labels': {       'com.docker.stack.namespace': 'infra'               }           }       } ], ... </code></pre>  <p>I am running those services on a local docker swarm cluster using <code>docker stack deploy</code> command, the cluster is composed of three VirtualBox instances.</p> ",
    "OwnerUserId": "2429966",
    "LastActivityDate": "2018-08-31T12:38:21.610",
    "Title": "Where docker named volumes are stored?",
    "Tags": "<docker><docker-compose><dockerfile>",
    "AnswerCount": "1",
    "CommentCount": "0",
    "ClosedDate": "2017-07-30T07:21:20.573",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>You can inspect docker volumes and see detailed informations.</p>  <p>See <a href='https://docs.docker.com/engine/reference/commandline/volume_inspect/#usage' rel='nofollow noreferrer'>docker volume reference</a></p>  <p>Edit (commented suggestion):</p>  <p>Alternatively (for exact answer) see the <a href='https://stackoverflow.com/questions/45271420/docker-where-is-docker-volume-located-for-this-compose-file'>already answered question</a>.</p> ",
    "highest_rated_answer": null
  },
  {
    "Id": "45412799",
    "PostTypeId": "1",
    "AcceptedAnswerId": "45420160",
    "CreationDate": "2017-07-31T10:00:41.980",
    "Score": "9",
    "ViewCount": "22561",
    "Body": "<p>My docker-compose have a two  service, and docker-compose.yml define enviroment variable  ip address with container name,</p>  <pre><code> version: '2'  services:   api:     build: ./api/     command: python3 manage.py runserver     volumes:       - ./api:/code     ports:       - '8000:80'     networks:       - dock_net     container_name: con_api    web:     build: ./web/     command: python3 manage.py runserver     volumes:       - ./web:/code     ports:       - '8001:80'     networks:       - dock_net     container_name: con_web     environment:         Ip:con_ip  networks:   dock_net:       driver: bridge </code></pre>  <p>But variable see 'con_ip' not 127.0.0.3</p> ",
    "OwnerUserId": "8392832",
    "LastActivityDate": "2017-07-31T17:34:35.167",
    "Title": "Docker-Compose container ip address with container name",
    "Tags": "<docker><docker-compose>",
    "AnswerCount": "1",
    "CommentCount": "4",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>I don't think that you are properly using environment variables. Please refer <a href='https://docs.docker.com/compose/environment-variables/#passing-environment-variables-through-to-containers' rel='noreferrer'>environment variables in compose</a>.</p>  <p>You can access one container from other container simply by using service name of that container. And this is the recommended way.</p>  <p>But if you prefer IP addresses for your own reasons, I am telling you how to set the static ip address of container, i would not recommend it though.</p>  <pre><code>version: '2' services:   api:     build: ./api/     command: python3 manage.py runserver     volumes:       - ./api:/code     ports:       - '8000:80'     networks:       - dock_net:           ipv4_address: 127.0.0.3     container_name: con_api    web:     build: ./web/     command: python3 manage.py runserver     volumes:       - ./web:/code     ports:       - '8001:80'     networks:       - dock_net:           ipv4_address: 127.0.0.4     container_name: con_web  networks:   dock_net:     driver: bridge     ipam:      config:        - subnet: 127.0.0.0/8          gateway: 127.0.0.1 </code></pre>  <p>This will assign the required IP-addresses to your containers. <code>API</code> will be at <code>127.0.0.3</code> and <code>web</code> will be at <code>127.0.0.4</code></p>  <p><strong>EDIT:</strong> If you want to access service named <code>api</code> from inside the web container then you can use its ip address as we have allocated here. <a href='http://127.0.0.3:80/' rel='noreferrer'>http://127.0.0.3:80/</a> or you can also use <a href='http://api:80/' rel='noreferrer'>http://api:80/</a></p>  <p>The <code>api</code> is used in place of IP address because its the service name and when no hostname is given, service name is taken as default hostname. If you want to know about hostnames refer to <a href='https://stackoverflow.com/questions/29924843/how-do-i-set-hostname-in-docker-compose'>this</a> question.</p> ",
    "highest_rated_answer": null
  },
  {
    "Id": "45658144",
    "PostTypeId": "1",
    "AcceptedAnswerId": "45658385",
    "CreationDate": "2017-08-13T07:32:21.063",
    "Score": "9",
    "ViewCount": "14187",
    "Body": "<p>I have an application using Flask and MySQL. The application does not connect to MySQL container from the Flask Application but it can be accessed using Sequel Pro with the same credentials.</p>  <p><strong>Docker Compose File</strong></p>  <pre><code>version: '2'  services:   web:     build: flask-app     ports:      - '5000:5000'     volumes:      - .:/code   mysql:     build: mysql-server     environment:       MYSQL_DATABASE: test       MYSQL_ROOT_PASSWORD: root       MYSQL_ROOT_HOST: 0.0.0.0       MYSQL_USER: testing       MYSQL_PASSWORD: testing     ports:       - '3306:3306' </code></pre>  <p><strong>Docker file for MySQL</strong></p>  <p>The docker file for MySQL will add schema from <code>test.dump</code> file.</p>  <pre><code>FROM mysql/mysql-server ADD test.sql  /docker-entrypoint-initdb.d </code></pre>  <p><strong>Docker file for Flask</strong></p>  <pre><code>FROM python:latest COPY . /app WORKDIR /app RUN pip install -r requirements.txt ENTRYPOINT ['python'] CMD ['app.py'] </code></pre>  <p><strong>Starting point <code>app.py</code></strong></p>  <pre><code>from flask import Flask, request, jsonify, Response import json import mysql.connector from flask_cors import CORS, cross_origin  app = Flask(__name__)  def getMysqlConnection():     return mysql.connector.connect(user='testing', host='0.0.0.0', port='3306', password='testing', database='test')  @app.route('/') def hello():     return 'Flask inside Docker!!'  @app.route('/api/getMonths', methods=['GET']) @cross_origin() # allow all origins all methods. def get_months():     db = getMysqlConnection()     print(db)     try:         sqlstr = 'SELECT * from retail_table'         print(sqlstr)         cur = db.cursor()         cur.execute(sqlstr)         output_json = cur.fetchall()     except Exception as e:         print('Error in SQL:\\n', e)     finally:         db.close()     return jsonify(results=output_json)  if __name__ == '__main__':     app.run(debug=True,host='0.0.0.0') </code></pre>  <p>When I do a GET request on <code>http://localhost:5000/</code> using REST Client I get a valid response. </p>  <p>A GET request on <code>http://localhost:5000/api/getMonths</code> gives error message:</p>  <pre><code>mysql.connector.errors.InterfaceError: 2003: Can't connect to MySQL server on '0.0.0.0:3306' (111 Connection refused) </code></pre>  <p>When the same credentials were used on Sequel Pro, I was able to access the database. <a href='https://i.stack.imgur.com/LdPon.png' rel='noreferrer'><img src='https://i.stack.imgur.com/LdPon.png' alt='enter image description here'></a></p>  <p>Please advice me on  how to connect the MySQL container from the Flask Application. This is my first time suing Docker and do forgive me if this is a silly mistake from my part.</p> ",
    "OwnerUserId": "5165377",
    "LastActivityDate": "2020-03-19T08:05:29.803",
    "Title": "Connecting to MySQL from Flask Application using docker-compose",
    "Tags": "<docker><docker-compose><mysql-python>",
    "AnswerCount": "2",
    "CommentCount": "0",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>Change this </p>  <pre><code>return mysql.connector.connect(user='testing', host='0.0.0.0', port='3306', password='testing', database='test') </code></pre>  <p>to</p>  <pre><code>return mysql.connector.connect(user='testing', host='mysql', port='3306', password='testing', database='test') </code></pre>  <p>Your code is running inside the container and not on your host. So you need to provide it a address where it can reach within container network. For <code>docker-compose</code> each service is reachable using its name. So in your it is <code>mysql</code> as that is name you have used for the service</p> ",
    "highest_rated_answer": "<p>For others who encounter similar issue, if you are mapping different ports from host to container for the MySQL service, make sure that container that needs to connect to the MySQL service is using the port for the container not for the host.</p>  <p>Here is an example of a docker compose file. Here you can see that my application (which is running in a container) will be using port 3306 to connect to the MySQL service (which is also running in a container on port 3306). Anyone connecting to this MySQL service from the outside of the 'backend' network which is basically anything that does not run in a container with the same network will need to use port 3308 to connect to this MySQL service.</p>  <pre class='lang-yaml prettyprint-override'><code>version: '3' services:    redis:     image: redis:alpine     command: redis-server --requirepass imroot     ports:       - '6379:6379'     networks:       - frontend    mysql:     image: mariadb:10.5     command: --default-authentication-plugin=mysql_native_password     ports:       - '3308:3306'     volumes:       - mysql-data:/var/lib/mysql/data     networks:       - backend     environment:       MYSQL_ROOT_PASSWORD: imroot       MYSQL_DATABASE: test_junkie_hq       MYSQL_HOST: 127.0.0.1    test-junkie-hq:     depends_on:       - mysql       - redis     image: test-junkie-hq:latest     ports:       - '80:5000'     networks:       - backend       - frontend     environment:       TJ_MYSQL_PASSWORD: imroot       TJ_MYSQL_HOST: mysql       TJ_MYSQL_DATABASE: test_junkie_hq       TJ_MYSQL_PORT: 3306       TJ_APPLICATION_PORT: 5000       TJ_APPLICATION_HOST: 0.0.0.0  networks:   backend:   frontend:  volumes:   mysql-data:  </code></pre> "
  },
  {
    "Id": "45711618",
    "PostTypeId": "1",
    "AcceptedAnswerId": "45713363",
    "CreationDate": "2017-08-16T10:52:54.577",
    "Score": "9",
    "ViewCount": "10616",
    "Body": "<p>I am trying to setup mysql to run in a docker container. I have a simple docker compose file :-</p>  <pre><code>db:   image: mysql:latest   ports:     - '3306:3306'   environment:     - MYSQL_RANDOM_ROOT_PASSWORD=yes </code></pre>  <p>when i run the docker-compose file, I get the following warning in docker logs instead of the random generated password.</p>  <blockquote>   <p>[Warning] root@localhost is created with an empty password ! Please   consider switching off the --initialize-insecure option.</p> </blockquote>  <p>Is there something I am missing?</p> ",
    "OwnerUserId": "7877397",
    "LastActivityDate": "2020-12-03T22:54:30.453",
    "Title": "Docker-compose mysql does not seem to recognize environment variable MYSQL_RANDOM_ROOT_PASSWORD",
    "Tags": "<mysql><docker><docker-compose>",
    "AnswerCount": "1",
    "CommentCount": "0",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>I tried the same and it worked. I think you're missing something in the logs. In the beginning there appears:</p>  <blockquote>   <p>root@localhost is created with an empty password ! Please consider   switching off the --initialize-insecure option.</p> </blockquote>  <p>But check a bit later. The keys are created and a bit further you'll find something like this:</p>  <blockquote>   <p>GENERATED ROOT PASSWORD: Cet6Xeiheecohleal9uexxxxDathah</p> </blockquote>  <p>You can find it more easy by using this command (replace with your container ID):</p>  <pre><code>docker logs da530043c63a 2&gt;/dev/null | grep 'GENERATED ROOT PASSWORD' </code></pre> ",
    "highest_rated_answer": null
  },
  {
    "Id": "45889233",
    "PostTypeId": "1",
    "CreationDate": "2017-08-25T20:50:08.943",
    "Score": "9",
    "ViewCount": "8528",
    "Body": "<p>I have an existing app that uses a app config file that looks like:</p>  <pre><code>'ConnectionInfo': {     'ServerName': 'The Server URL',     'DatabaseName': 'The DatabaseName',     'UserName': 'The User Name',     'Password': 'The Password'} </code></pre>  <p>Now, when I have a simple setting, say</p>  <pre><code>'ConnectionString':'My Connection String' </code></pre>  <p>I understand how to override it in the compose.yml file:</p>  <pre><code>environment:   - ConnectionString=what I want it to be </code></pre>  <p>The question is, how do you set, say, the server name in the top?</p> ",
    "OwnerUserId": "4130724",
    "LastEditorUserId": "382838",
    "LastEditDate": "2017-12-08T19:00:03.380",
    "LastActivityDate": "2020-02-25T08:52:21.750",
    "Title": "Docker Compose Nested Environment Variable",
    "Tags": "<docker><asp.net-core><environment-variables><docker-compose>",
    "AnswerCount": "2",
    "CommentCount": "5",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p>Please use double underscore (__) as the following instead of a colon (:).</p>  <pre><code>environment:   - ConnectionInfo__ServerName=MyServerName </code></pre>  <p>Please refer to <a href='https://learn.microsoft.com/en-us/aspnet/core/fundamentals/configuration/?view=aspnetcore-2.1&amp;tabs=basicconfiguration#configuration-by-environment' rel='noreferrer'>Configuration in ASP.NET Core</a></p>  <blockquote>   <p>For hierarchical config values specified in environment variables, a   colon (:) may not work on all platforms. Double underscore (__) is   supported by all platforms.</p> </blockquote> "
  },
  {
    "Id": "46007682",
    "PostTypeId": "1",
    "AcceptedAnswerId": "46007759",
    "CreationDate": "2017-09-01T20:37:36.663",
    "Score": "9",
    "ViewCount": "10627",
    "Body": "<p>What is an appropriate way to integration test systems that use ASB?  With something like Kafka and using docker-compose, I could spin up two services that will communicate asynchronously over Kafka.  Is there a way to do something similar with ASB?  If not, what is a common integration testing pattern?</p> ",
    "OwnerUserId": "2410065",
    "LastActivityDate": "2020-07-15T06:45:37.357",
    "Title": "Integration testing Azure Service Bus with Docker",
    "Tags": "<azure><docker-compose><integration-testing><azureservicebus>",
    "AnswerCount": "3",
    "CommentCount": "0",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>The pricing model for Service Bus has <a href='https://azure.microsoft.com/en-ca/pricing/details/service-bus/' rel='noreferrer'>12.5 million operations per month included for free</a>. Past that, it's less than a dollar per millions of messages sent. With these kinds of services it should be simple for you to spin up and tear down instances with absolutely zero cost to you as part of integration tests. </p>  <p>NUnit, for example provides the <code>[OneTimeSetup]</code> and <code>[OneTimeTearDown]</code> methods which you could use as part of an integration test suite to provision and subsequently delete a Service Bus instance.</p> ",
    "highest_rated_answer": "<p>Sadly, I don't think <a href='https://learn.microsoft.com/en-us/azure/storage/common/storage-use-emulator' rel='nofollow noreferrer'>Azure Storage Emulator</a> supports the service bus yet, but you could extract the functionality to an interface and either write a fake ASB and use that or have the interface use the queue in the Emulator. If you want to use the actual ASB and don't mind incurring costs, you could spin up a new one with a guid for a name or if create evanescent queues or topics/subscriptions with guids for their names as well, then tear it all down when you're done.</p>  <p>Not ideal perhaps, but it gives you some options. Perhaps if you shared more detail about what you're using ASB for, we could offer better solutions.</p> "
  },
  {
    "Id": "46174297",
    "PostTypeId": "1",
    "CreationDate": "2017-09-12T10:41:32.107",
    "Score": "9",
    "ViewCount": "8008",
    "Body": "<p>I'm trying to run a small docker-compose app inside a container-optimized Google Cloud Compute Engine node, but I'm getting stuck when it's trying to mount volumes during a <code>docker-compose up</code>:</p>  <pre><code>Creating lightning_redis_1 ...  Creating lightning_db_1 ...  Creating lightning_redis_1 Creating lightning_db_1 ... done Creating lightning_api_1 ...  Creating lightning_api_1 ... error ERROR: for lightning_api_1  Cannot start service api: error while creating mount source path '/rootfs/home/jeremy/lightning': mkdir /rootfs: read-only file sys tem ERROR: for api  Cannot start service api: error while creating mount source path '/rootfs/home/jeremy/lightning': mkdir /rootfs: read-only file system Encountered errors while bringing up the project. jeremy@instance-1 ~/lightning $  </code></pre>  <p>My docker-compose.yml file looks like this:</p>  <pre><code>version: '3' services:   client:     build: ./client     volumes:       - ./client:/usr/src/app     ports:       - '4200:4200'       - '9876:9876'     links:       - api     command: bash -c 'yarn --pure-lockfile &amp;&amp; yarn start'   sidekiq:     build: .     command: bundle exec sidekiq     volumes:       - .:/api     depends_on:       - db       - redis       - api   redis:     image: redis     ports:       - '6379:6379'   db:     image: postgres     ports:       - '5433:5432'   api:     build: .     command: bash -c 'rm -f tmp/pids/server.pid &amp;&amp; bundle exec rails s -p 3000 -b '0.0.0.0''     volumes:       - .:/myapp     ports:       - '3000:3000'     depends_on:       - db </code></pre>  <p>I don't want to have to change anything in the docker-compose.yml file - I'd prefer to be able to fix this issue by running commands inside the VM itself, or in how I set the VM up. Reason being is it's not my code and I can't change the docker-compose.yml file easily, and all I need to do is run it for a short period of time and execute a few docker-compose commands inside the VM.</p> ",
    "OwnerUserId": "4382519",
    "LastActivityDate": "2020-05-06T19:07:01.223",
    "Title": "Running docker-compose inside a Google Cloud Engine",
    "Tags": "<docker-compose><google-compute-engine>",
    "AnswerCount": "2",
    "CommentCount": "0",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p>Container optimized OS usually mounts most of the paths as read-only. That is why you are getting the error </p>  <pre><code>source path '/rootfs/home/jeremy/lightning': mkdir /rootfs: read-only file sys </code></pre>  <p>So you have few options</p>  <p><strong>Use named volumes in docker-compose</strong> </p>  <p>You will need to change your volumes like below</p>  <pre><code>volumes:   - myappvol:/myapp </code></pre>  <p>and define the top level volumes in compose</p>  <pre><code>volumes:   myappvol: {} </code></pre>  <p>As you said you don't want to modify the yaml then this may not work for you</p>  <p><strong>Run docker-compose inside docker</strong></p>  <p>Currently you run <code>docker-compose</code> on the main machine, instead you should use <code>docker-compose</code> inside another docker container which has the main root folder</p>  <pre><code>docker run \\     -v /var/run/docker.sock:/var/run/docker.sock \\     -v '$PWD:/rootfs/$PWD' \\     -w='/rootfs/$PWD' \\     docker/compose:1.13.0 up </code></pre>  <p>This would work but the data would be persisted inside the docker container itself. </p>  <p>See below article for more details</p>  <p><a href='https://cloud.google.com/community/tutorials/docker-compose-on-container-optimized-os' rel='noreferrer'>https://cloud.google.com/community/tutorials/docker-compose-on-container-optimized-os</a></p> "
  },
  {
    "Id": "46239662",
    "PostTypeId": "1",
    "AcceptedAnswerId": "46239994",
    "CreationDate": "2017-09-15T12:31:12.953",
    "Score": "9",
    "ViewCount": "8945",
    "Body": "<p>I'm unable to get the Docker Compose <a href='https://docs.docker.com/compose/compose-file/#domainname-hostname-ipc-mac_address-privileged-read_only-shm_size-stdin_open-tty-user-working_dir' rel='noreferrer'><code>hostname</code></a> command to work. </p>  <p>I'm running a simple <code>docker-compose.yml</code>:</p>  <pre><code>version: '3' services:   redis1:     image: 'redis:alpine'     hostname: redis1host   redis2:     image: 'redis:alpine'     hostname: redis2host </code></pre>  <p>Once I run this with <code>docker-compose up</code>, I should be able to run <code>docker-compose exec redis1 /bin/ash</code> and then <code>ping redis2host</code> to talk to the other Redis container, but the ping just doesn't reach its destination. I can ping the other Redis container with <code>ping redis2</code>.</p>  <p><code>ping redishost2</code> should work, no?</p> ",
    "OwnerUserId": "729819",
    "LastEditorUserId": "147356",
    "LastEditDate": "2017-09-15T12:41:12.227",
    "LastActivityDate": "2017-09-15T12:48:56.520",
    "Title": "Docker Compose hostname command not working",
    "Tags": "<docker><docker-compose>",
    "AnswerCount": "1",
    "CommentCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>The <code>hostname</code> directive simply sets the hostname inside the container (that is, the name you get back in response to the <code>hostname</code> or <code>uname -n</code> commands). It does not result in a DNS alias for the service.  For that, you want the <a href='https://docs.docker.com/compose/compose-file/#aliases' rel='noreferrer'>aliases</a> directive. Since that directive is per-network, you need to be explicit about networks rather than using the compose default, for example:</p>  <pre><code>version: '3' services:   redis1:     image: 'redis:alpine'     hostname: redis1host     networks:       redis:         aliases:           - redis1host   redis2:     image: 'redis:alpine'     hostname: redis2host     networks:       redis:         aliases:           - redis2host  networks:   redis: </code></pre> ",
    "highest_rated_answer": null
  },
  {
    "Id": "46536361",
    "PostTypeId": "1",
    "CreationDate": "2017-10-03T02:29:28.987",
    "Score": "9",
    "ViewCount": "15023",
    "Body": "<p>I'm using Docker Toolbox on Windows 10</p>  <p>I can access the php part succesfully via <a href='http://192.168.99.100:8000' rel='noreferrer'>http://192.168.99.100:8000</a>, I have been working around with the mariadb part but still having several problems</p>  <p>I have an sql file as <code>/mariadb/initdb/abc.sql</code> so I should be copied into <code>/docker-entrypoint-initdb.d</code>, after the container is created I use <code>docker-compose exec mariadb</code> to access the container, there is the file as <code>/docker-entrypoint-initdb.d/abc.sql</code> but the file never get executed, I also have tested to import the sql file to the container manually, it was succesful so the sql file is valid</p>  <p>I don't quite understand about the data folder mapping, and what to do to get the folder sync with the container, I always get the warning when recreate the container using <code>docker-compose up -d</code></p>  <p><code>WARNING: Service 'mariadb' is using volume '/var/lib/mysql' from the previous container. Host mapping '/.../mariadb/data' has no effect. Remove the existing containers (with docker-compose rm mariadb) to use the Recreating db ... done</code></p>  <p><strong>Questions</strong></p>  <ol> <li>How to get the sql file in <code>/docker-entrypoint-initdb.d</code> to be executed ?</li> <li>What is the right way to map the data folder with the mariadb container ?</li> </ol>  <p>Please guide Thanks</p>  <p>This is my <code>docker-compose.yml</code></p>  <pre><code>version: '3.2' services:     php:         image: php:7.1-apache         container_name: web         restart: always         volumes:             - /.../php:/var/www/html         ports:             - '8000:80'     mariadb:         image: mariadb:latest         container_name: db         restart: always         environment:             - MYSQL_ROOT_PASSWORD=12345         volumes:             - /.../mariadb/initdb:/docker-entrypoint-initdb.d             - /.../mariadb/data:/var/lib/mysql         ports:             - '3306:3306' </code></pre> ",
    "OwnerUserId": "1536973",
    "LastActivityDate": "2020-05-25T11:38:15.090",
    "Title": "File in docker-entrypoint-initdb.d never get executed when using docker compose",
    "Tags": "<docker><docker-compose><docker-machine>",
    "AnswerCount": "2",
    "CommentCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p>For me the issue was the fact that Docker didn't clean up my mounted volumes from previous runs.</p>  <p>Doing a:</p>  <pre><code>docker volume ls </code></pre>  <p>Will list any volumes, and if previous exist, then run 'rm' command on the volume to remove it.</p>  <p>As stated on docker mysql <a href='https://hub.docker.com/_/mysql/' rel='noreferrer'>docks</a>, scripts in the '/docker-entrypoint-initdb.d' folder is only evalutated the first time the container runs, and if a previous volume remains, it won't run the scripts.</p>  <p>As for the mapping, you simply need to mount your script folder to the '/docker-entrypoint-initdb.d' folder in the image:</p>  <pre><code>volumes:    - ./db/:/docker-entrypoint-initdb.d </code></pre>  <p>I have a single script file in a folder named db, relative to my docker-compose file.</p> "
  },
  {
    "Id": "47067944",
    "PostTypeId": "1",
    "AcceptedAnswerId": "47068162",
    "CreationDate": "2017-11-02T05:05:06.100",
    "Score": "9",
    "ViewCount": "20864",
    "Body": "<p>I hosted Git daemon on local host i.e. <code>'/usr/bin/git daemon --listen=127.0.0.1 --base-path=/opt'</code> as a <code>systemd</code> service and I am trying to access it from docker container. I didn't mentioned the port because I don't want to expose the port to outside network.</p>  <p>Dockerfile:</p>  <pre><code>RUN git clone git://127.0.0.1/repo/ repo_dir </code></pre>  <p>But its not working, its looks like inside container its trying to connect localhost of container.</p>  <p>So How to connect connect localhost of Host machine from Docker container?</p> ",
    "OwnerUserId": "2670520",
    "LastEditorUserId": "6309",
    "LastEditDate": "2017-11-02T05:25:57.780",
    "LastActivityDate": "2020-06-02T08:32:50.423",
    "Title": "How to access the Host's machine's localhost 127.0.0.1 from docker container",
    "Tags": "<docker><docker-compose><dockerfile><git-daemon>",
    "AnswerCount": "1",
    "CommentCount": "2",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<blockquote>   <p>its not working, its looks like inside container its trying to connect localhost of container.</p> </blockquote>  <p>Yes, that is the all idea behind the isolation provided by container, even <code>docker build</code> (which builds one container per Dockerfile line, and commits it in an intermediate image).</p>  <p>As commented by the <a href='https://stackoverflow.com/users/2670520/dhairya'>OP dhairya</a>, and mentioned in the documentation I referred in my comments: '<a href='https://docs.docker.com/engine/userguide/networking/' rel='noreferrer'>Docker container networking</a>' and <a href='https://docs.docker.com/engine/reference/commandline/build/' rel='noreferrer'><code>docker build</code></a>, you can set to <strong>host</strong> the networking mode for the <code>RUN</code> instructions during build: then <code>localhost</code> would refer to the host localhost.</p>  <pre><code> docker build --network='host' </code></pre>  <p>This is since <a href='https://docs.docker.com/engine/api/version-history/#v125-api-changes' rel='noreferrer'>API 1.25+ only</a>, in <a href='https://github.com/moby/moby/releases/tag/v1.13.0-rc5' rel='noreferrer'>docker v.1.13.0-rc5</a> (January 2017)</p>  <blockquote>   <p><code>POST /build</code> accepts <code>networkmode</code> parameter to specify network used during build.</p> </blockquote>  <hr>  <p>But if you don't need Git in your actual built image (for its main process to run), it would be easier to</p>  <ul> <li>clone the repo locally (directly on the host) <em>before</em> the docker build.<br> You need to clone it where your Dockerfile is, as it must be relative to the source directory that is being built (the context of the build).</li> <li><p>use a <a href='https://docs.docker.com/engine/reference/builder/#copy' rel='noreferrer'><code>COPY</code> directive</a> in the Dockerfile, to copy the host folder representing the checked out Git repo.<br> Note: add <code>.git/</code>: to your <code>.dockerignore</code> (file in the same folder as your Dockerfile), in order to <em>not</em> copy the <code>repo_dir/.git</code> folder of that cloned repo, if you don't have git in your target image.</p>  <pre><code>COPY repo_dir . </code></pre></li> </ul>  <p>(Here '<code>.</code>' represent the current <code>WORKDIR</code> within the image being built)</p> ",
    "highest_rated_answer": null
  },
  {
    "Id": "47187879",
    "PostTypeId": "1",
    "CreationDate": "2017-11-08T19:16:09.587",
    "Score": "9",
    "ViewCount": "1576",
    "Body": "<p>I have a docker-compose file like:</p>  <pre><code>services:   dns-server:     # ...   other:     image: ubuntu     dns: dns-server </code></pre>  <p>Is there any way to make the <code>other</code> container use <code>dns-server</code> as its DNS server without hardcoding the IP of that container? Using the container name doesn't give me an error but it doesn't seem to work.</p> ",
    "OwnerUserId": "115563",
    "LastActivityDate": "2017-11-08T19:16:09.587",
    "Title": "docker-compose use other container as DNS",
    "Tags": "<docker><docker-compose>",
    "AnswerCount": "0",
    "CommentCount": "1",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": null
  },
  {
    "Id": "47247952",
    "PostTypeId": "1",
    "AcceptedAnswerId": "47485123",
    "CreationDate": "2017-11-12T10:56:19.650",
    "Score": "9",
    "ViewCount": "34980",
    "Body": "<p>I'm on ubuntu 16.04. I have a (testing) docker (docker-compose) container running php 5.6 and apache 2.4.</p>  <p>On the production platform (without docker) the mail is sent with sendmail.</p>  <p>How to send test email on docker container (with sendmail)?</p>  <p>Thanks in advance for responses.</p> ",
    "OwnerUserId": "1768583",
    "LastEditorUserId": "1496974",
    "LastEditDate": "2020-01-06T17:00:46.990",
    "LastActivityDate": "2020-09-20T10:35:43.377",
    "Title": "Send email on testing docker container with php and sendmail",
    "Tags": "<php><docker><docker-compose><sendmail><php-5.6>",
    "AnswerCount": "2",
    "CommentCount": "1",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 4.0",
    "accepted_answer": "<p>It works.</p>  <p>In Dockerfile :</p>  <pre><code># sendmail config ############################################  RUN apt-get install -q -y ssmtp mailutils  # root is the person who gets all mail for userids &lt; 1000 RUN echo 'root=yourAdmin@email.com' &gt;&gt; /etc/ssmtp/ssmtp.conf  # Here is the gmail configuration (or change it to your private smtp server) RUN echo 'mailhub=smtp.gmail.com:587' &gt;&gt; /etc/ssmtp/ssmtp.conf RUN echo 'AuthUser=your@gmail.com' &gt;&gt; /etc/ssmtp/ssmtp.conf RUN echo 'AuthPass=yourGmailPass' &gt;&gt; /etc/ssmtp/ssmtp.conf  RUN echo 'UseTLS=YES' &gt;&gt; /etc/ssmtp/ssmtp.conf RUN echo 'UseSTARTTLS=YES' &gt;&gt; /etc/ssmtp/ssmtp.conf   # Set up php sendmail config RUN echo 'sendmail_path=sendmail -i -t' &gt;&gt; /usr/local/etc/php/conf.d/php-sendmail.ini </code></pre>  <p>For testing inside php sendmail container :</p>  <pre><code>echo 'Un message de test' | mail -s 'sujet de test' mailSendingAdresse@email.com </code></pre>  <p>I succeed with the help of this two documents :</p>  <ul> <li><a href='https://unix.stackexchange.com/questions/36982/can-i-set-up-system-mail-to-use-an-external-smtp-server'>https://unix.stackexchange.com/questions/36982/can-i-set-up-system-mail-to-use-an-external-smtp-server</a></li> <li><a href='https://github.com/cmaessen/docker-php-sendmail/blob/master/Dockerfile' rel='noreferrer'>https://github.com/cmaessen/docker-php-sendmail/blob/master/Dockerfile</a></li> </ul> ",
    "highest_rated_answer": "<p>If it says:</p> <blockquote> <p>&quot;Package 'ssmtp' has no installation candidate&quot;</p> </blockquote> <p>You can use <strong>msmtp</strong> instead.</p> <p>Add the following to your dockerfile</p> <pre><code># sendmail config ################# ARG SMTP_PASSWORD=not_provided # install RUN apt-get install -q -y msmtp mailutils # config COPY msmtprc /etc/msmtprc RUN chmod 600 /etc/msmtprc RUN chown www-data:www-data /etc/msmtprc ARG SMTP_PASSWORD=not_provided RUN sed -i &quot;s|YourAwesomeStr0ngP4zzw0rd|$SMTP_PASSWORD|g&quot; /etc/msmtprc # Set up php sendmail config RUN echo &quot;sendmail_path=/usr/bin/msmtp -t&quot; &gt;&gt; /etc/php/7.3/apache2/conf.d/php-sendmail.ini </code></pre> <p>Add a <code>msmtprc</code> file to your docker build context:</p> <pre><code>account default host mail.yoursmtpserver.com port 587 tls on tls_starttls on tls_trust_file /etc/ssl/certs/ca-certificates.crt tls_certcheck on auth on user my@mail.com password &quot;YourAwesomeStr0ngP4zzw0rd&quot; from &quot;my@mail.com&quot; logfile /var/log/msmtp.log </code></pre> <p>note: Some changes were made in order to make it work with my particular setup (branching <code>FROM eboraas/apache-php</code>). This applies particularily to the lines:</p> <ul> <li>ARG SMTP_PASSWORD=not_provided</li> <li>RUN chown www-data:www-data /etc/msmtprc</li> <li>RUN sed -i &quot;s|YourAwesomeStr0ngP4zzw0rd|$SMTP_PASSWORD |g&quot; /etc/msmtprc</li> <li>RUN echo &quot;sendmail_path=/usr/bin/msmtp -t&quot; &gt;&gt; /etc/php/7.3/apache2/conf.d/php-sendmail.ini</li> </ul> <p>You may need to adapt paths, passwords and so on to fit your needs. Keep in mind to set the <code>SMTP_PASSWORD</code> build argument from environment (e.g. <code>SMTP_PASSWORD=&lt;secret&gt; docker-compose build</code>) if you want to use this solution straight away.</p> <p>Useful resources:</p> <ul> <li><a href='https://wiki.debian.org/msmtp' rel='noreferrer'>https://wiki.debian.org/msmtp</a></li> <li><a href='https://owendavies.net/articles/setting-up-msmtp/' rel='noreferrer'>https://owendavies.net/articles/setting-up-msmtp/</a></li> <li><a href='https://wiki.archlinux.org/index.php/msmtp#Send_mail_with_PHP_using_msmtp' rel='noreferrer'>https://wiki.archlinux.org/index.php/msmtp#Send_mail_with_PHP_using_msmtp</a></li> <li><a href='https://stackoverflow.com/questions/25880689/linux-msmtp-configuration-sends-from-shell-but-fails-from-php-apache'>linux msmtp configuration sends from shell but fails from PHP/apache</a></li> </ul> "
  },
  {
    "Id": "47292669",
    "PostTypeId": "1",
    "CreationDate": "2017-11-14T18:14:37.283",
    "Score": "9",
    "ViewCount": "18224",
    "Body": "<p>I have problem with memcached in docker-compose. This is docker-compose.yml: </p>  <pre><code>nginx:     container_name: nginx     image: nginx:latest     ports:         - 127.0.0.2:8000:80     volumes:         - ./htdocs:/htdocs         - ./nginx.conf:/etc/nginx/conf.d/nginx.conf     links:         - php php:     container_name: php     build: ./php     volumes:         - ./htdocs:/htdocs         - ./php/php.ini:/usr/local/etc/php/php.ini       links:         - mysql         - memcached mysql:     container_name: mysql     image: mysql:latest     ports:         - 127.0.0.2:8001:3306     volumes:         - ./my.cnf:/etc/mysql/my.cnf         - ./db:/var/lib/mysql     environment:         - MYSQL_ROOT_PASSWORD=root         - MYSQL_DATABASE=db         - MYSQL_USER=root         - MYSQL_PASSWORD=root memcached:     container_name: memcached     image: memcached:latest     ports:         - '11211:11211' </code></pre>  <p>and this is my php code: </p>  <pre><code>error_reporting(E_ALL &amp; ~E_NOTICE);  $memcached = new Memcached;   $memcached-&gt;addServer('0.0.0.0', 11211);  echo '&lt;pre&gt;'; print_r($memcached-&gt;getServerList()); echo '&lt;/pre&gt;';  if($memcached-&gt;getStats() === false) {     echo 'returned false'; } else {     echo '&lt;pre&gt;'; print_r($memcached-&gt;getStats()); echo '&lt;/pre&gt;'; } </code></pre>  <p>and result is:</p>  <pre><code>Array (      [0] =&gt; Array         (             [host] =&gt; 0.0.0.0             [port] =&gt; 11211             [type] =&gt; TCP         ) )  returned false </code></pre>  <p>Why memcached cant see server? (getStats returned nothing) Command 'docker ps' return list where it`s running docker memcached:lastest as port '0.0.0.0:11211->11211/tcp'. Sorry  for my english. </p> ",
    "OwnerUserId": "4042608",
    "LastActivityDate": "2019-03-11T12:52:23.433",
    "Title": "memcached not working in docker-compose",
    "Tags": "<php><docker-compose><memcached>",
    "AnswerCount": "1",
    "CommentCount": "3",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p>Just replace host <code>0.0.0.0</code> to <code>memcached</code></p>  <p>Host <code>memcached</code> link to IP of memcached container.</p> "
  },
  {
    "Id": "47454597",
    "PostTypeId": "1",
    "AcceptedAnswerId": "52201432",
    "CreationDate": "2017-11-23T11:36:39.237",
    "Score": "9",
    "ViewCount": "6130",
    "Body": "<p>My postgres <code>yaml</code> part looking like this:</p>  <pre><code>postgres:     container_name: 'postgres'     image: postgres:10.1     environment:       - POSTGRES_USER=postgres       - POSTGRES_PASSWORD=root       - POSTGRES_DB=myids     ports:       - '5432:5432'     networks:       - app-network </code></pre>  <p>Then when I am logging in with that credentials using HeidiSQL I cant see my database: <a href='https://i.stack.imgur.com/q4vlu.png' rel='noreferrer'><img src='https://i.stack.imgur.com/q4vlu.png' alt='enter image description here'></a></p>  <p>Any ideas?</p>  <p>Update thanks to this answer I managed to find my database <a href='https://dba.stackexchange.com/a/1304'>https://dba.stackexchange.com/a/1304</a> using this select:</p>  <pre><code>SELECT datname FROM pg_database WHERE datistemplate = false; </code></pre>  <p>Now the question why HeidiSQL won't show that?</p> ",
    "OwnerUserId": "2926340",
    "LastActivityDate": "2022-11-08T14:46:05.577",
    "Title": "HeidiSQL won't list my database",
    "Tags": "<postgresql><docker><docker-compose><heidisql>",
    "AnswerCount": "2",
    "CommentCount": "2",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>We have to specify the database name in the connection manager, only then will it display the selected database. Please refer to the images below... <a href='https://i.stack.imgur.com/gAEOt.png' rel='noreferrer'><img src='https://i.stack.imgur.com/gAEOt.png' alt='enter image description here' /></a></p> <p><a href='https://i.stack.imgur.com/wSPa7.png' rel='noreferrer'><img src='https://i.stack.imgur.com/wSPa7.png' alt='enter image description here' /></a><a href='https://i.stack.imgur.com/nfwM5.png' rel='noreferrer'><img src='https://i.stack.imgur.com/nfwM5.png' alt='enter image description here' /></a><a href='https://i.stack.imgur.com/zWjlr.png' rel='noreferrer'><img src='https://i.stack.imgur.com/zWjlr.png' alt='enter image description here' /></a></p> <p>There is no provision to view all the databases in PostgreSQL in HeidiSQL, but for MySQL the database name field is optional.</p> ",
    "highest_rated_answer": "<p>Also make sure you don't have the star icon selected. Otherwise it will only show the tables marked as favorites</p> "
  },
  {
    "Id": "47784863",
    "PostTypeId": "1",
    "AcceptedAnswerId": "47795951",
    "CreationDate": "2017-12-13T03:00:47.480",
    "Score": "9",
    "ViewCount": "4395",
    "Body": "<p>I would like to create a named volume for one of my containers.</p>  <p>This container will need a lot more storage than other containers I run, so I would like to store that particular volume on a different disk that has lots of free space.</p>  <p>I still want the other volumes on the default disk, only that one named volume should go on another disk.</p>  <p>I don't want to use a bind mount because it will make backing up and migrating more complicated.</p>  <p>The only option I can think of is to manually move the volume after it is created (while the container is stopped), and create a symlink from its original location in <code>/var/lib/docker/...</code> to the new location on the other hard drive. This is very manual though, which leads me to think there must be a better way.</p>  <p>What is the recommended way of achieving this?</p> ",
    "OwnerUserId": "2774883",
    "LastActivityDate": "2017-12-13T14:52:39.483",
    "Title": "How to store a specific named docker volume at a specific location without changing the default?",
    "Tags": "<docker><docker-compose><docker-volume>",
    "AnswerCount": "1",
    "CommentCount": "1",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>Use the local volume driver:</p>  <pre><code>docker volume create -d local -o type=none -o o=bind -o device=/host/path volname </code></pre>  <p>(Taken from <a href='https://github.com/moby/moby/issues/19990#issuecomment-248955005' rel='noreferrer'>this github comment</a>)</p> ",
    "highest_rated_answer": null
  },
  {
    "Id": "47981240",
    "PostTypeId": "1",
    "AcceptedAnswerId": "47981910",
    "CreationDate": "2017-12-26T16:31:24.263",
    "Score": "9",
    "ViewCount": "11532",
    "Body": "<p>I have a super simple Node app and an Nginx config which acts as a reverse proxy to the Node app. Everything works fine if I run Nginx (via homebrew) and the Node app locally. If I visit the server defined by the Nginx config at port 8080 I get the output from the node app, which is running on port 3000. </p>  <p>I've been trying to convert this simple setup to use Docker and have written the following Docker-compose file: </p>  <pre><code>version: '3.0' services:   web:     build: .     ports:       - 3000:3000   nginx:     build:       context: .       dockerfile: Dockerfile.nginx     ports:       - 8080:8080 </code></pre>  <p>On running <code>docker-compose up</code> the images are built and there are no error messages in the console. On visiting <code>localhost:3000</code> I get the response from the Node app but on visiting <code>localhost:8080</code> I get an an Nginx 502 error page and the following error in the terminal:</p>  <blockquote>   <p>connect() failed (111: Connection refused) while connecting to   upstream, client: 172.18.0.1, server: localhost, request: 'GET /   HTTP/1.1', upstream: '<a href='http://127.0.0.1:3000/' rel='noreferrer'>http://127.0.0.1:3000/</a>', host: 'localhost:8080'</p> </blockquote>  <p>My Dockerfile for the node app looks like so:</p>  <pre><code>FROM node:carbon  WORKDIR /app  ADD . /app  RUN npm install  CMD ['node', '.']  EXPOSE 3000 </code></pre>  <p>and the Dockerfile.nginx looks like so:</p>  <pre><code>FROM nginx COPY nginx.conf /etc/nginx/nginx.conf </code></pre>  <p>and the nginx.conf looks like so:</p>  <pre><code>events {   worker_connections  1024; }  http {    upstream node_app {     server 127.0.0.1:3000;   }    server_tokens off;    # Define the MIME types for files.   include       mime.types;   default_type  application/octet-stream;    # Speed up file transfers by using sendfile()   # TODO: Read up on this   sendfile on;    server {     listen 8080;     server_name localhost;      location / {       proxy_pass http://node_app;       proxy_http_version 1.1;       proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;       proxy_set_header Host $http_host;       proxy_set_header X-Real-IP $remote_addr;     }   } } </code></pre>  <p>On starting Docker up I can see that Nginx is running on port 8080 (because I see the 502 Nginx page) and I can see that the node app is running (because I can visit it at localhost:3000). I can't work out why I get the 502 from nginx.</p>  <p>I've tried using various different things like using <code>links</code> to link the containers and <code>depends_on</code> but nothing seems to make any difference. I'm also using <code>docker-compose up --build</code> to make sure I'm not caching previous builds each time I make a change.</p>  <p>EDIT: Something that seems to make it work is to add a container_name property in to the docker-compose:</p>  <pre><code>  web:     container_name: nodeapp     build:       context: .       dockerfile: Dockerfile.node     ports:       - 3000:3000 </code></pre>  <p>and then using that container name in the upstream node_app config in nginx.conf: </p>  <pre><code>  upstream node_app {     server nodeapp:3000;   } </code></pre>  <p>which makes no sense to me?!</p> ",
    "OwnerUserId": "392572",
    "LastEditorUserId": "392572",
    "LastEditDate": "2017-12-26T16:59:04.680",
    "LastActivityDate": "2017-12-26T17:45:43.160",
    "Title": "Nginx can't find upstream node app when running via Docker-compose",
    "Tags": "<node.js><docker><nginx><docker-compose>",
    "AnswerCount": "1",
    "CommentCount": "2",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>The problem is that in your Nginx configuration you are referencing the IP of the Web service as 127.0.0.1 which is the loopback address of the Host machine running the docker container. This may work depending on your setup (OS, firewall) or may not. </p>  <p>The correct way would be to make the <code>nginx</code> service depends on the <code>web</code> service in your docker-compose.yml file and update the Nginx config to reference the Web service by name (<code>web</code>) instead of by IP address. <a href='https://docs.docker.com/compose/compose-file/#depends_on' rel='noreferrer'>Here</a> you can find more info related to docker compose depends on capability.</p>  <p>The updated docker-compose.yml file would be:</p>  <pre><code>version: '3.0' services:   web:     build: .   nginx:     build:       context: .       dockerfile: Dockerfile.nginx     ports:       - 8080:8080     depends_on:       - web </code></pre>  <p>Notice that I have stop exposing the port of the <code>web</code> service. May be you need to keep it to monitor the Web service but is not required for the <code>nginx</code> service.</p>  <p>With this update to the docker-compose.yml file the Nginx config will be as follows:</p>  <pre><code>events {   worker_connections  1024; }  http {    upstream node_app {     server web:3000;   }    server_tokens off;    # Define the MIME types for files.   include       mime.types;   default_type  application/octet-stream;    # Speed up file transfers by using sendfile()   # TODO: Read up on this   sendfile on;    server {     listen 8080;     server_name localhost;      location / {       proxy_pass http://node_app;       proxy_http_version 1.1;       proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;       proxy_set_header Host $http_host;       proxy_set_header X-Real-IP $remote_addr;     }   } } </code></pre> ",
    "highest_rated_answer": null
  },
  {
    "Id": "47993571",
    "PostTypeId": "1",
    "AcceptedAnswerId": "47993836",
    "CreationDate": "2017-12-27T14:06:34.893",
    "Score": "9",
    "ViewCount": "14633",
    "Body": "<p>I am trying to run a site using multiple container configuration - one for apache, second for mysql and third for myadmin. Everything starts fine, setup runs smooth but when I try to run a PHP application I get <code>mysqli::__construct(): (HY000/2002): Connection refused in system/libraries/drivers/Database/Mysqli.php [54]</code> error.</p>  <p>It seems that there's something wrong with the connection settings but I checked the site through PHP MyAdmin running on separate container and copied the db host IP from there just to be sure. How can I/should I connect from PHP container to MySQL db?</p>  <p>Here's my <code>docker-compose.yml</code> file:</p>  <pre><code>version: '3' services:   web:     build:       context: ./etc/php       args:          - APP_HOST=${APP_HOST}          - MYSQL_USER=${MYSQL_USER}          - MYSQL_PASSWORD=${MYSQL_PASSWORD}          - MYSQL_PORT=${MYSQL_PORT}          - MYSQL_DATABASE=${MYSQL_DATABASE}     ports:       - ${APP_PORT}:80       - ${APP_PORT_SSL}:443     volumes:       - ./var/bin/:/tmp/bin/       - ./app/:/var/www/html/       - ./log/apache2/:/var/log/apache2/       - ./etc/php/conf/:/usr/local/etc/php/conf.d/     environment:       - VIRTUAL_HOST=${VIRTUAL_HOST}   db:     build:       context: ./etc/mysql       args:         - MYSQL_DATABASE=${MYSQL_DATABASE}         - MYSQL_USER=${MYSQL_USER}         - MYSQL_PASSWORD=${MYSQL_PASSWORD}         - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}         - DUMP_FILE=${DUMP_FILE}     volumes:       - ./etc/mysql/conf:/etc/mysql/conf/conf.d       - ./data:/var/lib/mysql       - ./log/:/var/log/     ports:       - '${MYSQL_PORT}:3306'   phpmyadmin:     image: phpmyadmin/phpmyadmin     environment:       MYSQL_ROOT_PASSWORD: root     ports:       - ${MYADMIN_PORT}:80     environment:       - VIRTUAL_HOST=phpmyadmin.localhost </code></pre>  <p>And the <code>.env</code> with variables:</p>  <pre><code>VIRTUAL_HOST=foo.local  APP_PORT=80 APP_PORT_SSL=443 MYADMIN_PORT=8081  APP_HOST=foo.local ADMIN_APP_HOST=admin-foo.local  MYSQL_DATABASE=foo_local MYSQL_USER=root MYSQL_HOST=172.26.0.2 MYSQL_PASSWORD=root MYSQL_ROOT_PASSWORD=root MYSQL_PORT=3306 </code></pre>  <p>Oh, and here's the code I try to run:</p>  <pre><code> $this-&gt;link = $socket ? new mysqli(null, $user, $pass, $database, $port, $socket) : new mysqli($host, $user, $pass, $database, $port); </code></pre>  <p>Output of <code>echo $host.'&lt;br&gt;'.$user.'&lt;br&gt;'.$pass.'&lt;br&gt;'.$database.'&lt;br&gt;'.$port.'&lt;br&gt;'.$socket;</code> is following:</p>  <pre><code>172.26.0.2 root root hq_local 3306 </code></pre> ",
    "OwnerUserId": "401499",
    "LastEditorUserId": "401499",
    "LastEditDate": "2017-12-27T14:12:12.483",
    "LastActivityDate": "2021-11-23T22:37:02.090",
    "Title": "Docker PHP MySQL connection refused",
    "Tags": "<php><mysql><docker><mysqli><docker-compose>",
    "AnswerCount": "5",
    "CommentCount": "1",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>check your container IP with a <code>docker inspect [container name]</code> but write IP in a file is not very good. Replace IP by name of your db container.</p> ",
    "highest_rated_answer": "<p>You can always use <code>host.docker.internal</code> as IP, therefore you can use something like this:</p>  <pre><code>$db = new \\PDO('mysql:host=host.docker.internal;port=3306;dbname=db', 'root', 'pass'); </code></pre> "
  },
  {
    "Id": "48015477",
    "PostTypeId": "1",
    "AcceptedAnswerId": "48042583",
    "CreationDate": "2017-12-28T23:04:50.807",
    "Score": "9",
    "ViewCount": "15747",
    "Body": "<p>I am trying to setup separate docker containers for rabbitmq and the consumer for the container, i.e., the process that would listen on the queue and perform the necessary tasks.  I created the yml file, and the docker file.  </p>  <p>I am able to run the yml file, however when I check the docker-compose logs I see where there are ECONNREFUSED errors.  </p>  <p><strong>NewUserNotification.js:</strong></p>  <pre><code>require('seneca')()     .use('seneca-amqp-transport')     .add('action:new_user_notification\u2019, function(message, done) {         \u2026           return done(null, {         pid: process.pid,         status: `Process ${process.pid} status: OK`     })     .listen({         type: 'amqp',         pin: ['action:new_user_notification\u2019],         name: 'seneca.new_user_notification.queue',         url: process.env.AMQP_RECEIVE_URL,         timeout: 99999     }); </code></pre>  <p><strong>error message in docker-compose log:</strong></p>  <pre><code>    {'notice':'seneca: Action hook:listen,role:transport,type:amqp failed: connect ECONNREFUSED 127.0.0.1:5672.','code':     'act_execute','err':{'cause':{'errno':'ECONNREFUSED','code':'ECONNREFUSED','syscall':'connect','address':'127.0.0.1',     'port':5672},'isOperational':true,'errno':'ECONNREFUSED','code':'act_execute','syscall':'connect','address':'127.0.0.1',     'port':5672,'eraro':true,'orig':{'cause':{'errno':'ECONNREFUSED','code':'ECONNREFUSED','syscall':'connect','address':'127.0.0.1',     'port':5672},'isOperational':true,'errno':'ECONNREFUSED','code':'ECONNREFUSED','syscall':'connect','address':'127.0.0.1','port':5672},     'seneca':true,'package':'seneca','msg':'seneca: Action hook:listen,role:transport,type:amqp failed: connect ECONNREFUSED 127.0.0.1:5672.',     'details':{'message':'connect ECONNREFUSED 127.0.0.1:5672','pattern':'hook:listen,role:transport,type:amqp','instance':'Seneca/\u2026\u2026\u2026\u2026/\u2026\u2026\u2026\u2026/1/3.4.3/-\u201c,     \u201dorig$':{'cause':{'errno':'ECONNREFUSED','code':'ECONNREFUSED','syscall':'connect','address':'127.0.0.1','port':5672},'isOperational':true, 'errno':'ECONNREFUSED','code':'ECONNREFUSED','syscall':'connect','address':'127.0.0.1','port':5672} </code></pre>  <p><strong>sample docker-compose.yml file:</strong></p>  <pre><code>version: '2.1' services:  rabbitmq:     container_name: '4340_rabbitmq'     tty: true     image: rabbitmq:management     ports:       - 15672:15672       - 15671:15671       - 5672:5672     volumes:       - /rabbitmq/lib:/var/lib/rabbitmq       - /rabbitmq/log:/var/log/rabbitmq       - /rabbitmq/conf:/etc/rabbitmq/ account:     container_name: 'account'     build:       context: .       dockerfile: ./Account/Dockerfile     ports:       - 3000:3000     links:       - 'mongo'       - 'rabbitmq'     depends_on:       - 'mongo'       - 'rabbitmq' new_user_notification:     container_name: 'app_new_user_notification'     build:       context: .       dockerfile: ./Account/dev.newusernotification.Dockerfile     links:       - 'mongo'       - 'rabbitmq'     depends_on:       - 'mongo'       - 'rabbitmq'     command: ['./wait-for-it.sh', 'rabbitmq:5672', '-t', '90', '--', 'node', \u201cnewusernotification.js'] </code></pre>  <p><strong>amqp connection string:</strong>  (I tried both ways, with and without a user/pass) amqp://username:password@rabbitmq:5672</p>  <p>I added the link attribute to the docker-compose file and referenced the name in the .env file(rabbitmq).  I tried to run the NewUserNotification.js file from outside the container and it started fine.  What could be causing this problem?  Connection string issue?  Docker-Compose.yml configuration issue?  Other?</p> ",
    "OwnerUserId": "1790300",
    "LastActivityDate": "2018-01-01T14:47:43.430",
    "Title": "Docker and Rabbitmq: ECONNREFUSED between containers",
    "Tags": "<node.js><docker><docker-compose><seneca>",
    "AnswerCount": "2",
    "CommentCount": "6",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>Seems the environment variable <code>AMQP_RECEIVE_URL</code> is  not constructed properly. According to error log the listener is trying to connect to localhost(127.0.0.1) which is not the rabbitmq service container IP. Find the modified configurations for a working sample.</p>  <p>1 docker-compose.yml</p>  <pre><code>version: '2.1' services:  rabbitmq:     container_name: '4340_rabbitmq'     tty: true     image: rabbitmq:management     ports:       - 15672:15672       - 15671:15671       - 5672:5672     volumes:       - ./rabbitmq/lib:/var/lib/rabbitmq  new_user_notification:     container_name: 'app_new_user_notification'     build:       context: .       dockerfile: Dockerfile     env_file:       - ./un.env     links:       - rabbitmq     depends_on:       - rabbitmq     command: ['./wait-for-it.sh', 'rabbitmq:5672', '-t', '120', '--', 'node', 'newusernotification.js'] </code></pre>  <p>2 un.env </p>  <p><code>AMQP_RECEIVE_URL=amqp://guest:guest@rabbitmq:5672</code></p>  <p>Note that I've passed the <code>AMQP_RECEIVE_URL</code> as an environment variable to <code>new_user_notification</code> service using <code>env_file</code> and got rid of the <code>account</code> service</p>  <p>3 Dockerfile</p>  <pre><code>FROM node:7 WORKDIR /app COPY newusernotification.js /app COPY wait-for-it.sh /app RUN npm install --save seneca RUN npm install --save seneca-amqp-transport </code></pre>  <p>4 <code>newusernotification.js</code> use the same file in the question.</p>  <p>5 <a href='https://github.com/vishnubob/wait-for-it/blob/master/wait-for-it.sh' rel='noreferrer'>wait-for-it.sh</a></p> ",
    "highest_rated_answer": "<p>It is possible that your RabbitMQ service is not fully up, at the time the connection is attempted from the consuming service. </p>  <p>If this is the case, in Docker Compose, you can <code>wait</code> for services to come up using a container called <a href='https://github.com/dadarek/docker-wait-for-dependencies' rel='nofollow noreferrer'>dadarek/wait-for-dependencies</a>.</p>  <p><strong>1).</strong> Add a new service <code>waitforrabbit</code> to your docker-compose.yml</p>  <pre><code>waitforrabbit:   image: dadarek/wait-for-dependencies   depends_on:     - rabbitmq    command: rabbitmq:5672 </code></pre>  <p><strong>2).</strong> Include this service in the <code>depends_on</code> section of the service that requires RabbitMQ to be up. </p>  <pre><code>depends_on:    - waitforrabbit </code></pre>  <p><strong>3).</strong> Startup compose</p>  <pre><code>docker-compose run --rm waitforrabbit docker-compose up -d account new_user_notification </code></pre>  <p>Starting compose in this manner will essentially wait for RabbitMQ to be fully up before the connection from the consuming service is made.</p> "
  },
  {
    "Id": "48111049",
    "PostTypeId": "1",
    "AcceptedAnswerId": "48131399",
    "CreationDate": "2018-01-05T09:52:51.203",
    "Score": "9",
    "ViewCount": "15258",
    "Body": "<p>Using <code>docker stack deploy</code>, I can see the following message:</p>  <pre><code>Ignoring unsupported options: restart </code></pre>  <ul> <li>Does it mean that restart policies are not in place? </li> <li>Do they have to be specified outside the compose file?</li> </ul>  <p>You can see this message for example with the  <a href='https://hub.docker.com/_/joomla/' rel='noreferrer'>Joomla compose file available at the bottom of that page</a>. To start the compose file:</p>  <pre><code>sudo docker swarm init sudo docker stack deploy -c stackjoomla.yml joomla </code></pre> ",
    "OwnerUserId": "2641825",
    "LastActivityDate": "2021-03-23T07:58:29.227",
    "Title": "Does the Docker message: 'Ignoring unsupported options: restart' mean the restart policy is ignored?",
    "Tags": "<docker><docker-compose>",
    "AnswerCount": "2",
    "CommentCount": "1",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>A Compose YAML file is used by both <code>docker-compose</code> tool, for local (single-host) dev and test scenarios, and Swarm Stacks, for production multi-host concerns.</p> <p>There are many settings in the Compose file which only work in one tool or the other (<code>docker-compose up</code> vs. <code>docker stack deploy</code>) because some settings are specific to dev and others specific to production clusters. It's OK that they are there, and you'll see warnings in either tool when there are settings included that the specific tool will ignore. This is commonly seen for <code>build:</code> settings (which are docker-compose only) and <code>deploy:</code> settings (which are Swarm Stacks only).</p> <p>The whole goal here is a single file you can use in both tools, and the relevant sections of the compose file are used in that scenario, while the rest are ignored.</p> <p>All of this can be referenced for the individual setting in the <a href='https://docs.docker.com/compose/compose-file/' rel='nofollow noreferrer'>compose file documentation</a>. If you're often working in Compose YAML, I recommend always having a tab open on this page, as I've referenced it almost daily for years, as the spec keeps changing (we're on 3.4+ now).</p> <p>docker-compose does not restart containers by default, but it can if you set the single-setting <code>restart:</code> <a href='https://docs.docker.com/compose/compose-file/compose-file-v3/#restart' rel='nofollow noreferrer'>as documented here</a>. But that setting doesn't work for Swarm Stacks. It will show up as a warning in a <code>docker stack deploy</code> to remind you that the setting will not take effect in a Swarm Stack.</p> <p>Swarm Stacks use the <code>restart_policy:</code> under the <code>deploy:</code> setting, <a href='https://docs.docker.com/compose/compose-file/compose-file-v3/#restart_policy' rel='nofollow noreferrer'>which gives finer control with multiple sub-settings</a>. Like all Stack's, the defaults don't have to be specified in the compose file, and you'll see their default settings documented on that docs page.</p> <p>There is a list on that page of the <a href='https://docs.docker.com/compose/compose-file/compose-file-v3/#not-supported-for-docker-stack-deploy' rel='nofollow noreferrer'>settings that won't work in a Swarm Stack</a>, but it looks incomplete as the <code>restart:</code> setting should be there too. I'll submit a PR to fix that.</p> <p>Also, in the Joomla example you pointed us too, that README seems out of date as well, as it includes <code>links:</code> in the compose example, which are <a href='https://docs.docker.com/compose/compose-file/compose-file-v2/#links' rel='nofollow noreferrer'>depreciated as of Compose version 2</a>, and not needed anymore (because all containers on a custom virtual network can reach each other now).</p> ",
    "highest_rated_answer": "<p>If you <code>docker-compose up</code> your application on a Docker host in standalone mode, all that Compose will do is start containers. It will not monitor the state of these containers once they are created. So it is up to you to ensure that your application will still work if a container dies. You can do this by setting a <em>restart-policy</em>.</p>  <p>If you deploy an application into a Docker swarm with <code>docker stack deploy</code>, things are different. A stack is created that consists of service specifications. Docker swarm then makes sure that for each service in the stack, at all times the specified number of instances is running. If a container fails, swarm will <strong>always</strong> spawn a new instance in order to match the service specification again. In this context, a <em>restart-policy</em> does not make any sense and the corresponding setting in the compose file is ignored.</p>  <p>If you want to stop the containers of your application in swarm mode, you either have to undeploy the whole stack with <code>docker stack rm &lt;stack-name&gt;</code> or scale the service to zero with <code>docker service scale &lt;service-name&gt;=0</code>.</p> "
  },
  {
    "Id": "48277003",
    "PostTypeId": "1",
    "CreationDate": "2018-01-16T08:34:34.977",
    "Score": "9",
    "ViewCount": "2080",
    "Body": "<p>From the count of questions tagged with <code>docker</code> i assume StackOverflow is the right place to ask (instead of e.g. <a href='https://devops.stackexchange.com/questions/tagged/docker'>DevOps</a>), if not, please point me to the right place or move this question accordingly.</p>  <p>My scenario is the following:</p>  <ul> <li>multiple applications consisting of frontend (web GUI) and backend (REST services) are being developed following <a href='https://en.wikipedia.org/wiki/Service-oriented_architecture' rel='noreferrer'>SOA</a>/microservice approaches, each application has its own git repository</li> <li>some applications require a shared additional resource like frontend needs a HTTP server and multiple backend applications need a database server (with persistent storage)</li> <li>focus is primarily on offline mobile development (on the road) so a quick setup of required services/applications should be possible and the amount of resource overhead should be minimal. But of course the whole thing will be deployed/published at some point so i dont want to obstruct that if both can be managed</li> <li>development is done on windows and linux host machines</li> <li>access to all services from host machine is required for development purposes</li> </ul>  <p>What i am trying to achieve is to have a <code>docker-compose.yaml</code> file in the application repositories which i invoke via <code>docker-compose up</code> which would then start all required containers <a href='https://stackoverflow.com/q/44731451'>if not running already</a>, e.g. the database container is started when i invoke <code>docker-compose up</code> in a backend application repository.</p>  <p>My approach was to have a new git repository which defines all shared docker images/containers, with its own <code>docker-compose.yaml</code> where all devs would have to run <code>docker-compose build</code> whenever something changed (might be automated with a git commit hook in the future). The central <code>docker-compose.yaml</code> looks like this</p>  <pre><code>version: '3' services:    postgres:     build: ./images/postgres     image: MY-postgres     container_name: MY-postgres-server     ports:       - '5432:5432'    httpd:     build: ./images/httpd     image: MY-httpd     container_name: MY-httpd-server     ports:       - '80:80' </code></pre>  <p>The <code>Dockerfile</code> describing how each image is built is in its own subfolder and i think not relevant for the question, basically the default images for alpine + apache/postgres.</p>  <p>So the problem: how would a <code>docker-compose.yaml</code> in the application git repository look like that references the services/containers defined by the above central <code>docker-compose.yaml</code>.</p>  <p>Now since this <a href='https://stackoverflow.com/q/45915182/875020'>is</a> <a href='https://stackoverflow.com/q/35597579/875020'>no</a> <a href='https://stackoverflow.com/q/43384538/875020'>new</a> <a href='https://stackoverflow.com/q/43426699/875020'>problem</a> <a href='https://stackoverflow.com/q/35113957/875020'>scenario</a>, i did some research and honestly the variety of approaches and proposed solutions was confusing, for once the <a href='https://stackoverflow.com/a/41013048/875020'>various versions</a> and compatibilities, features that were deprecated, etc.</p>  <ul> <li>We want one single database instance for now for performance reasons and simplicity (<a href='https://www.reddit.com/r/docker/comments/4rkq24/when_you_run_a_database_server_from_docker_do_you/' rel='noreferrer'>reddit</a>) or is this the problem because it is truly <a href='http://microservices.io/patterns/data/shared-database.html' rel='noreferrer'>considered an anti-pattern</a> (via <a href='https://stackoverflow.com/a/41884241/875020'>this answer</a>). Each application would be using its own database within the container, so no sync required on application level.</li> <li>I am reading about <a href='https://stackoverflow.com/a/35598804/875020'>volumes</a> or <a href='https://github.com/docker/compose/issues/942' rel='noreferrer'>data only</a> <a href='https://stackoverflow.com/a/35414699/875020'>containers</a> to solve this problem, yet i cant understand how to implement</li> <li><a href='https://forums.docker.com/t/single-db-instance-with-multiple-apps/40106/9' rel='noreferrer'>Some</a> (Single Host scenario) suggest <a href='https://docs.docker.com/compose/networking/#links' rel='noreferrer'>links</a> (with <a href='https://github.com/docker/compose/issues/942' rel='noreferrer'><code>depends_on</code></a>) while i think this concept has been superseeded by <a href='https://docs.docker.com/compose/compose-file/#networks' rel='noreferrer'>networks</a> but is it still applying? There <a href='https://stackoverflow.com/a/33034919/875020'>seemed to be</a> an <a href='https://stackoverflow.com/a/35414699/875020'><code>extends</code></a> <a href='https://stackoverflow.com/a/33408244/875020'>option</a> as well</li> <li><code>docker-compose</code> has an option <code>--no-deps</code> which is described as <code>Don't start linked services.</code>. If i omit it, i would assume it does what i need, but here i think then problem is the difference in meaning of <a href='https://stackoverflow.com/q/35565770'>image/container/service</a></li> <li>Can a combination of <a href='https://docs.docker.com/compose/extends/#multiple-compose-files' rel='noreferrer'>multiple compose files</a> solve this problem? This would add a hard requirement on <a href='https://github.com/docker/compose/issues/1647' rel='noreferrer'>project paths</a> though</li> <li>If i cant start the containers from my application directory, id like to at least link to them, is <a href='https://github.com/docker/compose/issues/942' rel='noreferrer'><code>external_links</code></a> the <a href='https://blog.virtualzone.de/2016/09/docker-compose-link-containers-outside-compose-file-using-external_links.html' rel='noreferrer'>right</a> <a href='https://github.com/docker/compose/issues/3813#issuecomment-237549950' rel='noreferrer'>approach</a>?</li> <li>There are some feature requests (<a href='https://github.com/docker/compose/issues/318' rel='noreferrer'>feature: including external docker-compose.yml</a>, <a href='https://github.com/docker/compose/issues/2075' rel='noreferrer'>allow sharing containers across services</a>) so maybe its just not possible currently with docker means? Then how to solve it with third-party like <a href='https://github.com/dnephin/compose-addons#dcao-include' rel='noreferrer'>dcao include</a> (which doesnt support <code>version 3</code>)?</li> </ul>  <p>Wow, that escalated quickly. But i wanted to show the research i have done since i just cant believe that its currently not possible. </p> ",
    "OwnerUserId": "875020",
    "LastEditorUserId": "875020",
    "LastEditDate": "2018-01-18T04:46:25.147",
    "LastActivityDate": "2018-01-18T04:46:25.147",
    "Title": "Use shared database docker container in microservice architecture",
    "Tags": "<linux><windows><docker><docker-compose><development-environment>",
    "AnswerCount": "0",
    "CommentCount": "2",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": null
  },
  {
    "Id": "48557607",
    "PostTypeId": "1",
    "AcceptedAnswerId": "48558712",
    "CreationDate": "2018-02-01T07:39:50.803",
    "Score": "9",
    "ViewCount": "8040",
    "Body": "<p>I got the code from two branches with following config:</p>  <p>docker-compose.yml:</p>  <pre><code>version: '3' services:   server:     build: .     restart: always     image: XXXXX     entrypoint: ['./run.sh']     container_name: XXXX     ports:       - 127.0.0.1:8000:8000     volumes:       - .:/app      depends_on:       - redis   redis:     container_name: XXXXX     image: redis:4-alpine </code></pre>  <p>When I docker compose first branch, it works well, but when I compose up the second branch, the original container become the new branch container, which I want the two branches containers exist at the same time. </p>  <p>When I compose up the second branch code, the following message shows:</p>  <pre><code>Recreating XXXXX_branch2 ... done Attaching to XXXXX_branch1 </code></pre> ",
    "OwnerUserId": "8780179",
    "LastEditorUserId": "8780179",
    "LastEditDate": "2018-02-01T07:53:23.273",
    "LastActivityDate": "2018-02-01T09:08:46.667",
    "Title": "Docker compose up keep replace existing container",
    "Tags": "<docker><docker-compose><dockerfile>",
    "AnswerCount": "1",
    "CommentCount": "3",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>Docker compose associates the container with the project name (default directory name) and the service name or container_name if specified. Thus in case both branches have the compose file under the same directory name, and thus the compose files will be interpreted as refering to the same container, which will lead to the container being recreated.</p>  <p>To avoid this situation, you can the <code>--project-name</code> option to override the default one (directory name).</p>  <pre><code>docker-compose --project-name branch1 up -d docker-compose --project-name branch2 up -d </code></pre>  <p>In this case both containers will be created. </p>  <p>But note that if both compose files have the same <code>container_name</code> set, there will be a conflict and the second container creation will fail. To avoid that, either use different container names, or remove the <code>container_name</code> property,  to get the default container name which is <code>&lt;project_name&gt;_&lt;service_name&gt;_1</code></p> ",
    "highest_rated_answer": null
  },
  {
    "Id": "48711455",
    "PostTypeId": "1",
    "AcceptedAnswerId": "48712414",
    "CreationDate": "2018-02-09T17:47:54.860",
    "Score": "9",
    "ViewCount": "11477",
    "Body": "<p>I'm trying to index a containerized Elasticsearch db using the Python client <a href='https://github.com/elastic/elasticsearch-py' rel='nofollow noreferrer'>https://github.com/elastic/elasticsearch-py</a> called from a script (running in a container too).</p> <p>By looking at existing pieces of code, it seems that <code>docker-compose</code> is a useful tool to use for my purpose. My dir structure is</p> <pre><code>docker-compose.yml indexer/ - Dockerfile - indexer.py - requirements.txt elasticsearch/ - Dockerfile </code></pre> <p>My <code>docker-compose.yml</code> reads</p> <pre class='lang-yaml prettyprint-override'><code>version: '3'  services:   elasticsearch:     build: elasticsearch/     ports:        - 9200:9200     networks:       - deploy_network     container_name: elasticsearch    indexer:     build: indexer/     depends_on:       - elasticsearch     networks:       - deploy_network     container_name: indexer    networks:   deploy_network:     driver: bridge </code></pre> <p><code>indexer.py</code> reads</p> <pre class='lang-py prettyprint-override'><code>from elasticsearch import Elasticsearch from elasticsearch.helpers import bulk      es = Elasticsearch(hosts=[{&quot;host&quot;:'elasticsearch'}]) # what should I put here?  actions = [     {     '_index' : 'test',     '_type' : 'content',     '_id' : str(item['id']),     '_source' : item,     } for item in [{'id': 1, 'foo': 'bar'}, {'id': 2, 'foo': 'spam'}] ]      # create index print(&quot;Indexing Elasticsearch db... (please hold on)&quot;) bulk(es, actions) print(&quot;...done indexing :-)&quot;) </code></pre> <p>The Dockerfile for the elasticsearch service is</p> <pre><code>FROM docker.elastic.co/elasticsearch/elasticsearch-oss:6.1.3 EXPOSE 9200 EXPOSE 9300 </code></pre> <p>and that for the indexer is</p> <pre><code>FROM python:3.6-slim WORKDIR /app ADD . /app RUN pip install -r requirements.txt ENTRYPOINT [ &quot;python&quot; ] CMD [ &quot;indexer.py&quot; ] </code></pre> <p>with <code>requirements.txt</code> containing only <code>elasticsearch</code> to be downloaded with pip.</p> <p>Running with <code>docker-compose run indexer</code> gives me the error message at <a href='https://pastebin.com/6U8maxGX' rel='nofollow noreferrer'>https://pastebin.com/6U8maxGX</a> (<code>ConnectionRefusedError: [Errno 111] Connection refused</code>). elasticsearch is up as far as I can see with <code>curl -XGET 'http://localhost:9200/' </code>or by running <code>docker ps -a</code>.</p> <p>How can I modify my <code>docker-compose.yml</code> or <code>indexer.py</code> to solve the problem?</p> <p>P.S. A (working) version (informed by the answers below) of the code can be found here, for completeness' sake: <a href='https://github.com/davidefiocco/dockerized-elasticsearch-indexer' rel='nofollow noreferrer'>https://github.com/davidefiocco/dockerized-elasticsearch-indexer</a>.</p> ",
    "OwnerUserId": "4240413",
    "LastEditorUserId": "4240413",
    "LastEditDate": "2021-10-07T07:29:15.167",
    "LastActivityDate": "2021-10-07T07:29:15.167",
    "Title": "How do I create a (dockerized) Elasticsearch index using a python script running in a docker container?",
    "Tags": "<python><docker><elasticsearch><docker-compose><elasticsearch-py>",
    "AnswerCount": "3",
    "CommentCount": "0",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 4.0",
    "accepted_answer": "<p>The issue is a synchronisation bug: <code>elasticsearch</code> hasn't fully started when <code>indexer</code> tries to connect to it. You'll have to add some retry logic which makes sure that <code>elasticsearch</code> is up and running before you try to run queries against it. Something like running <code>es.ping()</code> in a loop until it succeeds with an exponential backoff should do the trick.</p>  <p>UPDATE: The Docker <a href='https://docs.docker.com/engine/reference/builder/#healthcheck' rel='noreferrer'><code>HEALTHCHECK</code></a> instruction can be used to achieve a similar result (i.e. make sure that <code>elasticsearch</code> is up and running before trying to run queries against it).</p> ",
    "highest_rated_answer": "<p>Making more explicit @Mihai_Todor update, we could use <code>HEALTHCHECK</code> (docker 1.12+), for instance with a command like:</p>  <pre><code>curl -fsSL 'http://$(hostname --ip-address):9200/_cat/health?h=status' | grep -E '^green' </code></pre>  <p>To answer this question using using <code>HEALTHCHECK</code>:</p>  <pre><code>FROM python:3.6-slim  WORKDIR /app ADD . /app RUN pip install -r requirements.txt  HEALTHCHECK CMD curl -fsSL 'http://$(hostname --ip-address):9200/_cat/health?h=status' | grep -E '^green'  ENTRYPOINT [ 'python' ] CMD [ 'indexer.py' ] </code></pre> "
  },
  {
    "Id": "48777109",
    "PostTypeId": "1",
    "AcceptedAnswerId": "48778576",
    "CreationDate": "2018-02-13T23:02:56.107",
    "Score": "9",
    "ViewCount": "2808",
    "Body": "<p>I have two projects and I need two differents docker environnement (containers). I have two <code>docker-compose.yml</code> files in two different projects. <code>foo</code> project and <code>bar</code> project.</p>  <p><code>foo/src/website/docker-compose.yml</code> #1 (<code>foo</code>)</p>  <pre class='lang-yaml prettyprint-override'><code>version: '3' services:   db:     env_file: .env     image: mariadb:10.0.23     container_name: foo-db     ports:       - '42333:3306'     restart: always   web:     image: project/foo     container_name: foo-web     env_file: .env     build: .     restart: always     command: bash -c 'rm -f tmp/pids/server.pid &amp;&amp; bundle exec rails server -p 3000 -b '0.0.0.0''     volumes:       - .:/webapps/foo     ports:       - '3000:3000'     depends_on:       - db </code></pre>  <p><code>bar/src/website/docker-compose.yml</code> #2 (<code>bar</code>)</p>  <pre class='lang-yaml prettyprint-override'><code>version: '3' services:   db:     image: mysql:5.5.50     container_name: bar-db     ports:       - '42333:3306'     env_file: .env     restart: always   web:     image: project/bar     container_name: bar-web     env_file: .env     build: .     restart: always     command: bash -c 'rm -f tmp/pids/server.pid &amp;&amp; bundle exec rails server -p 3000 -b '0.0.0.0''     volumes:       - .:/webapps/bar     ports:       - '3000:3000'     depends_on:       - db </code></pre>  <p>I do this command for my <code>foo</code> project <code>docker-compose build</code> and <code>docker-compose up</code>, everything works. In Kitematic I see my two containers with the good names (<code>foo-web</code>). </p>  <ol> <li>I do this command to stop my image <code>docker-compose stop</code>. </li> <li>I go to my second project (<code>bar</code>) and run <code>docker-compose build</code> and <code>docker-compose up</code>. everything works, but my container name in now replaced by <code>bar-web</code>. </li> <li>I stop my second image with <code>docker-compose stop</code> and I try to perform <code>docker-compose up</code> in my <code>foo</code> project folder again but it fails.</li> </ol>  <p><strong>How can I keep two different containers and easily switch from one to the other with <code>docker-compose stop</code> and <code>docker-compose up</code>?</strong></p>  <h2>Edit 1</h2>  <p>I found the issue, the main folder where my <code>docker-compose.yml</code> are located for my two projects have the same folder name. Can I fix this or I need to rename my folders?</p> ",
    "OwnerUserId": "979201",
    "LastEditorUserId": "1092815",
    "LastEditDate": "2018-02-22T20:24:53.017",
    "LastActivityDate": "2018-02-22T20:24:53.017",
    "Title": "docker-compose containers uses wrong container with multiple projects",
    "Tags": "<docker><docker-compose>",
    "AnswerCount": "3",
    "CommentCount": "0",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>Yes, the directory name is the default project name for <code>docker-compose</code>:</p>  <pre><code>$ docker-compose --help   ...   -p, --project-name NAME    Specify an alternate project name (default: directory name) </code></pre>  <p>Use the <code>-p</code> argument to specify a particular non-default project name.</p>  <p>Alternatively, you can also set the <a href='https://docs.docker.com/compose/reference/envvars/#compose_project_name' rel='noreferrer'><code>COMPOSE_PROJECT_NAME</code></a> environment variable (which defaults to the <code>basename</code> of the project directory).</p>  <p>If you are sharing compose configurations between files and projects with multiple compose files, refer to <a href='https://docs.docker.com/compose/extends/' rel='noreferrer'>this</a> link for more info.</p> ",
    "highest_rated_answer": "<p>I found the solution </p>  <pre><code>docker-compose -p projectname build  docker-compose -p projectname up </code></pre> "
  },
  {
    "Id": "48873652",
    "PostTypeId": "1",
    "AcceptedAnswerId": "48873922",
    "CreationDate": "2018-02-19T20:25:13.440",
    "Score": "9",
    "ViewCount": "7178",
    "Body": "<p>In the Docker Compose documentation, <a href='https://docs.docker.com/compose/compose-file/#long-syntax-3' rel='noreferrer'>here</a>, you have the following example related to the <code>volumes</code> section of <code>docker-compose.yml</code> files:</p>  <pre><code>volumes:   # (1) Just specify a path and let the Engine create a volume   - /var/lib/mysql    # (2) Specify an absolute path mapping   - /opt/data:/var/lib/mysql    # (3) Path on the host, relative to the Compose file   - ./cache:/tmp/cache    # (4) User-relative path   - ~/configs:/etc/configs/:ro    # (5) Named volume   - datavolume:/var/lib/mysql </code></pre>  <p>Which syntaxes produce a <em>bind mount</em> and which produce a <em>docker volume</em>? At some place of the documentation, the two concepts are strictly differentiated but at this place they are mixed together... so it is not clear to me. </p> ",
    "OwnerUserId": "2095024",
    "LastActivityDate": "2018-11-21T14:13:09.890",
    "Title": "Docker Compose: Which syntax produces a bind mount, which produces a volume",
    "Tags": "<docker><syntax><docker-compose><volumes>",
    "AnswerCount": "2",
    "CommentCount": "1",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>Whenever you see &quot;volume&quot; in the comment, that will <a href='https://docs.docker.com/storage/volumes/' rel='nofollow noreferrer'>create a <strong>volume</strong></a>: so (1) and (5).</p> <p>If there is not a volume in the comment, this is about <a href='https://docs.docker.com/storage/bind-mounts/' rel='nofollow noreferrer'>a bind mount</a>.</p> <p><a href='https://i.stack.imgur.com/exSEa.png' rel='nofollow noreferrer'><img src='https://i.stack.imgur.com/exSEa.png' alt='https://docs.docker.com/storage/images/types-of-mounts-bind.png' /></a></p> <p>The <a href='https://docs.docker.com/compose/compose-file/#volumes' rel='nofollow noreferrer'>documentation regarding volumes in docker-compose is here</a>:</p> <blockquote> <p>Mount host paths or named volumes, specified as sub-options to a service.</p> <p>You can mount a host path as part of a definition for a single service, and there is no need to define it in the top level volumes key.</p> <p>But, if you want to reuse a volume across multiple services, then define a named volume in the <a href='https://docs.docker.com/compose/compose-file/#volume-configuration-reference' rel='nofollow noreferrer'>top-level volumes key</a>.</p> <p>The top-level <a href='https://docs.docker.com/compose/compose-file/#volume-configuration-reference' rel='nofollow noreferrer'>volumes</a> key defines a named volume and references it from each service\u2019s volumes list. This replaces volumes_from in earlier versions of the Compose file format. See <a href='https://docs.docker.com/engine/admin/volumes/volumes/' rel='nofollow noreferrer'>Use volumes</a> and <a href='https://docs.docker.com/engine/extend/plugins_volume/' rel='nofollow noreferrer'>Volume Plugins</a> for general information on volumes.</p> </blockquote> ",
    "highest_rated_answer": "<p>Those are two completely different concepts. A volume means that given directory will be persisted between container runs. Imagine MySQL database. You don\u2019t want to lose your data. On the other hand there\u2019s a bind mount where you attach your local directory to the directory in the container. If the container writes something there it will appear in your file system and vice versa (synchronization).</p>  <p>As a side note a volume is nothing more than a symlink to the directory on your machine :) (to a <code>/var/lib/docker/volumes/...</code> directory by default)</p> "
  },
  {
    "Id": "48945972",
    "PostTypeId": "1",
    "CreationDate": "2018-02-23T10:44:20.780",
    "Score": "9",
    "ViewCount": "4025",
    "Body": "<p>I have gitlab-runner installed locally. </p>  <pre><code>km@Karls-MBP ~ $ gitlab-runner --version Version:      10.4.0 Git revision: 857480b6 Git branch:   10-4-stable GO version:   go1.8.5 Built:        Mon, 22 Jan 2018 09:47:12 +0000 OS/Arch:      darwin/amd64 </code></pre>  <p>Docker:</p>  <pre><code>km@Karls-MBP ~ $ docker --version Docker version 17.12.0-ce, build c97c6d6 </code></pre>  <p>.gitlab-ci.yml:</p>  <pre><code>image: docker/compose:1.19.0  before_script:   - echo wtf  test:   script:     - echo test </code></pre>  <p>Results:</p>  <pre><code>km@Karls-MBP ~ $ sudo gitlab-runner exec docker --docker-privileged test WARNING: Since GitLab Runner 10.0 this command is marked as DEPRECATED and will be removed in one of upcoming releases  WARNING: You most probably have uncommitted changes.  WARNING: These changes will not be tested.          Running with gitlab-runner 10.4.0 (857480b6)   on  () Using Docker executor with image docker/compose:1.19.0 ... Using docker image sha256:be4b46f2adbc8534c7f6738279ebedd6106969695f5e596079e89e815d375d9c for predefined container... Pulling docker image docker/compose:1.19.0 ... Using docker image docker/compose:1.19.0 ID=sha256:e06b58ce9de2ea3f11634e022ec814984601ea3a5180440c2c28d9217b713b30 for build container... Running on runner--project-0-concurrent-0 via x.x.x... Cloning repository... Cloning into '/builds/project-0'... done. Checking out b5a262c9 as km/ref... Skipping Git submodules setup No such command: sh  Commands:   build              Build or rebuild services   bundle             Generate a Docker bundle from the Compose file   config             Validate and view the Compose file   create             Create services   down               Stop and remove containers, networks, images, and volumes   events             Receive real time events from containers   exec               Execute a command in a running container   help               Get help on a command   images             List images   kill               Kill containers   logs               View output from containers   pause              Pause services   port               Print the public port for a port binding   ps                 List containers   pull               Pull service images   push               Push service images   restart            Restart services   rm                 Remove stopped containers   run                Run a one-off command   scale              Set number of containers for a service   start              Start services   stop               Stop services   top                Display the running processes   unpause            Unpause services   up                 Create and start containers   version            Show the Docker-Compose version information </code></pre>  <p>Don't really know what the issue is.</p> ",
    "OwnerUserId": "1137669",
    "LastActivityDate": "2022-01-19T09:24:10.150",
    "Title": "gitlab-runner locally - No such command sh",
    "Tags": "<docker><docker-compose><gitlab>",
    "AnswerCount": "2",
    "CommentCount": "2",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p>It seems that the <code>docker/compose</code> image is configured with <code>docker-compose</code> as an entrypoint.</p>  <p>You can <a href='https://docs.gitlab.com/ee/ci/docker/using_docker_images.html#overriding-the-entrypoint-of-an-image' rel='noreferrer'>override the default entrypoint</a> of the docker/compose image in your <strong>.gitlab-ci.yml</strong> file :</p>  <pre><code>image:    name: docker/compose:1.19.0   entrypoint: ['']  before_script:   - echo wtf  test:   script:     - echo test </code></pre> "
  },
  {
    "Id": "48954683",
    "PostTypeId": "1",
    "AcceptedAnswerId": "48957587",
    "CreationDate": "2018-02-23T19:10:02.103",
    "Score": "9",
    "ViewCount": "9336",
    "Body": "<p>For some reason, I can't detach from Docker containers after running <code>docker attach &lt;container&gt;</code>. The documentation says to use <code>Ctrl-p, Ctrl-q</code>, but that doesn't seem to work. I've also tried <code>ctrl-q + ctrl-p</code> (combination, as opposed to in-sequence) and <code>ctrl-shift-q, ctrl-shift-p</code> and <code>ctrl-shift-q + ctrl-shift-p</code>. Even setting a detach key, for example <code>--detach-keys='p'</code> won't detach from the container. </p>  <p>Furthermore, other things don't seem to be working. For example, according to documentation, <code>ctrl-c</code> should cause the attached container to stop and detach. However, I haven't been able to get <code>ctrl-c</code> working on any of my containers. Unlike <code>ctrl-q</code>, however, <code>ctrl-c</code> renders feedback as expected, showing a <code>^C</code> in the terminal.</p>  <p>I've noticed that for some reason, when I press <code>ctrl-p</code>, I get a <code>^P</code> in the terminal, but pressing <code>ctrl-q</code> or <code>ctrl-shift-q</code> renders no terminal feedback.</p>  <p>Can anyone venture a guess as to why this might be happening?</p>  <p>I'm using iTerm2 on MacOS if it matters. Also, the containers in question were launched with <code>docker-compose</code>.</p>  <p>Edit: For clarity, I launched my container with <code>docker-compose up</code> on the following <code>compose</code> file:</p>  <pre><code>version: '3'  services:   test:     build:       context: .       dockerfile: Dockerfile     container_name: container-test     ports:       - '5050:5050' </code></pre>  <p>Then I attach using:</p>  <pre><code>$ docker attach container-test </code></pre>  <p>Edit 2: After some testing, I can confirm this issue still exists in MacOS Terminal. </p> ",
    "OwnerUserId": "1975301",
    "LastEditorUserId": "1975301",
    "LastEditDate": "2018-02-24T03:04:42.530",
    "LastActivityDate": "2018-02-24T03:22:28.637",
    "Title": "docker attach: Why can't I detach from my Docker container?",
    "Tags": "<macos><docker><docker-compose><docker-for-mac>",
    "AnswerCount": "2",
    "CommentCount": "3",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>You can use <code>--sig-proxy=false</code> to prevent signals being passed to container and detach using <code>Ctrl+C</code>:</p>  <pre><code>docker attach --sig-proxy=false container-test </code></pre>  <hr>  <p>This seems to be a known issue: <a href='https://github.com/docker/for-mac/issues/1598' rel='noreferrer'>https://github.com/docker/for-mac/issues/1598</a></p> ",
    "highest_rated_answer": "<p>I found that by adding the following lines to my <code>docker-compose</code>, I could get it to exit and respond to inputs. </p>  <pre><code>services:   test:     // etc...     stdin_open: true     tty: true     // etc... </code></pre> "
  },
  {
    "Id": "49062367",
    "PostTypeId": "1",
    "AcceptedAnswerId": "49083190",
    "CreationDate": "2018-03-02T03:24:11.887",
    "Score": "9",
    "ViewCount": "10224",
    "Body": "<p>I am new to Docker and find that there are numerous images that are getting created (as seen in <code>sudo docker images</code>) and found somewhere in stackoverflow to periodically run <code>sudo docker rmi $(sudo docker images -q)</code> to remove all images. Why so many images get created? is there something wrong in my configuration?</p>  <p>docker-compose.yml</p>  <pre><code>nginx:   build: ./nginx   restart: always   ports:     - '80:80'     - '443:443'   volumes:     - /etc/letsencrypt/:/etc/letsencrypt/   links:     - node:node  node:   build: ./node   restart: always   ports:    - '8080:8080'   volumes:     - ./node:/usr/src/app     - /usr/src/app/node_modules </code></pre>  <p>The nginx dockerfile is</p>  <pre><code>FROM nginx:alpine  COPY nginx.conf /etc/nginx/conf.d/default.conf </code></pre>  <p>The nodejs dockerfile is</p>  <pre><code>FROM node:9.3.0-alpine  WORKDIR /usr/src/app  COPY package*.json /usr/src/app/  RUN npm install --only=production  COPY . /usr/src/app  EXPOSE 8080 CMD [ 'npm', 'start' ] </code></pre>  <p>The website/app works fine. Except that periodically, I am removing all containers, images and then run: <code>sudo docker-compose up --build -d</code>. </p> ",
    "OwnerUserId": "4064958",
    "LastEditorUserId": "4420967",
    "LastEditDate": "2018-03-03T07:00:49.327",
    "LastActivityDate": "2020-05-26T06:46:15.077",
    "Title": "Docker creating multiple images",
    "Tags": "<docker><docker-compose><dockerfile>",
    "AnswerCount": "3",
    "CommentCount": "0",
    "FavoriteCount": "0",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": "<p>Images are immutable, so any change you make results in a new image being created. Since your compose file specifies the build command, it will rerun the build command when you start the containers. If any files you are including with a <code>COPY</code> or <code>ADD</code> change, then the existing image cache is no longer used and it will build a new image without deleting the old image.</p>  <p>Note, I'd recommend naming your image in the compose file so it's clear which image is being rebuilt. And you can watch the compose build output to the the first step that doesn't report using the cache to see what is changing. If I was to guess, the line that breaks your cache and causes a new image is this one in nodejs:</p>  <pre><code>COPY . /usr/src/app </code></pre>  <p>If the files being changed and causing the rebuild are not needed in your container, then use a <code>.dockerignore</code> file to exclude the unnecessary files.</p> ",
    "highest_rated_answer": "<p>I had the same problem when I was building my Dockerfile. </p>  <p>I found the solution, use this command to build your file : </p>  <pre><code>`docker build --rm -t &lt;tag&gt; .` </code></pre>  <p>The option <code>--rm</code> removes intermediate containers after a successful build.</p> "
  },
  {
    "Id": "49478428",
    "PostTypeId": "1",
    "CreationDate": "2018-03-25T17:00:15.657",
    "Score": "9",
    "ViewCount": "10155",
    "Body": "<p>I'm trying to build a CI Pipeline with Gitlab CI/CD. My project is a really simple API based on Symfony. To create a consistent environment i'm using docker-compose with four very simple containers (nginx,PHP,MySQL &amp; composer). My .gitlab-ci.yaml looks like this:</p>  <pre><code>stages:     - setup  setup:   stage: setup    before_script:     - docker-compose up -d    script:     - sleep 15     - docker-compose exec -T php php bin/console doctrine:schema:create   after_script:     - [...]      - docker-compose down </code></pre>  <p>The problem I'm encountering is that the ci script does not wait till the <code>docker-compose up -d</code> is finished. To bypass this I've added this stupid sleep. </p>  <p>Is there a better way to do this? </p> ",
    "OwnerUserId": "7376954",
    "LastActivityDate": "2022-04-07T21:30:12.683",
    "Title": "GitLab CI & Docker: wait till docker-compose up is finished",
    "Tags": "<docker><docker-compose><gitlab><gitlab-ci><gitlab-ci-runner>",
    "AnswerCount": "1",
    "CommentCount": "5",
    "ContentLicense": "CC BY-SA 3.0",
    "accepted_answer": null,
    "highest_rated_answer": "<p>To save some time for the people searching it, I implemented the solution commented by @gbrener.</p> <p><strong>The idea:</strong> Wait until getting the log that shows that the container is up, then continue the pipeline.</p> <p>1 - Get the log to be the checkpoint. I used the last log of my container. Ex: Generated backend app.</p> <p>2 - Get the container name. Ex: ai-server-dev.</p> <p>3 - Create a sh script like the below and name it something.sh. Ex:</p> <pre><code>#!/bin/bash while ! docker logs ai-server-dev --tail=1 | grep -q &quot;Generated backend app&quot;; do     sleep 10     echo &quot;Waiting for backend to load ...&quot; done </code></pre> <p>4 - Replace the 'sleep 15' with 'sh wait_server.sh' as in the question to run the script in your pipeline.</p> "
  }
]