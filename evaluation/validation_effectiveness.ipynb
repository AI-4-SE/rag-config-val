{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def measure_scores(file_path: str, model_name: str, index_name: str):\n",
    "\n",
    "    true_positives = []\n",
    "    true_negatives = []\n",
    "    false_positives = []\n",
    "    false_negatives = []\n",
    "    accuracy = []\n",
    "\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    file_name = file_path.split(\"/\")[-1]\n",
    "    \n",
    "\n",
    "    for x in df.to_dict(\"records\"):\n",
    "\n",
    "        #print(x[\"final_rating\"], x[f\"{model_name}_{index_name}\"])\n",
    "        #print(type(bool(x[\"final_rating\"])), type(x[f\"{model_name}_{index_name}\"]))\n",
    "\n",
    "        if x[\"final_rating\"] == \"Borderline\":\n",
    "            if x[f\"{model_name}_{index_name}\"]:\n",
    "                accuracy.append(1)\n",
    "                true_positives.append(1)\n",
    "            if not x[f\"{model_name}_{index_name}\"]:\n",
    "                accuracy.append(1)\n",
    "                true_negatives.append(1)\n",
    "\n",
    "\n",
    "        # TP: The LLM validates a dependency as correct and the dependency is correct\n",
    "        if x[f\"{model_name}_{index_name}\"] and str(x[\"final_rating\"]).lower() == \"true\":\n",
    "            accuracy.append(1)\n",
    "            true_positives.append(1)\n",
    "        \n",
    "        # FP: The LLM validates a dependency as correct, but the dependency is actually incorrect\n",
    "        if x[f\"{model_name}_{index_name}\"] and str(x[\"final_rating\"]).lower() == \"false\":\n",
    "            accuracy.append(0)\n",
    "            false_positives.append(1)\n",
    "\n",
    "        # TN: The LLM validates a dependency as incorrect and the dependency is incorrect\n",
    "        if not x[f\"{model_name}_{index_name}\"] and str(x[\"final_rating\"]).lower() == \"false\":\n",
    "            accuracy.append(1)\n",
    "            true_negatives.append(1)\n",
    "\n",
    "        # FN: The LLM validates a dependency as incorrect, but the dependency is actually correct\n",
    "        if not x[f\"{model_name}_{index_name}\"] and  str(x[\"final_rating\"]).lower() == \"true\":\n",
    "            accuracy.append(0)\n",
    "            false_negatives.append(1)\n",
    "\n",
    "\n",
    "    print(\"File: \", file_name)\n",
    "    print(\"TP\", sum(true_positives))\n",
    "    print(\"FP\", sum(false_positives))\n",
    "    print(\"TN\", sum(true_negatives))\n",
    "    print(\"FN\", sum(false_negatives))\n",
    "\n",
    "    precision = sum(true_positives)/(sum(true_positives)+sum(false_positives))\n",
    "    recall = sum(true_positives)/(sum(true_positives)+sum(false_negatives))\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    print(\"Accuracy\", sum(accuracy)/len(accuracy))\n",
    "    print(\"Precision\", precision)\n",
    "    print(\"Recall\", recall)\n",
    "    print(\"F1 Score: \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File:  all_dependencies.csv\n",
      "TP 162\n",
      "FP 143\n",
      "TN 159\n",
      "FN 36\n",
      "Accuracy 0.642\n",
      "Precision 0.5311475409836065\n",
      "Recall 0.8181818181818182\n",
      "F1 Score:  0.6441351888667992\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import traceback\n",
    "\n",
    "model_names = [\"gpt-4o\", \"gpt-3.5-turbo\", \"llama3:8b\", \"llama3:70b\"]\n",
    "\n",
    "FILE_PATH = \"../data/evaluation/all_dependencies.csv\"\n",
    "MODEL_NAME = \"gpt-3.5-turbo\"\n",
    "\n",
    "measure_scores(file_path=FILE_PATH, model_name=MODEL_NAME, index_name=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
