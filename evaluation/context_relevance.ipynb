{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "env_file = \"./.env\"\n",
    "\n",
    "load_dotenv(dotenv_path=env_file)\n",
    "\n",
    "model = OpenAI(api_key=os.getenv(\"OPENAI_KEY\"))\n",
    "\n",
    "# define the custom evaluation model class\n",
    "class CustomEvalModel(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model\n",
    "    ):\n",
    "        self.model = model\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        res = chat_model.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            model=\"gpt-4o-mini-2024-07-18\",\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "\n",
    "        return res.choices[0].message.content\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Custom Azure OpenAI Model\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ContextualRelevancyVerdict\nverdict\n  Field required [type=missing, input_value={'plan': 'Compare the mod...0, 'isDependency': True}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.7/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 87\u001b[0m\n\u001b[1;32m     80\u001b[0m config_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m#for model_name in model_names:\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m#file_name = f\"../data/results/{config_str}/all_dependencies_all_{model_name}.json\"\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m#compute_context_relevance(file_name=file_name, eval_model=custom_eval_model)\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[43mcompute_context_relevance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/results/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mconfig_str\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/all_dependencies_all_gpt-3.5-turbo-0125.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 52\u001b[0m, in \u001b[0;36mcompute_context_relevance\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m     49\u001b[0m context_str \u001b[38;5;241m=\u001b[39m entry[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext_str\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     50\u001b[0m response_str \u001b[38;5;241m=\u001b[39m entry[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 52\u001b[0m context_relevance_score \u001b[38;5;241m=\u001b[39m \u001b[43mget_context_relevance_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_str\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m entry[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext_relevance_score\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m context_relevance_score\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m context \u001b[38;5;129;01min\u001b[39;00m entry[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/GitHub/cval/env/lib/python3.9/site-packages/backoff/_sync.py:105\u001b[0m, in \u001b[0;36mretry_exception.<locals>.retry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m details \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: target,\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melapsed\u001b[39m\u001b[38;5;124m\"\u001b[39m: elapsed,\n\u001b[1;32m    102\u001b[0m }\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    107\u001b[0m     max_tries_exceeded \u001b[38;5;241m=\u001b[39m (tries \u001b[38;5;241m==\u001b[39m max_tries_value)\n",
      "Cell \u001b[0;32mIn[22], line 33\u001b[0m, in \u001b[0;36mget_context_relevance_score\u001b[0;34m(input_str, response_str, context_str)\u001b[0m\n\u001b[1;32m     21\u001b[0m context_relevancy_metric \u001b[38;5;241m=\u001b[39m ContextualRelevancyMetric(\n\u001b[1;32m     22\u001b[0m     threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\n\u001b[1;32m     23\u001b[0m     model\u001b[38;5;241m=\u001b[39mcustom_eval_model,\n\u001b[1;32m     24\u001b[0m     include_reason\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m test_case \u001b[38;5;241m=\u001b[39m LLMTestCase(\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39minput_str,\n\u001b[1;32m     29\u001b[0m     actual_output\u001b[38;5;241m=\u001b[39mresponse_str,\n\u001b[1;32m     30\u001b[0m     retrieval_context\u001b[38;5;241m=\u001b[39m[context_str]\n\u001b[1;32m     31\u001b[0m )\n\u001b[0;32m---> 33\u001b[0m \u001b[43mcontext_relevancy_metric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeasure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_case\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelevance Score: \u001b[39m\u001b[38;5;124m\"\u001b[39m, context_relevancy_metric\u001b[38;5;241m.\u001b[39mscore)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReason: \u001b[39m\u001b[38;5;124m\"\u001b[39m, context_relevancy_metric\u001b[38;5;241m.\u001b[39mreason)\n",
      "File \u001b[0;32m~/GitHub/cval/env/lib/python3.9/site-packages/deepeval/metrics/contextual_relevancy/contextual_relevancy.py:63\u001b[0m, in \u001b[0;36mContextualRelevancyMetric.measure\u001b[0;34m(self, test_case)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_mode:\n\u001b[1;32m     62\u001b[0m     loop \u001b[38;5;241m=\u001b[39m get_or_create_event_loop()\n\u001b[0;32m---> 63\u001b[0m     \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ma_measure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_case\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_show_indicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverdicts: List[ContextualRelevancyVerdict] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_verdicts(\n\u001b[1;32m     69\u001b[0m             test_case\u001b[38;5;241m.\u001b[39minput, test_case\u001b[38;5;241m.\u001b[39mretrieval_context\n\u001b[1;32m     70\u001b[0m         )\n\u001b[1;32m     71\u001b[0m     )\n",
      "File \u001b[0;32m~/GitHub/cval/env/lib/python3.9/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.9/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/usr/lib/python3.9/asyncio/tasks.py:258\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    256\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 258\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_must_cancel:\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "File \u001b[0;32m~/GitHub/cval/env/lib/python3.9/site-packages/deepeval/metrics/contextual_relevancy/contextual_relevancy.py:101\u001b[0m, in \u001b[0;36mContextualRelevancyMetric.a_measure\u001b[0;34m(self, test_case, _show_indicator)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musing_native_model \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m metric_progress_indicator(\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     97\u001b[0m     async_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     98\u001b[0m     _show_indicator\u001b[38;5;241m=\u001b[39m_show_indicator,\n\u001b[1;32m     99\u001b[0m ):\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverdicts: List[ContextualRelevancyVerdict] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 101\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_a_generate_verdicts(\n\u001b[1;32m    102\u001b[0m             test_case\u001b[38;5;241m.\u001b[39minput, test_case\u001b[38;5;241m.\u001b[39mretrieval_context\n\u001b[1;32m    103\u001b[0m         )\n\u001b[1;32m    104\u001b[0m     )\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_score()\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_a_generate_reason(test_case\u001b[38;5;241m.\u001b[39minput)\n",
      "File \u001b[0;32m~/GitHub/cval/env/lib/python3.9/site-packages/deepeval/metrics/contextual_relevancy/contextual_relevancy.py:216\u001b[0m, in \u001b[0;36mContextualRelevancyMetric._a_generate_verdicts\u001b[0;34m(self, text, retrieval_context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_a_generate_verdict(prompt))\n\u001b[1;32m    215\u001b[0m     tasks\u001b[38;5;241m.\u001b[39mappend(task)\n\u001b[0;32m--> 216\u001b[0m verdicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m verdicts\n",
      "File \u001b[0;32m/usr/lib/python3.9/asyncio/tasks.py:328\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m/usr/lib/python3.9/asyncio/tasks.py:256\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/GitHub/cval/env/lib/python3.9/site-packages/deepeval/metrics/contextual_relevancy/contextual_relevancy.py:204\u001b[0m, in \u001b[0;36mContextualRelevancyMetric._a_generate_verdict\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    202\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39ma_generate(prompt)\n\u001b[1;32m    203\u001b[0m data \u001b[38;5;241m=\u001b[39m trimAndLoadJson(res, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mContextualRelevancyVerdict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/cval/env/lib/python3.9/site-packages/pydantic/main.py:176\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    175\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ContextualRelevancyVerdict\nverdict\n  Field required [type=missing, input_value={'plan': 'Compare the mod...0, 'isDependency': True}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.7/v/missing"
     ]
    }
   ],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from deepeval.metrics import ContextualRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "import json\n",
    "import backoff\n",
    "\n",
    "\n",
    "format_str = PromptTemplate(\n",
    "    \"Respond in a JSON format as shown below:\\n\"\n",
    "    \"{{\\n\"\n",
    "    \"\\t“plan”: string, // Write down a step-by-step plan on how to solve the task given the information above.\\n\"\n",
    "    \"\\t“rationale”: string, // Provide a concise explanation of whether and why the configuration options depend on each other due to value-equality.\\n\"\n",
    "    \"\\t“uncertainty”: integer, // Rate your certainty of this dependency on a scale from 0 (completely uncertain) to 10 (absolutely certain), given the context, plan, and rationale.\\n\"\n",
    "    \"\\t“isDependency”: boolean // True if a dependency exists, or False otherwise.\\n\"\n",
    "    \"}}\"\n",
    ")\n",
    "\n",
    "\n",
    "@backoff.on_exception(backoff.expo, Exception, max_tries=3)\n",
    "def get_context_relevance_score(input_str: str, response_str: str, context_str: str):\n",
    "    context_relevancy_metric = ContextualRelevancyMetric(\n",
    "        threshold=0.7,\n",
    "        model=custom_eval_model,\n",
    "        include_reason=True\n",
    "    )\n",
    "\n",
    "    test_case = LLMTestCase(\n",
    "        input=input_str,\n",
    "        actual_output=response_str,\n",
    "        retrieval_context=[context_str]\n",
    "    )\n",
    "\n",
    "    context_relevancy_metric.measure(test_case)\n",
    "\n",
    "    print(\"Relevance Score: \", context_relevancy_metric.score)\n",
    "    print(\"Reason: \", context_relevancy_metric.reason)\n",
    "\n",
    "    return context_relevancy_metric.score\n",
    "\n",
    "def compute_context_relevance(file_name: str):\n",
    "\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as src:\n",
    "        data = json.load(src)\n",
    "\n",
    "\n",
    "    for entry in data:\n",
    "\n",
    "        input_str = f\"{entry['task_str']}\\n\\n{format_str.format()}\"\n",
    "        context_str = entry[\"context_str\"]\n",
    "        response_str = entry[\"response\"]\n",
    "\n",
    "        context_relevance_score = get_context_relevance_score(\n",
    "            input_str=input_str,\n",
    "            response_str=response_str,\n",
    "            context_str=context_str\n",
    "        )\n",
    "        \n",
    "        entry[\"context_relevance_score\"] = context_relevance_score\n",
    "\n",
    "\n",
    "        for context in entry[\"context\"]:\n",
    "            relevance_score = get_context_relevance_score(\n",
    "                input_str=input_str,\n",
    "                response_str=response_str,\n",
    "                context_str=context[\"content\"]\n",
    "            )\n",
    "            \n",
    "            context[\"relevance_score\"] = relevance_score\n",
    "\n",
    "\n",
    "        break\n",
    "\n",
    "\n",
    "    with open(f\"../data/results/{config_str}/all_dependencies_all_gpt-3.5-turbo-0125_test.json\", \"w\", encoding=\"utf-8\") as dest:\n",
    "        json.dump(data, dest, indent=2)\n",
    "\n",
    "\n",
    "custom_eval_model = CustomEvalModel(model=model)\n",
    "model_names = [\"gpt-40-2024-05-13\", \"gpt-3.5-turbo-0125\", \"llama3:8b\", \"llama3:70b\"]\n",
    "config_str = \"config1\"\n",
    "\n",
    "#for model_name in model_names:\n",
    "    #file_name = f\"../data/results/{config_str}/all_dependencies_all_{model_name}.json\"\n",
    "    #compute_context_relevance(file_name=file_name, eval_model=custom_eval_model)\n",
    "\n",
    "\n",
    "compute_context_relevance(file_name=f\"../data/results/{config_str}/all_dependencies_all_gpt-3.5-turbo-0125.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cval_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
