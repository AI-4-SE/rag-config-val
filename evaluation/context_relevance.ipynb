{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "from openai import OpenAI\n",
    "\n",
    "model_name = \"gpt-4-0125-preview\"\n",
    "api_key = \"a29859a1-e871-47c5-b15e-57be20c61e8c\"\n",
    "base_url = \"http://172.26.92.115\"\n",
    "\n",
    "# define OpenAI model using the credentials of the proxy server\n",
    "model = OpenAI(base_url=base_url, api_key=api_key)\n",
    "\n",
    "\n",
    "# define the custom evaluation model class\n",
    "class CustomEvalModel(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model\n",
    "    ):\n",
    "        self.model = model\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        res = chat_model.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            model=model_name\n",
    "        )\n",
    "\n",
    "        return res.choices[0].message.content\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Custom Azure OpenAI Model\"\n",
    "\n",
    "\n",
    "# define the custom evaluation model\n",
    "custom_eval_model = CustomEvalModel(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric, ContextualRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from dotenv import dotenv_values\n",
    "import os\n",
    "\n",
    "config = dotenv_values(dotenv_path=\"../.env\")\n",
    "os.environ['OPENAI_API_KEY'] = config[\"OPENAI_KEY\"]\n",
    "\n",
    "answer_relevancy_metric = AnswerRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=custom_eval_model,\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "faithfulness_metric = FaithfulnessMetric(\n",
    "    threshold=0.7,\n",
    "    model=custom_eval_model,\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "context_relevancy_metric = ContextualRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=custom_eval_model,\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "\n",
    "for response in data:\n",
    "    test_case = LLMTestCase(\n",
    "        input=response[\"input\"],\n",
    "        actual_output=response[\"response\"],\n",
    "        retrieval_context=[x for x in response[\"context\"]]\n",
    "    )\n",
    "\n",
    "    context_relevancy_metric.measure(test_case)\n",
    "    faithfulness_metric.measure(test_case)\n",
    "    answer_relevancy_metric.measure(test_case)\n",
    "\n",
    "    print(\"Context Relevancy: \", context_relevancy_metric.score)\n",
    "    print(\"Answer Relevancy: \", answer_relevancy_metric.score)\n",
    "    print(\"Faithfulness: \", faithfulness_metric.score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def measure_validation_metrics(outputs: List, df_baseline: pd.DataFrame):\n",
    "\n",
    "    true_positives = []\n",
    "    true_negatives = []\n",
    "    false_positives = []\n",
    "    false_negatives = []\n",
    "    accuracy = []\n",
    "\n",
    "    for response, baseline in zip(outputs, df_baseline.to_dict(\"records\")):\n",
    "\n",
    "        response_dict = json.loads(response.response)\n",
    "\n",
    "        if baseline[\"rating\"] in (\"True\") and response_dict[\"isDependency\"]:\n",
    "            accuracy.append(1)\n",
    "            true_positives.append(1)\n",
    "        if baseline[\"rating\"] in (\"False\") and not response_dict[\"isDependency\"]:\n",
    "            accuracy.append(1)\n",
    "            true_negatives.append(1)\n",
    "        if baseline[\"rating\"] in (\"True\") and not response_dict[\"isDependency\"]:\n",
    "            accuracy.append(0)\n",
    "            false_negatives.append(1)\n",
    "        if baseline[\"rating\"] in (\"False\") and response_dict[\"isDependency\"]:\n",
    "            accuracy.append(0)\n",
    "            false_positives.append(1)\n",
    "\n",
    "\n",
    "    #precision = sum(true_positives)/(sum(true_positives)+sum(false_positives))\n",
    "    #recall = sum(true_positives)/(sum(true_positives)+sum(false_negatives))\n",
    "    #f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    print(\"Accuracy\", sum(accuracy)/len(accuracy))\n",
    "    print(\"TP\", sum(true_positives))\n",
    "    print(\"FP\", sum(false_positives))\n",
    "    print(\"TN\", sum(true_negatives))\n",
    "    print(\"FN\", sum(false_negatives))\n",
    "    #print(\"Precision\", precision)\n",
    "    #print(\"Recall\", recall)\n",
    "    #print(\"F1 Score: \", f1_score)\n",
    "\n",
    "    return {\n",
    "        \"true_positives\": sum(true_positives),\n",
    "        \"false_positives\": sum(false_positives),\n",
    "        \"true_negatives\": sum(true_negatives),\n",
    "        \"false_negatives\": sum(false_negatives),\n",
    "        \"accuracy\": sum(accuracy)/len(accuracy)\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cval_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
